{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dQZoZ_VbaeK",
    "outputId": "16b6e51b-2365-4a5f-ac88-160fc666a9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qqq\n",
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4s18l34ZKnsy"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    N_SPLITS = 5\n",
    "    N_EPOCHS = 275\n",
    "    BATCH_SIZE = 128\n",
    "    CAT_DIM = 256\n",
    "    START_LR = 3e-4\n",
    "    END_LR = 2e-5\n",
    "    LSTM_SIZE = 512\n",
    "    DENSE_SIZE = 512\n",
    "    N_LSTM_LAYERS = 3\n",
    "    SEQUENCE_LENGTH = 39\n",
    "    DROPOUT = 0.5\n",
    "    EMBEDDING_DROPOUT = 0.1\n",
    "    WANDB_ENABLED = True\n",
    "    WANDB_API_KEY = '4dad92fd17d935955886d8b751df8aa51f7a8523'\n",
    "    VER = 'v45'\n",
    "    SAMPLE_TRAIN_BREATH_IDS = None\n",
    "    SAMPLE_TEST_BREATH_IDS = None\n",
    "    DATA_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xwBJAvI__kMv"
   },
   "outputs": [],
   "source": [
    "# class Config:\n",
    "#     N_SPLITS = 2\n",
    "#     N_EPOCHS = 5\n",
    "#     BATCH_SIZE = 128\n",
    "#     CAT_DIM = 256\n",
    "#     START_LR = 3e-4\n",
    "#     END_LR = 2e-5\n",
    "#     LSTM_SIZE = 512\n",
    "#     DENSE_SIZE = 512\n",
    "#     N_LSTM_LAYERS = 3\n",
    "#     SEQUENCE_LENGTH = 39\n",
    "#     DROPOUT = 0.5\n",
    "#     EMBEDDING_DROPOUT = 0.1\n",
    "#     WANDB_ENABLED = False\n",
    "#     WANDB_API_KEY = '4dad92fd17d935955886d8b751df8aa51f7a8523'\n",
    "#     VER = 'v45'\n",
    "#     SAMPLE_TRAIN_BREATH_IDS = 100\n",
    "#     SAMPLE_TEST_BREATH_IDS = 100\n",
    "#     DATA_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'\n",
    "#     OUTPUT_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V91NfQOluHX5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZGH1DnPDuHX9"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(Config.DATA_PATH + os.sep + 'sample_submission.csv')\n",
    "train = pd.read_csv(Config.DATA_PATH + os.sep + 'train.csv')\n",
    "test = pd.read_csv(Config.DATA_PATH + os.sep + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TUh16yEOqdzA"
   },
   "outputs": [],
   "source": [
    "fltr_out = train['u_out'] == 0\n",
    "target = train['pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lPdu6ZK1nSk5"
   },
   "outputs": [],
   "source": [
    "train['u_in_2'] = train['u_in'].copy()\n",
    "test['u_in_2'] = test['u_in'].copy()\n",
    "fltr = train['u_out'] == 0\n",
    "train.loc[fltr_out, 'u_in_2'] = 0\n",
    "test.loc[fltr_out, 'u_in_2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lDpiumSM0IXF"
   },
   "outputs": [],
   "source": [
    "oof_df = train[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s47sFS23uHX9"
   },
   "outputs": [],
   "source": [
    "SAMPLE_TRAIN = False\n",
    "SAMPLE_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "70wPWXT4uHX-"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "if Config.SAMPLE_TRAIN_BREATH_IDS is not None:\n",
    "    sampled = np.random.choice(train['breath_id'].unique(), size=Config.SAMPLE_TRAIN_BREATH_IDS, replace=False)\n",
    "    print(train.shape)\n",
    "    train = train[train['breath_id'].isin(sampled)].reset_index(drop=True)\n",
    "    print(train.shape)\n",
    "\n",
    "if Config.SAMPLE_TEST_BREATH_IDS is not None:\n",
    "    sampled = np.random.choice(test['breath_id'].unique(), size=Config.SAMPLE_TEST_BREATH_IDS, replace=False)\n",
    "    test = test[test['breath_id'].isin(sampled)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n9FI4fk79pZB"
   },
   "outputs": [],
   "source": [
    "train['bidc'] = train.groupby('breath_id').cumcount() + 1\n",
    "test['bidc'] = test.groupby('breath_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GHCIQsLX-c2W"
   },
   "outputs": [],
   "source": [
    "for df_tmp in [train, test]:\n",
    "    df_tmp['R_cat'] = df_tmp['R'].map({20: 1, 50: 2, 5: 0})\n",
    "    df_tmp['C_cat'] = df_tmp['C'].map({20: 1, 50: 2, 10: 0})\n",
    "    df_tmp['RC_cat'] = df_tmp['R'].astype('str')+'-' + df_tmp['C'].astype('str')\n",
    "\n",
    "mapper = pd.Series(index=train['RC_cat'].unique(), data=np.arange(train['RC_cat'].nunique()))\n",
    "for df_tmp in [train, test]:\n",
    "    df_tmp['RC_cat'] = df_tmp['RC_cat'].map(mapper)\n",
    "\n",
    "\n",
    "cat_cols = ['R_cat', 'C_cat', 'RC_cat']\n",
    "cat_cols_unq_dct = {}\n",
    "for c in cat_cols:\n",
    "    cat_cols_unq_dct[c] = train[c].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0LYWGWcRfBGl"
   },
   "outputs": [],
   "source": [
    "c = 'u_in'\n",
    "LAG_WINDOW_RANGE = range(3)\n",
    "train = train.assign(**{f'{c}_t-{t}': train.groupby('breath_id')[c].shift(t) for t in LAG_WINDOW_RANGE})\n",
    "test = test.assign(**{f'{c}_t-{t}': test.groupby('breath_id')[c].shift(t) for t in LAG_WINDOW_RANGE})\n",
    "use_fts = [f'{c}_t-{t}' for t in LAG_WINDOW_RANGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wimp0EV1axFc"
   },
   "outputs": [],
   "source": [
    "for df_tmp in [train, test]:\n",
    "    df_tmp['bidc'] = df_tmp.groupby('breath_id').cumcount()\n",
    "    df_tmp['u_in_lag_1'] = df_tmp.groupby('breath_id')['u_in'].shift(1).fillna(-999)\n",
    "    df_tmp['u_in_lag_2'] = df_tmp.groupby('breath_id')['u_in'].shift(2).fillna(-999)\n",
    "    df_tmp['u_in_lag_3'] = df_tmp.groupby('breath_id')['u_in'].shift(3).fillna(-999)\n",
    "    df_tmp['u_in_lag_4'] = df_tmp.groupby('breath_id')['u_in'].shift(4).fillna(-999)\n",
    "\n",
    "    df_tmp['u_in_cumsum'] = df_tmp.groupby('breath_id')['u_in'].cumsum()\n",
    "    df_tmp['u_in_cumsum_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(1).fillna(-999)\n",
    "    df_tmp['u_in_cumsum_lag_2'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(2).fillna(-999)\n",
    "    df_tmp['u_in_cumsum_lag_1-u_in_cumsum_lag_2'] = df_tmp['u_in_cumsum_lag_1'] - df_tmp['u_in_cumsum_lag_2']\n",
    "\n",
    "    df_tmp['u_in_cumsum_lag_3'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(3).fillna(0)\n",
    "    df_tmp['u_in_cumsum_lag_4'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(4).fillna(0)\n",
    "\n",
    "    df_tmp['u_in_cummean'] = df_tmp['u_in_cumsum']/(df_tmp.groupby('breath_id')['u_in'].cumcount() + 1)\n",
    "    df_tmp['u_in_cummax'] = df_tmp.groupby('breath_id')['u_in'].cummax()\n",
    "    df_tmp['next_u_in'] = df_tmp.groupby('breath_id')['u_in'].shift(-1).fillna(0)\n",
    "\n",
    "    df_tmp['area'] = df_tmp['time_step'] * df_tmp['u_in']\n",
    "    df_tmp['area'] = df_tmp.groupby('breath_id')['area'].cumsum()\n",
    "    df_tmp['area_lag_1'] = df_tmp.groupby('breath_id')['area'].shift(1).fillna(0)\n",
    "    df_tmp['area_lag_2'] = df_tmp.groupby('breath_id')['area'].shift(2).fillna(0)\n",
    "    df_tmp['area_lead_1'] = df_tmp.groupby('breath_id')['area'].shift(-1).fillna(0)\n",
    "    df_tmp['area_lead_2'] = df_tmp.groupby('breath_id')['area'].shift(-2).fillna(0)\n",
    "    df_tmp['area_diff_lag_1'] = df_tmp['area'] - df_tmp['area_lag_1']\n",
    "    df_tmp['area_diff_lead_1'] = df_tmp['area'] - df_tmp['area_lead_1']\n",
    "\n",
    "\n",
    "    df_tmp['u_in_cumsum*time_step'] = df_tmp['u_in_cumsum']*df_tmp['time_step']\n",
    "    df_tmp['u_in_cumsum*time_step_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*time_step'].shift(1).fillna(0)\n",
    "    df_tmp['u_in_cumsum*time_step/c'] = df_tmp['u_in_cumsum*time_step']/df_tmp['C']\n",
    "    df_tmp['u_in_cumsum*time_step/c_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*time_step/c'].shift(1).fillna(0)\n",
    "    df_tmp['area/c'] = df_tmp['area']/df_tmp['C']\n",
    "    df_tmp['area/c_lag_1'] = df_tmp.groupby('breath_id')['area/c'].shift(1).fillna(0)\n",
    "    \n",
    "    df_tmp['u_out_lag_1'] = df_tmp.groupby('breath_id')['u_out'].shift(1).fillna(0)\n",
    "\n",
    "    df_tmp['time_step*u_out']= df_tmp['time_step']*df_tmp['u_out']\n",
    "\n",
    "    df_tmp['R+C'] = df_tmp['R'] + df_tmp['C']\n",
    "    df_tmp['R/C'] = df_tmp['R'] / df_tmp['C']\n",
    "    df_tmp['u_in/C'] = df_tmp['u_in'] / df_tmp['C']\n",
    "    df_tmp['u_in/R'] = df_tmp['u_in'] / df_tmp['R']\n",
    "    df_tmp['u_in_cumsum/C'] = df_tmp['u_in_cumsum'] / df_tmp['C']\n",
    "    df_tmp['u_in_cumsum/R'] = df_tmp['u_in_cumsum'] / df_tmp['R']\n",
    "    df_tmp['area*R/C'] = df_tmp['area'] * df_tmp['R/C']\n",
    "    df_tmp['u_in_cumsum*R/C'] = df_tmp['u_in_cumsum'] * df_tmp['R/C']\n",
    "    df_tmp['u_in_cumsum*R/C_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*R/C'].shift(1).fillna(0)\n",
    "\n",
    "\n",
    "    df_tmp['timestep_diff'] = (df_tmp['time_step'] - df_tmp.groupby('breath_id')['time_step'].shift(1)).fillna(0)\n",
    "    df_tmp['u_in_diff'] = (df_tmp['u_in'] - df_tmp.groupby('breath_id')['u_in'].shift(1)).fillna(0)\n",
    "    df_tmp['u_in_pct_change'] = (df_tmp['u_in_diff']/(df_tmp['u_in_lag_1'] + 1e-4)).fillna(0)\n",
    "    df_tmp['u_in_diff_next'] = (df_tmp['u_in'] - df_tmp.groupby('breath_id')['u_in'].shift(-1)).fillna(0)\n",
    "    df_tmp['u_in_log'] = np.log1p(df_tmp['u_in'])\n",
    "    df_tmp['u_in_cumsum_log'] = np.log1p(df_tmp['u_in_cumsum'])\n",
    "    df_tmp['u_in_lag_1_is_zero'] = (df_tmp['u_in_lag_1'] == 0)\n",
    "    df_tmp['u_in_zero'] = (df_tmp['u_in_lag_1'] == 0)\n",
    "    df_tmp['u_in_lead_1'] = df_tmp.groupby('breath_id')['u_in'].shift(-1)\n",
    "    df_tmp['maop'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp[df_tmp['u_in_zero']].groupby('breath_id')['bidc'].min())\n",
    "    df_tmp['spike'] = (df_tmp['u_in'] > df_tmp['u_in_lag_1']) & (df_tmp['u_in'] > df_tmp['u_in_lead_1'])\n",
    "    df_tmp['u_in_lag_1_is_zero_cumsum'] = df_tmp.groupby('breath_id')['u_in_lag_1_is_zero'].cumsum()\n",
    "    df_tmp['is_max_u_in'] = df_tmp['u_in'] == df_tmp.groupby('breath_id')['u_in'].transform('max')\n",
    "    df_tmp['nki'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp.groupby('breath_id')['u_in'].apply(np.argmax))\n",
    "    df_tmp['nki2'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp.groupby('breath_id')['u_in_cumsum'].apply(np.argmax))\n",
    "    df_tmp['nki3'] = df_tmp['nki'] - df_tmp['nki2']\n",
    "    df_tmp['nki4'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp[df_tmp['u_in_lag_1_is_zero']].groupby('breath_id')['bidc'].max())\n",
    "    df_tmp['u_in_cummax - u_in'] = df_tmp['u_in_cummax'] - df_tmp['u_in']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x6CRywriMWg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = train[train['bidc'] <= Config.SEQUENCE_LENGTH].reset_index(drop=True)\n",
    "test = test[test['bidc'] <= Config.SEQUENCE_LENGTH].reset_index(drop=True)\n",
    "\n",
    "sample_oof_df = train[['id']].copy()\n",
    "sample_preds_df = test[['id']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NsHmW4xh_b6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3_wqzAOiconq"
   },
   "outputs": [],
   "source": [
    "num_cols = [c for c in train.columns if c not in ['id', 'pressure', 'breath_id', 'u_out'] + cat_cols]\n",
    "train[num_cols] = train[num_cols].fillna(0)\n",
    "test[num_cols] = test[num_cols].fillna(0)\n",
    "\n",
    "RS = StandardScaler()\n",
    "train[num_cols] = RS.fit_transform(train[num_cols])\n",
    "test[num_cols] = RS.transform(test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dulPxYLFayze"
   },
   "outputs": [],
   "source": [
    "N_CLASSES = train['pressure'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bhzpCGJmgPkC"
   },
   "outputs": [],
   "source": [
    "class VentilatorDataset:\n",
    "    def __init__(self, df, cat_cols, num_cols, is_train=True):\n",
    "\n",
    "        if is_train:\n",
    "            self.inv_mapper = df['pressure'].drop_duplicates().sort_values().reset_index(drop=True).to_dict()\n",
    "            self.mapper = {val: key for key, val in self.inv_mapper.items()}\n",
    "            df['pressure_int'] = df['pressure'].map(self.mapper)\n",
    "            self.pressures_int = df[['pressure_int']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "            self.pressures = df[['pressure']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "            _ = gc.collect()\n",
    "\n",
    "        self.u_outs = df[['u_out']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "        self.inputs = df[num_cols + cat_cols].values.reshape(-1, df['bidc'].nunique(), len(num_cols + cat_cols)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SiHBR3FuHYB",
    "outputId": "d45b3e85-9069-401b-e0af-0bc8e581a88c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.37 s, sys: 9.16 ms, total: 3.38 s\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data = VentilatorDataset(test, cat_cols, num_cols, is_train=False)\n",
    "train_data = VentilatorDataset(train, cat_cols, num_cols, is_train=True)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BiBhZ5UXkA2A"
   },
   "outputs": [],
   "source": [
    "del train\n",
    "del test\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csc30ZcgiVHy",
    "outputId": "7ba00765-f901-46d4-dcb2-647217a2ebed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75450, 40, 67), (75450, 40), (75450, 40))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.inputs.shape, train_data.pressures.shape, train_data.u_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EE6qgsxkbg-a",
    "outputId": "59ed425f-c39b-4bda-f7fc-df619dee32ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50300, 40, 67), (50300, 40))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.inputs.shape, test_data.u_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3L8GRxQuHYC",
    "outputId": "59cfc232-dfda-401a-b4cc-2333e172d6a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERICAL_INPUTS_CNT = len(num_cols)\n",
    "N_FEATS = train_data.inputs.shape[2]\n",
    "WINDOW_SIZE=train_data.inputs.shape[1]\n",
    "\n",
    "NUMERICAL_INPUTS_CNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RLt6wWnwh_b-"
   },
   "outputs": [],
   "source": [
    "mapper = train_data.inv_mapper\n",
    "def map_class_to_cont(x):\n",
    "    return mapper[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "C5F2KfZ2Q-Yj"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yVAWMoV_h_b_"
   },
   "outputs": [],
   "source": [
    "class WeightedSum(tf.keras.layers.Layer):\n",
    "    \"\"\"A custom keras layer to learn a weighted sum of tensors\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape=1):\n",
    "        self.a = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=(),\n",
    "            initializer='ones',\n",
    "            dtype='float32',\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, model_outputs):\n",
    "        return self.a * model_outputs[0] + model_outputs[1]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AdlY8Z9ADL1i"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = inputs\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias = False,\n",
    "        dropout=Config.DROPOUT\n",
    "    )([x, x, x])\n",
    "    res = WeightedSum()([x, inputs])\n",
    "    res = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    x = tf.keras.layers.Dense(inputs.shape[-1])(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    x = WeightedSum()([x, res])\n",
    "    return tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZvSRXNx_h_cA"
   },
   "outputs": [],
   "source": [
    "def custom_mean_absolute_error(y_true, y_pred, w_out):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "def custom_metric(y_true, y_pred, w_out):\n",
    "    return mae(y_true[w_out == 0], y_pred[w_out == 0])\n",
    "\n",
    "def get_LSTM_model(Config):\n",
    "    w_out = tf.keras.layers.Input(shape=(WINDOW_SIZE, 1))\n",
    "    targets = tf.keras.layers.Input(shape=(WINDOW_SIZE, 1))\n",
    "    inputs = tf.keras.layers.Input(shape=(WINDOW_SIZE, N_FEATS))\n",
    "    \n",
    "    num_inputs = inputs[:, :, :NUMERICAL_INPUTS_CNT]\n",
    "    cat_inputs = inputs[:, :, NUMERICAL_INPUTS_CNT:]\n",
    "\n",
    "    initializer = tf.keras.initializers.glorot_uniform(seed=66)\n",
    "\n",
    "    all_conv_layers = []\n",
    "    for sz in [2, 3, 5, 7, 11, 15]:\n",
    "        conv_curr = tf.keras.layers.Conv1D(16, kernel_size=(sz,), padding='same', use_bias=False)(num_inputs)\n",
    "        all_conv_layers.append(conv_curr)\n",
    "    conv_op = tf.keras.layers.Concatenate()(all_conv_layers)\n",
    "    num_inputs = tf.keras.layers.Concatenate()([num_inputs, conv_op])\n",
    "\n",
    "\n",
    "    for i, c in enumerate(cat_cols):\n",
    "      cat_inp = cat_inputs[:, :, i:i+1]\n",
    "      embed = tf.keras.layers.Embedding(input_dim=cat_cols_unq_dct[c]+2, output_dim=Config.CAT_DIM, embeddings_initializer=initializer)(cat_inp)\n",
    "      reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
    "      reshaped = tf.keras.layers.SpatialDropout1D(Config.EMBEDDING_DROPOUT, seed=2)(reshaped)\n",
    "      num_inputs = tf.keras.layers.Concatenate(axis=2)([reshaped, num_inputs])\n",
    "    \n",
    "    \n",
    "    x = num_inputs\n",
    "    for i in range(12):\n",
    "        x = transformer_encoder(x, 8, 128, 1024, 0)\n",
    "         \n",
    "    x = tf.keras.layers.Dense(Config.DENSE_SIZE, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    out = tf.keras.layers.Dense(N_CLASSES)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs, w_out], outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEo2n76YL7hf",
    "outputId": "512d523e-65e3-4c89-c9eb-88070ffce2c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69499982"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_LSTM_model(Config).count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dbCcFea3-kDv"
   },
   "outputs": [],
   "source": [
    "def get_preds(best_model, X_test, wt_test, STEP=5000):\n",
    "    test_preds = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for start in range(0, X_test.shape[0], STEP):\n",
    "        preds = best_model.predict([X_test[start: start+STEP], wt_test[start: start+STEP]], verbose=0,\n",
    "                                   batch_size=512, use_multiprocessing=False)\n",
    "        test_preds.append(preds)\n",
    "\n",
    "    test_preds = np.concatenate(test_preds, axis=0)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vzMCjsmhuHYD"
   },
   "outputs": [],
   "source": [
    "\n",
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data=(), interval=10, save_model=True, Config={}, fold=0):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val, self.w_out_val = validation_data\n",
    "        self.best_score = np.inf\n",
    "        self.save_model = save_model\n",
    "        self.learning_rates = pd.Series(index=np.arange(Config.N_EPOCHS),\n",
    "                                        data=np.linspace(Config.END_LR,\n",
    "                                                         Config.START_LR,\n",
    "                                                         num=Config.N_EPOCHS)[::-1]).to_dict()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.start_time = time()\n",
    "\n",
    "        if epoch in self.learning_rates:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[epoch])\n",
    "\n",
    "        self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "\n",
    "        y_pred = get_preds(self.model, self.X_val, self.w_out_val, STEP=3000)\n",
    "        y_pred = np.vectorize(map_class_to_cont)(np.argmax(y_pred, axis=2))\n",
    "\n",
    "        val_score = custom_metric(self.y_val.flatten(), y_pred.flatten(), self.w_out_val.flatten())\n",
    "        \n",
    "        if val_score < self.best_score:\n",
    "            self.best_score = val_score\n",
    "            if self.save_model and epoch > 150: ##### Saving models takes time, so don't save for the first N_EPOCHS // 2\n",
    "                tf.keras.models.save_model(self.model, f'best_model', save_format=\"h5\")\n",
    "\n",
    "        total_time = round(time() - self.start_time, 2)\n",
    "        if Config.WANDB_ENABLED:\n",
    "            wandb.log({f'Fold {fold} epoch': epoch, f\"Fold {fold} train_loss\": logs['loss'], f\"Fold {fold} val_score\": val_score, f'Fold {fold} best_val_score': self.best_score, f'Fold {fold} total_time_in_seconds': total_time})\n",
    "        \n",
    "        if epoch % self.interval == 0:\n",
    "            print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.03f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "rIpzPQIaFo-j",
    "outputId": "95a9ad1c-550c-43a5-e592-3d0043f23cc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpressureprediction\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/pressureprediction/google-brain/runs/2dc637up\" target=\"_blank\">nikhil_pressure_prediction_dl_v45</a></strong> to <a href=\"https://wandb.ai/pressureprediction/google-brain\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Config_dct = {key:value for key, value in Config.__dict__.items() if not key.startswith('__') and not callable(key)}\n",
    "if Config.WANDB_ENABLED:\n",
    "    import wandb\n",
    "    wandb.login(key=Config.WANDB_API_KEY)\n",
    "    wandb.init(project=\"google-brain\",\n",
    "               name=f'nikhil_pressure_prediction_dl_{Config.VER}',\n",
    "               save_code=True,\n",
    "               allow_val_change=True,\n",
    "               config=Config_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6Szzy2GK-23",
    "outputId": "78c3ee95-25e1-425d-8d37-1b66621a90cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.20.89.242:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.20.89.242:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 0 < --------------- \n",
      "\n",
      "Epoch: 000 curr_lr: 3.0e-04 - train_loss: 4.628 val_score: 1.052  best_val_score: 1.052  last_epoch t=202.72s\n",
      "Epoch: 001 curr_lr: 3.0e-04 - train_loss: 3.818 val_score: 0.783  best_val_score: 0.783  last_epoch t=54.47s\n",
      "Epoch: 002 curr_lr: 3.0e-04 - train_loss: 3.584 val_score: 0.676  best_val_score: 0.676  last_epoch t=55.13s\n",
      "Epoch: 003 curr_lr: 3.0e-04 - train_loss: 3.447 val_score: 0.594  best_val_score: 0.594  last_epoch t=54.58s\n",
      "Epoch: 004 curr_lr: 3.0e-04 - train_loss: 3.341 val_score: 0.548  best_val_score: 0.548  last_epoch t=54.85s\n",
      "Epoch: 005 curr_lr: 2.9e-04 - train_loss: 3.262 val_score: 0.509  best_val_score: 0.509  last_epoch t=54.67s\n",
      "Epoch: 006 curr_lr: 2.9e-04 - train_loss: 3.194 val_score: 0.467  best_val_score: 0.467  last_epoch t=54.41s\n",
      "Epoch: 007 curr_lr: 2.9e-04 - train_loss: 3.137 val_score: 0.433  best_val_score: 0.433  last_epoch t=54.4s\n",
      "Epoch: 008 curr_lr: 2.9e-04 - train_loss: 3.076 val_score: 0.422  best_val_score: 0.422  last_epoch t=55.52s\n",
      "Epoch: 009 curr_lr: 2.9e-04 - train_loss: 3.031 val_score: 0.397  best_val_score: 0.397  last_epoch t=54.71s\n",
      "Epoch: 010 curr_lr: 2.9e-04 - train_loss: 2.983 val_score: 0.365  best_val_score: 0.365  last_epoch t=54.77s\n",
      "Epoch: 011 curr_lr: 2.9e-04 - train_loss: 2.951 val_score: 0.365  best_val_score: 0.365  last_epoch t=54.59s\n",
      "Epoch: 012 curr_lr: 2.9e-04 - train_loss: 2.919 val_score: 0.351  best_val_score: 0.351  last_epoch t=54.42s\n",
      "Epoch: 013 curr_lr: 2.9e-04 - train_loss: 2.877 val_score: 0.354  best_val_score: 0.351  last_epoch t=54.51s\n",
      "Epoch: 014 curr_lr: 2.9e-04 - train_loss: 2.852 val_score: 0.330  best_val_score: 0.330  last_epoch t=54.59s\n",
      "Epoch: 015 curr_lr: 2.8e-04 - train_loss: 2.825 val_score: 0.324  best_val_score: 0.324  last_epoch t=55.41s\n",
      "Epoch: 016 curr_lr: 2.8e-04 - train_loss: 2.799 val_score: 0.311  best_val_score: 0.311  last_epoch t=54.54s\n",
      "Epoch: 017 curr_lr: 2.8e-04 - train_loss: 2.767 val_score: 0.323  best_val_score: 0.311  last_epoch t=54.62s\n",
      "Epoch: 018 curr_lr: 2.8e-04 - train_loss: 2.749 val_score: 0.307  best_val_score: 0.307  last_epoch t=54.56s\n",
      "Epoch: 019 curr_lr: 2.8e-04 - train_loss: 2.733 val_score: 0.300  best_val_score: 0.300  last_epoch t=54.49s\n"
     ]
    }
   ],
   "source": [
    "history_lst = []\n",
    "oofs = np.zeros(train_data.pressures.shape)\n",
    "preds_lst = []\n",
    "kf = KFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=101)\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(train_data.pressures, train_data.pressures)):\n",
    "\n",
    "            print('-'*15, '>', f'Fold {fold}', '<', '-'*15, '\\n')\n",
    "            X_trn, X_val = train_data.inputs[trn_idx], train_data.inputs[val_idx]\n",
    "            y_trn, y_val = train_data.pressures[trn_idx], train_data.pressures[val_idx]\n",
    "            y_trn_int, y_val_int = train_data.pressures_int[trn_idx], train_data.pressures_int[val_idx]\n",
    "     \n",
    "            wt_trn, wt_val = train_data.u_outs[trn_idx], train_data.u_outs[val_idx]\n",
    "\n",
    "            model = get_LSTM_model(Config)\n",
    "\n",
    "            print_val_results = IntervalEvaluation(\n",
    "                validation_data=(X_val, y_val, wt_val),\n",
    "                interval=1,\n",
    "                Config=Config,\n",
    "                fold=fold,\n",
    "                )\n",
    "\n",
    "            history = model.fit([X_trn, wt_trn], y_trn_int,\n",
    "                        verbose=0,\n",
    "                        epochs=Config.N_EPOCHS,\n",
    "                        batch_size=Config.BATCH_SIZE,\n",
    "                        callbacks=[print_val_results]\n",
    "                        )\n",
    "\n",
    "            history_lst.append(history)\n",
    "            del model, X_trn, y_trn, wt_trn, print_val_results\n",
    "            _ = gc.collect()\n",
    "\n",
    "            custom_objects = {\n",
    "                'custom_mean_absolute_error': custom_mean_absolute_error,\n",
    "                'WeightedSum': WeightedSum,\n",
    "            }\n",
    "\n",
    "            best_model = load_model('best_model', custom_objects=custom_objects)\n",
    "\n",
    "            vp = get_preds(best_model, X_val, wt_val, STEP=3000)\n",
    "            vp = np.vectorize(map_class_to_cont)(np.argmax(vp, axis=2))\n",
    "\n",
    "            oofs[val_idx] = vp\n",
    "\n",
    "            val_score = round(custom_metric(y_val.flatten(), vp.flatten(), wt_val.flatten()), 4)\n",
    "            print(f'\\nMAE val: {val_score}')\n",
    "\n",
    "            tp = get_preds(best_model, test_data.inputs, test_data.u_outs, STEP=3000)\n",
    "            tp = np.vectorize(map_class_to_cont)(np.argmax(tp, axis=2))\n",
    "            \n",
    "            preds_lst.append(tp)\n",
    "\n",
    "            del X_val, y_val, wt_val, best_model, vp, tp\n",
    "            _ = gc.collect()\n",
    "\n",
    "    oof_score = round(custom_metric(train_data.pressures.flatten(), oofs.flatten(), train_data.u_outs.flatten()), 4)\n",
    "    print(f'\\nMAE OOF: {oof_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtPqYBlQh_cC"
   },
   "outputs": [],
   "source": [
    "if Config.WANDB_ENABLED:\n",
    "    wandb.config.OOF_SCORE = oof_score\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IPC3dbfqBKm"
   },
   "outputs": [],
   "source": [
    "preds = np.median(np.array(preds_lst), axis=0)\n",
    "sample_oof_df['pressure'] = oofs.flatten()\n",
    "sample_preds_df['pressure'] = preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCX7x3j9pdhI"
   },
   "outputs": [],
   "source": [
    "submission = pd.merge(submission[['id']], sample_preds_df, on='id', how='left')\n",
    "submission['pressure'] = submission['pressure'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6gjoMwCokQ0"
   },
   "outputs": [],
   "source": [
    "print(oof_df.shape)\n",
    "oof_df = pd.merge(oof_df[['id']], sample_oof_df, on='id', how='left')\n",
    "oof_df['pressure'] = oof_df['pressure'].fillna(0)\n",
    "print(oof_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPGhRYHTuHYE"
   },
   "outputs": [],
   "source": [
    "path = Config.OUTPUT_PATH + os.sep + f'dl_oof_{Config.VER}.csv'\n",
    "oof_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9O5xzYAuHYE"
   },
   "outputs": [],
   "source": [
    "path = Config.OUTPUT_PATH + os.sep + f'dl_sub_{Config.VER}.csv'\n",
    "submission.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "best-transformers-classification-model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
