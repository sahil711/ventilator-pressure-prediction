{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appreciated-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from torch import nn\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_columns=100\n",
    "from multiprocessing import Pool,cpu_count\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "european-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'num':torch.rand((8,40,52)),'cat':torch.cat([torch.ones((8,40,1),dtype=torch.long)]*3,dim=-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Conv1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod(data['num']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "australian-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('../configs/lstm-transformer-v2.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dietary-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modellib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "european-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'init_type': 'yakama'}\n",
      "LSTMDpReLu(\n",
      "  (rnn): LSTM(160, 512, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "LSTMDpReLu(\n",
      "  (rnn): LSTM(1184, 512, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "LSTMDpReLu(\n",
      "  (rnn): LSTM(2208, 512, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "LSTMDpReLu(\n",
      "  (rnn): LSTM(3232, 512, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mod = getattr(modellib,config.model['class'])(config.model['kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "collectible-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mechanical-costume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4256-96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "empty-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "infinite-baking",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mod(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developmental-lunch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "looking-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConformer(nn.Module):\n",
    "    def __init__(self,in_dim,d_model,num_heads,dim_ff,dropout,kernel_size,num_kernels):\n",
    "        super().__init__()\n",
    "        self.proj_layer = nn.Linear(in_features=in_dim,out_features=d_model,bias=False)\n",
    "        self.reverse_proj_layer = nn.Linear(in_features=d_model,out_features=in_dim,bias=False)        \n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=d_model,nhead = num_heads,dim_feedforward=dim_ff,dropout=dropout)\n",
    "        self.cnn = nn.Conv1d(in_channels=in_dim,out_channels=num_kernels,kernel_size=kernel_size,padding=(kernel_size//2))\n",
    "    def forward(self,x):\n",
    "        x = self.proj_layer(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.reverse_proj_layer(x)\n",
    "        x_cnn = self.cnn(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = torch.cat([x,x_cnn],dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MultiheadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecological-granny",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "powered-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = CustomConformer(in_dim=40,d_model=1024,num_heads=128,dim_ff=1024,dropout=0.1,kernel_size=3,num_kernels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "surgical-knight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 40, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 40, 56])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(data['num']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cordless-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.TransformerEncoderLayer(d_model=32,nhead=1,dim_feedforward=128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = inputs\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias = False,\n",
    "        dropout=Config.DROPOUT\n",
    "    )([x, x, x])\n",
    "\n",
    "    res = WeightedSum()([x, inputs])\n",
    "    res = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(inputs.shape[-1])(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    x = WeightedSum()([x, res])\n",
    "\n",
    "    return tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,nhead,dim_feedforward,dropout,activation='relu'):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model,num_heads=nhead,dropout=dropout)        \n",
    "        self.linear1 = nn.Linear(in_features=d_model,out_features=dim_feedforward)\n",
    "        self.linear2 = nn.Linear(in_features=dim_feedforward,out_features=d_model)        \n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)        \n",
    "    def forward(self,x):\n",
    "        attn = self.self_attn(x,x,x)\n",
    "        res = x + attn\n",
    "        res = self.norm1(res)\n",
    "        x = self.linear1(attn)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        res = x+res\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-finger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhead,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.TransformerEncoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-recorder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-addiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
