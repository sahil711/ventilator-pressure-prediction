{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dQZoZ_VbaeK",
    "outputId": "817a8b84-60dc-4230-afdd-b4826fd303ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qqq\n",
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4s18l34ZKnsy"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    N_SPLITS = 10\n",
    "    N_EPOCHS = 400\n",
    "    BATCH_SIZE = 128\n",
    "    CAT_DIM = 256\n",
    "    START_LR = 2e-4\n",
    "    END_LR = 1e-6\n",
    "    DENSE_SIZE = 512\n",
    "    SEQUENCE_LENGTH = 39\n",
    "    DROPOUT = 0.75\n",
    "    EMBEDDING_DROPOUT = 0.7\n",
    "    WANDB_ENABLED = False\n",
    "    WANDB_API_KEY = '4dad92fd17d935955886d8b751df8aa51f7a8523'\n",
    "    VER = 'v55'\n",
    "    SAMPLE_TRAIN_BREATH_IDS = None\n",
    "    SAMPLE_TEST_BREATH_IDS = None\n",
    "    DATA_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'\n",
    "    OUTPUT_PATH = '/content/drive/MyDrive/kaggle/pressure prediction'\n",
    "    START_FOLD = 0\n",
    "    END_FOLD = 1\n",
    "    RANDOM_STATE=523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V91NfQOluHX5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import tensorflow_addons as tfa\n",
    "from pathlib import Path\n",
    "pd.options.display.max_columns=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IIwOswmyWZ9l"
   },
   "outputs": [],
   "source": [
    "# VERSION_OUTPUT_PATH = Config.OUTPUT_PATH + os.sep + Config.VER\n",
    "# Path(VERSION_OUTPUT_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZGH1DnPDuHX9"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/mnt/disks/extra_data/kaggle/ventilator_prediction/'\n",
    "# submission = pd.read_csv(Config.DATA_PATH + os.sep + 'sample_submission.csv')\n",
    "train = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TUh16yEOqdzA"
   },
   "outputs": [],
   "source": [
    "fltr_out = train['u_out'] == 0\n",
    "target = train['pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lDpiumSM0IXF"
   },
   "outputs": [],
   "source": [
    "oof_df = train[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "70wPWXT4uHX-"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "if Config.SAMPLE_TRAIN_BREATH_IDS is not None:\n",
    "    sampled = np.random.choice(train['breath_id'].unique(), size=Config.SAMPLE_TRAIN_BREATH_IDS, replace=False)\n",
    "    print(train.shape)\n",
    "    train = train[train['breath_id'].isin(sampled)].reset_index(drop=True)\n",
    "    print(train.shape)\n",
    "\n",
    "if Config.SAMPLE_TEST_BREATH_IDS is not None:\n",
    "    sampled = np.random.choice(test['breath_id'].unique(), size=Config.SAMPLE_TEST_BREATH_IDS, replace=False)\n",
    "    test = test[test['breath_id'].isin(sampled)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n9FI4fk79pZB"
   },
   "outputs": [],
   "source": [
    "train['bidc'] = train.groupby('breath_id').cumcount() + 1\n",
    "test['bidc'] = test.groupby('breath_id').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GHCIQsLX-c2W"
   },
   "outputs": [],
   "source": [
    "for df_tmp in [train, test]:\n",
    "    df_tmp['R_cat'] = df_tmp['R'].map({20: 1, 50: 2, 5: 0})\n",
    "    df_tmp['C_cat'] = df_tmp['C'].map({20: 1, 50: 2, 10: 0})\n",
    "    df_tmp['RC_cat'] = df_tmp['R'].astype('str')+'-' + df_tmp['C'].astype('str')\n",
    "\n",
    "mapper = pd.Series(index=train['RC_cat'].unique(), data=np.arange(train['RC_cat'].nunique()))\n",
    "for df_tmp in [train, test]:\n",
    "    df_tmp['RC_cat'] = df_tmp['RC_cat'].map(mapper)\n",
    "\n",
    "\n",
    "cat_cols = ['R_cat', 'C_cat', 'RC_cat']\n",
    "cat_cols_unq_dct = {}\n",
    "for c in cat_cols:\n",
    "    cat_cols_unq_dct[c] = train[c].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0LYWGWcRfBGl"
   },
   "outputs": [],
   "source": [
    "# c = 'u_in'\n",
    "# LAG_WINDOW_RANGE = range(3)\n",
    "# train = train.assign(**{f'{c}_t-{t}': train.groupby('breath_id')[c].shift(t) for t in LAG_WINDOW_RANGE})\n",
    "# test = test.assign(**{f'{c}_t-{t}': test.groupby('breath_id')[c].shift(t) for t in LAG_WINDOW_RANGE})\n",
    "# use_fts = [f'{c}_t-{t}' for t in LAG_WINDOW_RANGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "      <th>bidc</th>\n",
       "      <th>R_cat</th>\n",
       "      <th>C_cat</th>\n",
       "      <th>RC_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id   R   C  time_step       u_in  u_out   pressure  bidc  R_cat  \\\n",
       "0   1          1  20  50   0.000000   0.083334      0   5.837492     1      1   \n",
       "1   2          1  20  50   0.033652  18.383041      0   5.907794     2      1   \n",
       "2   3          1  20  50   0.067514  22.509278      0   7.876254     3      1   \n",
       "3   4          1  20  50   0.101542  22.808822      0  11.742872     4      1   \n",
       "4   5          1  20  50   0.135756  25.355850      0  12.234987     5      1   \n",
       "\n",
       "   C_cat  RC_cat  \n",
       "0      2       0  \n",
       "1      2       0  \n",
       "2      2       0  \n",
       "3      2       0  \n",
       "4      2       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Wimp0EV1axFc"
   },
   "outputs": [],
   "source": [
    "for df_tmp in [train]:\n",
    "    df_tmp['bidc'] = df_tmp.groupby('breath_id').cumcount()\n",
    "    df_tmp['u_in_lag_1'] = df_tmp.groupby('breath_id')['u_in'].shift(1).fillna(-999)\n",
    "    df_tmp['u_in_lag_2'] = df_tmp.groupby('breath_id')['u_in'].shift(2).fillna(-999)\n",
    "    df_tmp['u_in_lag_3'] = df_tmp.groupby('breath_id')['u_in'].shift(3).fillna(-999)\n",
    "    df_tmp['u_in_lag_4'] = df_tmp.groupby('breath_id')['u_in'].shift(4).fillna(-999)\n",
    "\n",
    "    df_tmp['u_in_cumsum'] = df_tmp.groupby('breath_id')['u_in'].cumsum()\n",
    "    df_tmp['u_in_cumsum_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(1).fillna(-999)\n",
    "    df_tmp['u_in_cumsum_lag_2'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(2).fillna(-999)\n",
    "    df_tmp['u_in_cumsum_lag_1-u_in_cumsum_lag_2'] = df_tmp['u_in_cumsum_lag_1'] - df_tmp['u_in_cumsum_lag_2']\n",
    "\n",
    "    df_tmp['u_in_cumsum_lag_3'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(3).fillna(0)\n",
    "    df_tmp['u_in_cumsum_lag_4'] = df_tmp.groupby('breath_id')['u_in_cumsum'].shift(4).fillna(0)\n",
    "\n",
    "    df_tmp['u_in_cummean'] = df_tmp['u_in_cumsum']/(df_tmp.groupby('breath_id')['u_in'].cumcount() + 1)\n",
    "    df_tmp['u_in_cummax'] = df_tmp.groupby('breath_id')['u_in'].cummax()\n",
    "    df_tmp['next_u_in'] = df_tmp.groupby('breath_id')['u_in'].shift(-1).fillna(0)\n",
    "\n",
    "    df_tmp['area'] = df_tmp['time_step'] * df_tmp['u_in']\n",
    "    df_tmp['area'] = df_tmp.groupby('breath_id')['area'].cumsum()\n",
    "    df_tmp['area_lag_1'] = df_tmp.groupby('breath_id')['area'].shift(1).fillna(0)\n",
    "    df_tmp['area_lag_2'] = df_tmp.groupby('breath_id')['area'].shift(2).fillna(0)\n",
    "    df_tmp['area_lead_1'] = df_tmp.groupby('breath_id')['area'].shift(-1).fillna(0)\n",
    "    df_tmp['area_lead_2'] = df_tmp.groupby('breath_id')['area'].shift(-2).fillna(0)\n",
    "    df_tmp['area_diff_lag_1'] = df_tmp['area'] - df_tmp['area_lag_1']\n",
    "    df_tmp['area_diff_lead_1'] = df_tmp['area'] - df_tmp['area_lead_1']\n",
    "\n",
    "\n",
    "    df_tmp['u_in_cumsum*time_step'] = df_tmp['u_in_cumsum']*df_tmp['time_step']\n",
    "    df_tmp['u_in_cumsum*time_step_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*time_step'].shift(1).fillna(0)\n",
    "    df_tmp['u_in_cumsum*time_step/c'] = df_tmp['u_in_cumsum*time_step']/df_tmp['C']\n",
    "    df_tmp['u_in_cumsum*time_step/c_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*time_step/c'].shift(1).fillna(0)\n",
    "    df_tmp['area/c'] = df_tmp['area']/df_tmp['C']\n",
    "    df_tmp['area/c_lag_1'] = df_tmp.groupby('breath_id')['area/c'].shift(1).fillna(0)\n",
    "    \n",
    "    df_tmp['u_out_lag_1'] = df_tmp.groupby('breath_id')['u_out'].shift(1).fillna(0)\n",
    "\n",
    "    df_tmp['time_step*u_out']= df_tmp['time_step']*df_tmp['u_out']\n",
    "\n",
    "    df_tmp['R+C'] = df_tmp['R'] + df_tmp['C']\n",
    "    df_tmp['R/C'] = df_tmp['R'] / df_tmp['C']\n",
    "    df_tmp['u_in/C'] = df_tmp['u_in'] / df_tmp['C']\n",
    "    df_tmp['u_in/R'] = df_tmp['u_in'] / df_tmp['R']\n",
    "    df_tmp['u_in_cumsum/C'] = df_tmp['u_in_cumsum'] / df_tmp['C']\n",
    "    df_tmp['u_in_cumsum/R'] = df_tmp['u_in_cumsum'] / df_tmp['R']\n",
    "    df_tmp['area*R/C'] = df_tmp['area'] * df_tmp['R/C']\n",
    "    df_tmp['u_in_cumsum*R/C'] = df_tmp['u_in_cumsum'] * df_tmp['R/C']\n",
    "    df_tmp['u_in_cumsum*R/C_lag_1'] = df_tmp.groupby('breath_id')['u_in_cumsum*R/C'].shift(1).fillna(0)\n",
    "\n",
    "\n",
    "    df_tmp['timestep_diff'] = (df_tmp['time_step'] - df_tmp.groupby('breath_id')['time_step'].shift(1)).fillna(0)\n",
    "    df_tmp['u_in_diff'] = (df_tmp['u_in'] - df_tmp.groupby('breath_id')['u_in'].shift(1)).fillna(0)\n",
    "    df_tmp['u_in_pct_change'] = (df_tmp['u_in_diff']/(df_tmp['u_in_lag_1'] + 1e-4)).fillna(0)\n",
    "    df_tmp['u_in_diff_next'] = (df_tmp['u_in'] - df_tmp.groupby('breath_id')['u_in'].shift(-1)).fillna(0)\n",
    "    df_tmp['u_in_log'] = np.log1p(df_tmp['u_in'])\n",
    "    df_tmp['u_in_cumsum_log'] = np.log1p(df_tmp['u_in_cumsum'])\n",
    "    df_tmp['u_in_lag_1_is_zero'] = (df_tmp['u_in_lag_1'] == 0)\n",
    "    df_tmp['u_in_zero'] = (df_tmp['u_in_lag_1'] == 0)\n",
    "    df_tmp['u_in_lead_1'] = df_tmp.groupby('breath_id')['u_in'].shift(-1)\n",
    "    df_tmp['maop'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp[df_tmp['u_in_zero']].groupby('breath_id')['bidc'].min())\n",
    "    df_tmp['spike'] = (df_tmp['u_in'] > df_tmp['u_in_lag_1']) & (df_tmp['u_in'] > df_tmp['u_in_lead_1'])\n",
    "    df_tmp['u_in_lag_1_is_zero_cumsum'] = df_tmp.groupby('breath_id')['u_in_lag_1_is_zero'].cumsum()\n",
    "    df_tmp['is_max_u_in'] = df_tmp['u_in'] == df_tmp.groupby('breath_id')['u_in'].transform('max')\n",
    "    df_tmp['nki'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp.groupby('breath_id')['u_in'].apply(np.argmax))\n",
    "    df_tmp['nki2'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp.groupby('breath_id')['u_in_cumsum'].apply(np.argmax))\n",
    "    df_tmp['nki3'] = df_tmp['nki'] - df_tmp['nki2']\n",
    "    df_tmp['nki4'] = df_tmp['bidc'] - df_tmp['breath_id'].map(df_tmp[df_tmp['u_in_lag_1_is_zero']].groupby('breath_id')['bidc'].max())\n",
    "    df_tmp['u_in_cummax - u_in'] = df_tmp['u_in_cummax'] - df_tmp['u_in']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6036000, 67)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "x6CRywriMWg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = train[train['bidc'] <= Config.SEQUENCE_LENGTH].reset_index(drop=True)\n",
    "test = test[test['bidc'] <= Config.SEQUENCE_LENGTH].reset_index(drop=True)\n",
    "\n",
    "sample_oof_df = train[['id']].copy()\n",
    "sample_preds_df = test[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3018000, 67)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X_cCw2zK5FbE"
   },
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_wqzAOiconq",
    "outputId": "8021ce72-23c2-4c9f-8762-81a1798e76ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:14<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "num_cols = [c for c in train.columns if c not in ['id', 'pressure', 'breath_id', 'u_out'] + cat_cols]\n",
    "\n",
    "train[num_cols] = train[num_cols].fillna(0)\n",
    "# test[num_cols] = test[num_cols].fillna(0)\n",
    "\n",
    "for c in tqdm(num_cols):\n",
    "    RS = StandardScaler()\n",
    "    train[c] = RS.fit_transform(train[[c]])[:, 0]\n",
    "#     test[c] = RS.transform(test[[c]])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "      <th>bidc</th>\n",
       "      <th>R_cat</th>\n",
       "      <th>C_cat</th>\n",
       "      <th>RC_cat</th>\n",
       "      <th>u_in_lag_1</th>\n",
       "      <th>u_in_lag_2</th>\n",
       "      <th>u_in_lag_3</th>\n",
       "      <th>u_in_lag_4</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "      <th>u_in_cumsum_lag_1</th>\n",
       "      <th>u_in_cumsum_lag_2</th>\n",
       "      <th>u_in_cumsum_lag_1-u_in_cumsum_lag_2</th>\n",
       "      <th>u_in_cumsum_lag_3</th>\n",
       "      <th>u_in_cumsum_lag_4</th>\n",
       "      <th>u_in_cummean</th>\n",
       "      <th>u_in_cummax</th>\n",
       "      <th>next_u_in</th>\n",
       "      <th>area</th>\n",
       "      <th>area_lag_1</th>\n",
       "      <th>area_lag_2</th>\n",
       "      <th>area_lead_1</th>\n",
       "      <th>area_lead_2</th>\n",
       "      <th>area_diff_lag_1</th>\n",
       "      <th>area_diff_lead_1</th>\n",
       "      <th>u_in_cumsum*time_step</th>\n",
       "      <th>u_in_cumsum*time_step_lag_1</th>\n",
       "      <th>u_in_cumsum*time_step/c</th>\n",
       "      <th>u_in_cumsum*time_step/c_lag_1</th>\n",
       "      <th>area/c</th>\n",
       "      <th>area/c_lag_1</th>\n",
       "      <th>u_out_lag_1</th>\n",
       "      <th>time_step*u_out</th>\n",
       "      <th>R+C</th>\n",
       "      <th>R/C</th>\n",
       "      <th>u_in/C</th>\n",
       "      <th>u_in/R</th>\n",
       "      <th>u_in_cumsum/C</th>\n",
       "      <th>u_in_cumsum/R</th>\n",
       "      <th>area*R/C</th>\n",
       "      <th>u_in_cumsum*R/C</th>\n",
       "      <th>u_in_cumsum*R/C_lag_1</th>\n",
       "      <th>timestep_diff</th>\n",
       "      <th>u_in_diff</th>\n",
       "      <th>u_in_pct_change</th>\n",
       "      <th>u_in_diff_next</th>\n",
       "      <th>u_in_log</th>\n",
       "      <th>u_in_cumsum_log</th>\n",
       "      <th>u_in_lag_1_is_zero</th>\n",
       "      <th>u_in_zero</th>\n",
       "      <th>u_in_lead_1</th>\n",
       "      <th>maop</th>\n",
       "      <th>spike</th>\n",
       "      <th>u_in_lag_1_is_zero_cumsum</th>\n",
       "      <th>is_max_u_in</th>\n",
       "      <th>nki</th>\n",
       "      <th>nki2</th>\n",
       "      <th>nki3</th>\n",
       "      <th>nki4</th>\n",
       "      <th>u_in_cummax - u_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.685675</td>\n",
       "      <td>-0.614677</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "      <td>-1.689278</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.204765</td>\n",
       "      <td>-4.344579</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.857938</td>\n",
       "      <td>-3.118732</td>\n",
       "      <td>-2.763682</td>\n",
       "      <td>-0.229643</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.920852</td>\n",
       "      <td>-1.068380</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>-0.683444</td>\n",
       "      <td>-0.665904</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.697024</td>\n",
       "      <td>-0.704757</td>\n",
       "      <td>-0.566897</td>\n",
       "      <td>0.491921</td>\n",
       "      <td>-0.685961</td>\n",
       "      <td>-0.670334</td>\n",
       "      <td>-0.775451</td>\n",
       "      <td>-0.757956</td>\n",
       "      <td>-0.809349</td>\n",
       "      <td>-0.793041</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.543888</td>\n",
       "      <td>-0.422895</td>\n",
       "      <td>-0.964948</td>\n",
       "      <td>-0.522067</td>\n",
       "      <td>-0.604429</td>\n",
       "      <td>-0.733632</td>\n",
       "      <td>-0.720441</td>\n",
       "      <td>-6.080090</td>\n",
       "      <td>0.032266</td>\n",
       "      <td>-0.108135</td>\n",
       "      <td>-1.812876</td>\n",
       "      <td>-1.064578</td>\n",
       "      <td>-3.458379</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>-1.923327</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.695780</td>\n",
       "      <td>-1.683736</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.448229</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.597761</td>\n",
       "      <td>0.397359</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "      <td>-1.602648</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.087395</td>\n",
       "      <td>-4.344579</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.806998</td>\n",
       "      <td>-0.670142</td>\n",
       "      <td>-2.763682</td>\n",
       "      <td>6.115873</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.412099</td>\n",
       "      <td>-0.494947</td>\n",
       "      <td>0.656989</td>\n",
       "      <td>-0.679151</td>\n",
       "      <td>-0.665904</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.686688</td>\n",
       "      <td>-0.689295</td>\n",
       "      <td>-0.490275</td>\n",
       "      <td>0.380639</td>\n",
       "      <td>-0.684250</td>\n",
       "      <td>-0.670334</td>\n",
       "      <td>-0.774619</td>\n",
       "      <td>-0.757956</td>\n",
       "      <td>-0.806976</td>\n",
       "      <td>-0.793041</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.159510</td>\n",
       "      <td>-0.116007</td>\n",
       "      <td>-0.940831</td>\n",
       "      <td>-0.508587</td>\n",
       "      <td>-0.602902</td>\n",
       "      <td>-0.717240</td>\n",
       "      <td>-0.720365</td>\n",
       "      <td>0.261680</td>\n",
       "      <td>1.813803</td>\n",
       "      <td>-0.103604</td>\n",
       "      <td>-0.433589</td>\n",
       "      <td>0.995823</td>\n",
       "      <td>-1.442889</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.656989</td>\n",
       "      <td>-1.866924</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.652397</td>\n",
       "      <td>-1.597482</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.373675</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.509299</td>\n",
       "      <td>0.625553</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "      <td>-1.516018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202645</td>\n",
       "      <td>0.175893</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.744623</td>\n",
       "      <td>-0.625088</td>\n",
       "      <td>-0.530394</td>\n",
       "      <td>-0.112886</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.166038</td>\n",
       "      <td>-0.365648</td>\n",
       "      <td>0.673895</td>\n",
       "      <td>-0.668607</td>\n",
       "      <td>-0.661518</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.670936</td>\n",
       "      <td>-0.666313</td>\n",
       "      <td>-0.378672</td>\n",
       "      <td>0.282288</td>\n",
       "      <td>-0.678342</td>\n",
       "      <td>-0.668559</td>\n",
       "      <td>-0.771747</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-0.801147</td>\n",
       "      <td>-0.790600</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.072840</td>\n",
       "      <td>-0.046809</td>\n",
       "      <td>-0.911302</td>\n",
       "      <td>-0.492083</td>\n",
       "      <td>-0.599152</td>\n",
       "      <td>-0.697168</td>\n",
       "      <td>-0.703649</td>\n",
       "      <td>0.301218</td>\n",
       "      <td>0.433968</td>\n",
       "      <td>-0.108130</td>\n",
       "      <td>-0.061195</td>\n",
       "      <td>1.133688</td>\n",
       "      <td>-0.906754</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.673895</td>\n",
       "      <td>-1.810520</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.609014</td>\n",
       "      <td>-1.511229</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.299121</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.420404</td>\n",
       "      <td>0.642119</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "      <td>-1.429389</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228632</td>\n",
       "      <td>0.258692</td>\n",
       "      <td>0.239309</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.681419</td>\n",
       "      <td>-0.569921</td>\n",
       "      <td>-0.489302</td>\n",
       "      <td>-0.086679</td>\n",
       "      <td>-0.795088</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.038844</td>\n",
       "      <td>-0.356262</td>\n",
       "      <td>0.817648</td>\n",
       "      <td>-0.652538</td>\n",
       "      <td>-0.650744</td>\n",
       "      <td>-0.644113</td>\n",
       "      <td>-0.647525</td>\n",
       "      <td>-0.635429</td>\n",
       "      <td>-0.280038</td>\n",
       "      <td>0.143209</td>\n",
       "      <td>-0.668124</td>\n",
       "      <td>-0.662430</td>\n",
       "      <td>-0.766780</td>\n",
       "      <td>-0.754114</td>\n",
       "      <td>-0.792264</td>\n",
       "      <td>-0.784604</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.066548</td>\n",
       "      <td>-0.041786</td>\n",
       "      <td>-0.881379</td>\n",
       "      <td>-0.475358</td>\n",
       "      <td>-0.593436</td>\n",
       "      <td>-0.676829</td>\n",
       "      <td>-0.683180</td>\n",
       "      <td>0.332444</td>\n",
       "      <td>0.061427</td>\n",
       "      <td>-0.108135</td>\n",
       "      <td>-0.279908</td>\n",
       "      <td>1.142732</td>\n",
       "      <td>-0.603956</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.817648</td>\n",
       "      <td>-1.754116</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.565631</td>\n",
       "      <td>-1.424975</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.224567</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.331024</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "      <td>-1.342759</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230518</td>\n",
       "      <td>0.277362</td>\n",
       "      <td>0.307875</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>-0.611156</td>\n",
       "      <td>-0.514021</td>\n",
       "      <td>-0.438986</td>\n",
       "      <td>-0.084776</td>\n",
       "      <td>-0.742144</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>0.065797</td>\n",
       "      <td>-0.276449</td>\n",
       "      <td>0.925109</td>\n",
       "      <td>-0.628654</td>\n",
       "      <td>-0.634324</td>\n",
       "      <td>-0.633084</td>\n",
       "      <td>-0.616063</td>\n",
       "      <td>-0.598535</td>\n",
       "      <td>-0.140558</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>-0.652634</td>\n",
       "      <td>-0.651829</td>\n",
       "      <td>-0.759250</td>\n",
       "      <td>-0.748961</td>\n",
       "      <td>-0.779062</td>\n",
       "      <td>-0.775466</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>-0.848115</td>\n",
       "      <td>-0.456766</td>\n",
       "      <td>-0.584940</td>\n",
       "      <td>-0.654218</td>\n",
       "      <td>-0.662439</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.280227</td>\n",
       "      <td>-0.108133</td>\n",
       "      <td>-0.217334</td>\n",
       "      <td>1.215333</td>\n",
       "      <td>-0.373495</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.925109</td>\n",
       "      <td>-1.697712</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.522247</td>\n",
       "      <td>-1.338721</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.150013</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id         R         C  time_step      u_in  u_out   pressure  \\\n",
       "0   1          1 -0.359072  1.394522  -1.685675 -0.614677      0   5.837492   \n",
       "1   2          1 -0.359072  1.394522  -1.597761  0.397359      0   5.907794   \n",
       "2   3          1 -0.359072  1.394522  -1.509299  0.625553      0   7.876254   \n",
       "3   4          1 -0.359072  1.394522  -1.420404  0.642119      0  11.742872   \n",
       "4   5          1 -0.359072  1.394522  -1.331024  0.782979      0  12.234987   \n",
       "\n",
       "       bidc  R_cat  C_cat  RC_cat  u_in_lag_1  u_in_lag_2  u_in_lag_3  \\\n",
       "0 -1.689278      1      2       0   -6.204765   -4.344579   -3.504065   \n",
       "1 -1.602648      1      2       0    0.087395   -4.344579   -3.504065   \n",
       "2 -1.516018      1      2       0    0.202645    0.175893   -3.504065   \n",
       "3 -1.429389      1      2       0    0.228632    0.258692    0.239309   \n",
       "4 -1.342759      1      2       0    0.230518    0.277362    0.307875   \n",
       "\n",
       "   u_in_lag_4  u_in_cumsum  u_in_cumsum_lag_1  u_in_cumsum_lag_2  \\\n",
       "0   -2.994913    -0.857938          -3.118732          -2.763682   \n",
       "1   -2.994913    -0.806998          -0.670142          -2.763682   \n",
       "2   -2.994913    -0.744623          -0.625088          -0.530394   \n",
       "3   -2.994913    -0.681419          -0.569921          -0.489302   \n",
       "4    0.292417    -0.611156          -0.514021          -0.438986   \n",
       "\n",
       "   u_in_cumsum_lag_1-u_in_cumsum_lag_2  u_in_cumsum_lag_3  u_in_cumsum_lag_4  \\\n",
       "0                            -0.229643          -0.795328          -0.775243   \n",
       "1                             6.115873          -0.795328          -0.775243   \n",
       "2                            -0.112886          -0.795328          -0.775243   \n",
       "3                            -0.086679          -0.795088          -0.775243   \n",
       "4                            -0.084776          -0.742144          -0.775000   \n",
       "\n",
       "   u_in_cummean  u_in_cummax  next_u_in      area  area_lag_1  area_lag_2  \\\n",
       "0     -0.920852    -1.068380   0.424106 -0.683444   -0.665904   -0.648603   \n",
       "1     -0.412099    -0.494947   0.656989 -0.679151   -0.665904   -0.648603   \n",
       "2     -0.166038    -0.365648   0.673895 -0.668607   -0.661518   -0.648603   \n",
       "3     -0.038844    -0.356262   0.817648 -0.652538   -0.650744   -0.644113   \n",
       "4      0.065797    -0.276449   0.925109 -0.628654   -0.634324   -0.633084   \n",
       "\n",
       "   area_lead_1  area_lead_2  area_diff_lag_1  area_diff_lead_1  \\\n",
       "0    -0.697024    -0.704757        -0.566897          0.491921   \n",
       "1    -0.686688    -0.689295        -0.490275          0.380639   \n",
       "2    -0.670936    -0.666313        -0.378672          0.282288   \n",
       "3    -0.647525    -0.635429        -0.280038          0.143209   \n",
       "4    -0.616063    -0.598535        -0.140558         -0.002983   \n",
       "\n",
       "   u_in_cumsum*time_step  u_in_cumsum*time_step_lag_1  \\\n",
       "0              -0.685961                    -0.670334   \n",
       "1              -0.684250                    -0.670334   \n",
       "2              -0.678342                    -0.668559   \n",
       "3              -0.668124                    -0.662430   \n",
       "4              -0.652634                    -0.651829   \n",
       "\n",
       "   u_in_cumsum*time_step/c  u_in_cumsum*time_step/c_lag_1    area/c  \\\n",
       "0                -0.775451                      -0.757956 -0.809349   \n",
       "1                -0.774619                      -0.757956 -0.806976   \n",
       "2                -0.771747                      -0.757093 -0.801147   \n",
       "3                -0.766780                      -0.754114 -0.792264   \n",
       "4                -0.759250                      -0.748961 -0.779062   \n",
       "\n",
       "   area/c_lag_1  u_out_lag_1  time_step*u_out       R+C       R/C    u_in/C  \\\n",
       "0     -0.793041    -0.524734        -0.560718  0.681566 -0.732796 -0.543888   \n",
       "1     -0.793041    -0.524734        -0.560718  0.681566 -0.732796 -0.159510   \n",
       "2     -0.790600    -0.524734        -0.560718  0.681566 -0.732796 -0.072840   \n",
       "3     -0.784604    -0.524734        -0.560718  0.681566 -0.732796 -0.066548   \n",
       "4     -0.775466    -0.524734        -0.560718  0.681566 -0.732796 -0.013049   \n",
       "\n",
       "     u_in/R  u_in_cumsum/C  u_in_cumsum/R  area*R/C  u_in_cumsum*R/C  \\\n",
       "0 -0.422895      -0.964948      -0.522067 -0.604429        -0.733632   \n",
       "1 -0.116007      -0.940831      -0.508587 -0.602902        -0.717240   \n",
       "2 -0.046809      -0.911302      -0.492083 -0.599152        -0.697168   \n",
       "3 -0.041786      -0.881379      -0.475358 -0.593436        -0.676829   \n",
       "4  0.000928      -0.848115      -0.456766 -0.584940        -0.654218   \n",
       "\n",
       "   u_in_cumsum*R/C_lag_1  timestep_diff  u_in_diff  u_in_pct_change  \\\n",
       "0              -0.720441      -6.080090   0.032266        -0.108135   \n",
       "1              -0.720365       0.261680   1.813803        -0.103604   \n",
       "2              -0.703649       0.301218   0.433968        -0.108130   \n",
       "3              -0.683180       0.332444   0.061427        -0.108135   \n",
       "4              -0.662439       0.367400   0.280227        -0.108133   \n",
       "\n",
       "   u_in_diff_next  u_in_log  u_in_cumsum_log  u_in_lag_1_is_zero  u_in_zero  \\\n",
       "0       -1.812876 -1.064578        -3.458379            -0.67504   -0.67504   \n",
       "1       -0.433589  0.995823        -1.442889            -0.67504   -0.67504   \n",
       "2       -0.061195  1.133688        -0.906754            -0.67504   -0.67504   \n",
       "3       -0.279908  1.142732        -0.603956            -0.67504   -0.67504   \n",
       "4       -0.217334  1.215333        -0.373495            -0.67504   -0.67504   \n",
       "\n",
       "   u_in_lead_1      maop     spike  u_in_lag_1_is_zero_cumsum  is_max_u_in  \\\n",
       "0     0.424106 -1.923327 -0.380129                  -0.660098     -0.16961   \n",
       "1     0.656989 -1.866924 -0.380129                  -0.660098     -0.16961   \n",
       "2     0.673895 -1.810520 -0.380129                  -0.660098     -0.16961   \n",
       "3     0.817648 -1.754116 -0.380129                  -0.660098     -0.16961   \n",
       "4     0.925109 -1.697712 -0.380129                  -0.660098     -0.16961   \n",
       "\n",
       "        nki      nki2     nki3      nki4  u_in_cummax - u_in  \n",
       "0 -0.695780 -1.683736  0.17441 -1.448229           -0.776598  \n",
       "1 -0.652397 -1.597482  0.17441 -1.373675           -0.776598  \n",
       "2 -0.609014 -1.511229  0.17441 -1.299121           -0.776598  \n",
       "3 -0.565631 -1.424975  0.17441 -1.224567           -0.776598  \n",
       "4 -0.522247 -1.338721  0.17441 -1.150013           -0.776598  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_pickle('../data/nm_processed_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "      <th>R_cat</th>\n",
       "      <th>C_cat</th>\n",
       "      <th>RC_cat</th>\n",
       "      <th>bidc</th>\n",
       "      <th>u_in_lag_1</th>\n",
       "      <th>u_in_lag_2</th>\n",
       "      <th>u_in_lag_3</th>\n",
       "      <th>u_in_lag_4</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "      <th>u_in_cumsum_lag_1</th>\n",
       "      <th>u_in_cumsum_lag_2</th>\n",
       "      <th>u_in_cumsum_lag_1-u_in_cumsum_lag_2</th>\n",
       "      <th>u_in_cumsum_lag_3</th>\n",
       "      <th>u_in_cumsum_lag_4</th>\n",
       "      <th>u_in_cummean</th>\n",
       "      <th>u_in_cummax</th>\n",
       "      <th>next_u_in</th>\n",
       "      <th>area</th>\n",
       "      <th>area_lag_1</th>\n",
       "      <th>area_lag_2</th>\n",
       "      <th>area_lead_1</th>\n",
       "      <th>area_lead_2</th>\n",
       "      <th>area_diff_lag_1</th>\n",
       "      <th>area_diff_lead_1</th>\n",
       "      <th>u_in_cumsum*time_step</th>\n",
       "      <th>u_in_cumsum*time_step_lag_1</th>\n",
       "      <th>u_in_cumsum*time_step/c</th>\n",
       "      <th>u_in_cumsum*time_step/c_lag_1</th>\n",
       "      <th>area/c</th>\n",
       "      <th>area/c_lag_1</th>\n",
       "      <th>u_out_lag_1</th>\n",
       "      <th>time_step*u_out</th>\n",
       "      <th>R+C</th>\n",
       "      <th>R/C</th>\n",
       "      <th>u_in/C</th>\n",
       "      <th>u_in/R</th>\n",
       "      <th>u_in_cumsum/C</th>\n",
       "      <th>u_in_cumsum/R</th>\n",
       "      <th>area*R/C</th>\n",
       "      <th>u_in_cumsum*R/C</th>\n",
       "      <th>u_in_cumsum*R/C_lag_1</th>\n",
       "      <th>timestep_diff</th>\n",
       "      <th>u_in_diff</th>\n",
       "      <th>u_in_pct_change</th>\n",
       "      <th>u_in_diff_next</th>\n",
       "      <th>u_in_log</th>\n",
       "      <th>u_in_cumsum_log</th>\n",
       "      <th>u_in_lag_1_is_zero</th>\n",
       "      <th>u_in_zero</th>\n",
       "      <th>u_in_lead_1</th>\n",
       "      <th>maop</th>\n",
       "      <th>spike</th>\n",
       "      <th>u_in_lag_1_is_zero_cumsum</th>\n",
       "      <th>is_max_u_in</th>\n",
       "      <th>nki</th>\n",
       "      <th>nki2</th>\n",
       "      <th>nki3</th>\n",
       "      <th>nki4</th>\n",
       "      <th>u_in_cummax - u_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.685675</td>\n",
       "      <td>-0.614677</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.689278</td>\n",
       "      <td>-6.204765</td>\n",
       "      <td>-4.344579</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.857938</td>\n",
       "      <td>-3.118732</td>\n",
       "      <td>-2.763682</td>\n",
       "      <td>-0.229643</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.920852</td>\n",
       "      <td>-1.068380</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>-0.683444</td>\n",
       "      <td>-0.665904</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.697024</td>\n",
       "      <td>-0.704757</td>\n",
       "      <td>-0.566897</td>\n",
       "      <td>0.491921</td>\n",
       "      <td>-0.685961</td>\n",
       "      <td>-0.670334</td>\n",
       "      <td>-0.775451</td>\n",
       "      <td>-0.757956</td>\n",
       "      <td>-0.809349</td>\n",
       "      <td>-0.793041</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.543888</td>\n",
       "      <td>-0.422895</td>\n",
       "      <td>-0.964948</td>\n",
       "      <td>-0.522067</td>\n",
       "      <td>-0.604429</td>\n",
       "      <td>-0.733632</td>\n",
       "      <td>-0.720441</td>\n",
       "      <td>-6.080090</td>\n",
       "      <td>0.032266</td>\n",
       "      <td>-0.108135</td>\n",
       "      <td>-1.812876</td>\n",
       "      <td>-1.064578</td>\n",
       "      <td>-3.458379</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.424106</td>\n",
       "      <td>-1.923327</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.695780</td>\n",
       "      <td>-1.683736</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.448229</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.597761</td>\n",
       "      <td>0.397359</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.602648</td>\n",
       "      <td>0.087395</td>\n",
       "      <td>-4.344579</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.806998</td>\n",
       "      <td>-0.670142</td>\n",
       "      <td>-2.763682</td>\n",
       "      <td>6.115873</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.412099</td>\n",
       "      <td>-0.494947</td>\n",
       "      <td>0.656989</td>\n",
       "      <td>-0.679151</td>\n",
       "      <td>-0.665904</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.686688</td>\n",
       "      <td>-0.689295</td>\n",
       "      <td>-0.490275</td>\n",
       "      <td>0.380639</td>\n",
       "      <td>-0.684250</td>\n",
       "      <td>-0.670334</td>\n",
       "      <td>-0.774619</td>\n",
       "      <td>-0.757956</td>\n",
       "      <td>-0.806976</td>\n",
       "      <td>-0.793041</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.159510</td>\n",
       "      <td>-0.116007</td>\n",
       "      <td>-0.940831</td>\n",
       "      <td>-0.508587</td>\n",
       "      <td>-0.602902</td>\n",
       "      <td>-0.717240</td>\n",
       "      <td>-0.720365</td>\n",
       "      <td>0.261680</td>\n",
       "      <td>1.813803</td>\n",
       "      <td>-0.103604</td>\n",
       "      <td>-0.433589</td>\n",
       "      <td>0.995823</td>\n",
       "      <td>-1.442889</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.656989</td>\n",
       "      <td>-1.866924</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.652397</td>\n",
       "      <td>-1.597482</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.373675</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.509299</td>\n",
       "      <td>0.625553</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.516018</td>\n",
       "      <td>0.202645</td>\n",
       "      <td>0.175893</td>\n",
       "      <td>-3.504065</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.744623</td>\n",
       "      <td>-0.625088</td>\n",
       "      <td>-0.530394</td>\n",
       "      <td>-0.112886</td>\n",
       "      <td>-0.795328</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.166038</td>\n",
       "      <td>-0.365648</td>\n",
       "      <td>0.673895</td>\n",
       "      <td>-0.668607</td>\n",
       "      <td>-0.661518</td>\n",
       "      <td>-0.648603</td>\n",
       "      <td>-0.670936</td>\n",
       "      <td>-0.666313</td>\n",
       "      <td>-0.378672</td>\n",
       "      <td>0.282288</td>\n",
       "      <td>-0.678342</td>\n",
       "      <td>-0.668559</td>\n",
       "      <td>-0.771747</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-0.801147</td>\n",
       "      <td>-0.790600</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.072840</td>\n",
       "      <td>-0.046809</td>\n",
       "      <td>-0.911302</td>\n",
       "      <td>-0.492083</td>\n",
       "      <td>-0.599152</td>\n",
       "      <td>-0.697168</td>\n",
       "      <td>-0.703649</td>\n",
       "      <td>0.301218</td>\n",
       "      <td>0.433968</td>\n",
       "      <td>-0.108130</td>\n",
       "      <td>-0.061195</td>\n",
       "      <td>1.133688</td>\n",
       "      <td>-0.906754</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.673895</td>\n",
       "      <td>-1.810520</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.609014</td>\n",
       "      <td>-1.511229</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.299121</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.420404</td>\n",
       "      <td>0.642119</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.429389</td>\n",
       "      <td>0.228632</td>\n",
       "      <td>0.258692</td>\n",
       "      <td>0.239309</td>\n",
       "      <td>-2.994913</td>\n",
       "      <td>-0.681419</td>\n",
       "      <td>-0.569921</td>\n",
       "      <td>-0.489302</td>\n",
       "      <td>-0.086679</td>\n",
       "      <td>-0.795088</td>\n",
       "      <td>-0.775243</td>\n",
       "      <td>-0.038844</td>\n",
       "      <td>-0.356262</td>\n",
       "      <td>0.817648</td>\n",
       "      <td>-0.652538</td>\n",
       "      <td>-0.650744</td>\n",
       "      <td>-0.644113</td>\n",
       "      <td>-0.647525</td>\n",
       "      <td>-0.635429</td>\n",
       "      <td>-0.280038</td>\n",
       "      <td>0.143209</td>\n",
       "      <td>-0.668124</td>\n",
       "      <td>-0.662430</td>\n",
       "      <td>-0.766780</td>\n",
       "      <td>-0.754114</td>\n",
       "      <td>-0.792264</td>\n",
       "      <td>-0.784604</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.066548</td>\n",
       "      <td>-0.041786</td>\n",
       "      <td>-0.881379</td>\n",
       "      <td>-0.475358</td>\n",
       "      <td>-0.593436</td>\n",
       "      <td>-0.676829</td>\n",
       "      <td>-0.683180</td>\n",
       "      <td>0.332444</td>\n",
       "      <td>0.061427</td>\n",
       "      <td>-0.108135</td>\n",
       "      <td>-0.279908</td>\n",
       "      <td>1.142732</td>\n",
       "      <td>-0.603956</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.817648</td>\n",
       "      <td>-1.754116</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.565631</td>\n",
       "      <td>-1.424975</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.224567</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.359072</td>\n",
       "      <td>1.394522</td>\n",
       "      <td>-1.331024</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.342759</td>\n",
       "      <td>0.230518</td>\n",
       "      <td>0.277362</td>\n",
       "      <td>0.307875</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>-0.611156</td>\n",
       "      <td>-0.514021</td>\n",
       "      <td>-0.438986</td>\n",
       "      <td>-0.084776</td>\n",
       "      <td>-0.742144</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>0.065797</td>\n",
       "      <td>-0.276449</td>\n",
       "      <td>0.925109</td>\n",
       "      <td>-0.628654</td>\n",
       "      <td>-0.634324</td>\n",
       "      <td>-0.633084</td>\n",
       "      <td>-0.616063</td>\n",
       "      <td>-0.598535</td>\n",
       "      <td>-0.140558</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>-0.652634</td>\n",
       "      <td>-0.651829</td>\n",
       "      <td>-0.759250</td>\n",
       "      <td>-0.748961</td>\n",
       "      <td>-0.779062</td>\n",
       "      <td>-0.775466</td>\n",
       "      <td>-0.524734</td>\n",
       "      <td>-0.560718</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>-0.732796</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>-0.848115</td>\n",
       "      <td>-0.456766</td>\n",
       "      <td>-0.584940</td>\n",
       "      <td>-0.654218</td>\n",
       "      <td>-0.662439</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.280227</td>\n",
       "      <td>-0.108133</td>\n",
       "      <td>-0.217334</td>\n",
       "      <td>1.215333</td>\n",
       "      <td>-0.373495</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>-0.67504</td>\n",
       "      <td>0.925109</td>\n",
       "      <td>-1.697712</td>\n",
       "      <td>-0.380129</td>\n",
       "      <td>-0.660098</td>\n",
       "      <td>-0.16961</td>\n",
       "      <td>-0.522247</td>\n",
       "      <td>-1.338721</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>-1.150013</td>\n",
       "      <td>-0.776598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id         R         C  time_step      u_in  u_out  pressure  \\\n",
       "0   1          1 -0.359072  1.394522  -1.685675 -0.614677      0       110   \n",
       "1   2          1 -0.359072  1.394522  -1.597761  0.397359      0       111   \n",
       "2   3          1 -0.359072  1.394522  -1.509299  0.625553      0       139   \n",
       "3   4          1 -0.359072  1.394522  -1.420404  0.642119      0       194   \n",
       "4   5          1 -0.359072  1.394522  -1.331024  0.782979      0       201   \n",
       "\n",
       "   R_cat  C_cat  RC_cat      bidc  u_in_lag_1  u_in_lag_2  u_in_lag_3  \\\n",
       "0      1      2       0 -1.689278   -6.204765   -4.344579   -3.504065   \n",
       "1      1      2       0 -1.602648    0.087395   -4.344579   -3.504065   \n",
       "2      1      2       0 -1.516018    0.202645    0.175893   -3.504065   \n",
       "3      1      2       0 -1.429389    0.228632    0.258692    0.239309   \n",
       "4      1      2       0 -1.342759    0.230518    0.277362    0.307875   \n",
       "\n",
       "   u_in_lag_4  u_in_cumsum  u_in_cumsum_lag_1  u_in_cumsum_lag_2  \\\n",
       "0   -2.994913    -0.857938          -3.118732          -2.763682   \n",
       "1   -2.994913    -0.806998          -0.670142          -2.763682   \n",
       "2   -2.994913    -0.744623          -0.625088          -0.530394   \n",
       "3   -2.994913    -0.681419          -0.569921          -0.489302   \n",
       "4    0.292417    -0.611156          -0.514021          -0.438986   \n",
       "\n",
       "   u_in_cumsum_lag_1-u_in_cumsum_lag_2  u_in_cumsum_lag_3  u_in_cumsum_lag_4  \\\n",
       "0                            -0.229643          -0.795328          -0.775243   \n",
       "1                             6.115873          -0.795328          -0.775243   \n",
       "2                            -0.112886          -0.795328          -0.775243   \n",
       "3                            -0.086679          -0.795088          -0.775243   \n",
       "4                            -0.084776          -0.742144          -0.775000   \n",
       "\n",
       "   u_in_cummean  u_in_cummax  next_u_in      area  area_lag_1  area_lag_2  \\\n",
       "0     -0.920852    -1.068380   0.424106 -0.683444   -0.665904   -0.648603   \n",
       "1     -0.412099    -0.494947   0.656989 -0.679151   -0.665904   -0.648603   \n",
       "2     -0.166038    -0.365648   0.673895 -0.668607   -0.661518   -0.648603   \n",
       "3     -0.038844    -0.356262   0.817648 -0.652538   -0.650744   -0.644113   \n",
       "4      0.065797    -0.276449   0.925109 -0.628654   -0.634324   -0.633084   \n",
       "\n",
       "   area_lead_1  area_lead_2  area_diff_lag_1  area_diff_lead_1  \\\n",
       "0    -0.697024    -0.704757        -0.566897          0.491921   \n",
       "1    -0.686688    -0.689295        -0.490275          0.380639   \n",
       "2    -0.670936    -0.666313        -0.378672          0.282288   \n",
       "3    -0.647525    -0.635429        -0.280038          0.143209   \n",
       "4    -0.616063    -0.598535        -0.140558         -0.002983   \n",
       "\n",
       "   u_in_cumsum*time_step  u_in_cumsum*time_step_lag_1  \\\n",
       "0              -0.685961                    -0.670334   \n",
       "1              -0.684250                    -0.670334   \n",
       "2              -0.678342                    -0.668559   \n",
       "3              -0.668124                    -0.662430   \n",
       "4              -0.652634                    -0.651829   \n",
       "\n",
       "   u_in_cumsum*time_step/c  u_in_cumsum*time_step/c_lag_1    area/c  \\\n",
       "0                -0.775451                      -0.757956 -0.809349   \n",
       "1                -0.774619                      -0.757956 -0.806976   \n",
       "2                -0.771747                      -0.757093 -0.801147   \n",
       "3                -0.766780                      -0.754114 -0.792264   \n",
       "4                -0.759250                      -0.748961 -0.779062   \n",
       "\n",
       "   area/c_lag_1  u_out_lag_1  time_step*u_out       R+C       R/C    u_in/C  \\\n",
       "0     -0.793041    -0.524734        -0.560718  0.681566 -0.732796 -0.543888   \n",
       "1     -0.793041    -0.524734        -0.560718  0.681566 -0.732796 -0.159510   \n",
       "2     -0.790600    -0.524734        -0.560718  0.681566 -0.732796 -0.072840   \n",
       "3     -0.784604    -0.524734        -0.560718  0.681566 -0.732796 -0.066548   \n",
       "4     -0.775466    -0.524734        -0.560718  0.681566 -0.732796 -0.013049   \n",
       "\n",
       "     u_in/R  u_in_cumsum/C  u_in_cumsum/R  area*R/C  u_in_cumsum*R/C  \\\n",
       "0 -0.422895      -0.964948      -0.522067 -0.604429        -0.733632   \n",
       "1 -0.116007      -0.940831      -0.508587 -0.602902        -0.717240   \n",
       "2 -0.046809      -0.911302      -0.492083 -0.599152        -0.697168   \n",
       "3 -0.041786      -0.881379      -0.475358 -0.593436        -0.676829   \n",
       "4  0.000928      -0.848115      -0.456766 -0.584940        -0.654218   \n",
       "\n",
       "   u_in_cumsum*R/C_lag_1  timestep_diff  u_in_diff  u_in_pct_change  \\\n",
       "0              -0.720441      -6.080090   0.032266        -0.108135   \n",
       "1              -0.720365       0.261680   1.813803        -0.103604   \n",
       "2              -0.703649       0.301218   0.433968        -0.108130   \n",
       "3              -0.683180       0.332444   0.061427        -0.108135   \n",
       "4              -0.662439       0.367400   0.280227        -0.108133   \n",
       "\n",
       "   u_in_diff_next  u_in_log  u_in_cumsum_log  u_in_lag_1_is_zero  u_in_zero  \\\n",
       "0       -1.812876 -1.064578        -3.458379            -0.67504   -0.67504   \n",
       "1       -0.433589  0.995823        -1.442889            -0.67504   -0.67504   \n",
       "2       -0.061195  1.133688        -0.906754            -0.67504   -0.67504   \n",
       "3       -0.279908  1.142732        -0.603956            -0.67504   -0.67504   \n",
       "4       -0.217334  1.215333        -0.373495            -0.67504   -0.67504   \n",
       "\n",
       "   u_in_lead_1      maop     spike  u_in_lag_1_is_zero_cumsum  is_max_u_in  \\\n",
       "0     0.424106 -1.923327 -0.380129                  -0.660098     -0.16961   \n",
       "1     0.656989 -1.866924 -0.380129                  -0.660098     -0.16961   \n",
       "2     0.673895 -1.810520 -0.380129                  -0.660098     -0.16961   \n",
       "3     0.817648 -1.754116 -0.380129                  -0.660098     -0.16961   \n",
       "4     0.925109 -1.697712 -0.380129                  -0.660098     -0.16961   \n",
       "\n",
       "        nki      nki2     nki3      nki4  u_in_cummax - u_in  \n",
       "0 -0.695780 -1.683736  0.17441 -1.448229           -0.776598  \n",
       "1 -0.652397 -1.597482  0.17441 -1.373675           -0.776598  \n",
       "2 -0.609014 -1.511229  0.17441 -1.299121           -0.776598  \n",
       "3 -0.565631 -1.424975  0.17441 -1.224567           -0.776598  \n",
       "4 -0.522247 -1.338721  0.17441 -1.150013           -0.776598  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3093450, 67)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "breath_id\n",
       "1         41\n",
       "2         41\n",
       "3         41\n",
       "4         41\n",
       "5         41\n",
       "          ..\n",
       "125740    41\n",
       "125742    41\n",
       "125743    41\n",
       "125745    41\n",
       "125749    41\n",
       "Length: 75450, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('breath_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnTWaV7J5H2x"
   },
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dulPxYLFayze"
   },
   "outputs": [],
   "source": [
    "N_CLASSES = train['pressure'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhzpCGJmgPkC"
   },
   "outputs": [],
   "source": [
    "class VentilatorDataset:\n",
    "    def __init__(self, df, cat_cols, num_cols, is_train=True):\n",
    "\n",
    "        if is_train:\n",
    "            self.inv_mapper = df['pressure'].drop_duplicates().sort_values().reset_index(drop=True).to_dict()\n",
    "            self.mapper = {val: key for key, val in self.inv_mapper.items()}\n",
    "            df['pressure_int'] = df['pressure'].map(self.mapper)\n",
    "            self.pressures_int = df[['pressure_int']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "            self.pressures = df[['pressure']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "            del self.mapper\n",
    "            _ = gc.collect()\n",
    "\n",
    "        self.u_outs = df[['u_out']].to_numpy().reshape(-1, df['bidc'].nunique())\n",
    "        self.inputs = df[num_cols + cat_cols].values.reshape(-1, df['bidc'].nunique(), len(num_cols + cat_cols)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SiHBR3FuHYB",
    "outputId": "30086133-5f93-4e20-fc0b-c7909c439336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 1.61 s, total: 4.21 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data = VentilatorDataset(test, cat_cols, num_cols, is_train=False)\n",
    "del test\n",
    "_ = gc.collect()\n",
    "train_data = VentilatorDataset(train, cat_cols, num_cols, is_train=True)\n",
    "del train\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csc30ZcgiVHy",
    "outputId": "ecd8887a-7af3-4a25-da53-6c45d03eca86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75450, 40, 66), (75450, 40), (75450, 40))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.inputs.shape, train_data.pressures.shape, train_data.u_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EE6qgsxkbg-a",
    "outputId": "f02fc56d-a2fa-4859-a59f-ab670d80c664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50300, 40, 66), (50300, 40))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.inputs.shape, test_data.u_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3L8GRxQuHYC",
    "outputId": "0edfcf2d-2da4-4330-d883-c88006685b6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERICAL_INPUTS_CNT = len(num_cols)\n",
    "N_FEATS = train_data.inputs.shape[2]\n",
    "WINDOW_SIZE=train_data.inputs.shape[1]\n",
    "\n",
    "NUMERICAL_INPUTS_CNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLt6wWnwh_b-"
   },
   "outputs": [],
   "source": [
    "mapper = train_data.inv_mapper\n",
    "def map_class_to_cont(x):\n",
    "    return mapper[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVAWMoV_h_b_"
   },
   "outputs": [],
   "source": [
    "class WeightedSum(tf.keras.layers.Layer):\n",
    "    \"\"\"A custom keras layer to learn a weighted sum of tensors\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape=1):\n",
    "        self.a = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=(),\n",
    "            initializer='ones',\n",
    "            dtype='float32',\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, model_outputs):\n",
    "        return self.a * model_outputs[0] + model_outputs[1]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCwKO5nTISAL"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = inputs\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias = False,\n",
    "        dropout=Config.DROPOUT\n",
    "    )([x, x, x])\n",
    "\n",
    "    res = WeightedSum()([x, inputs])\n",
    "    res = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(inputs.shape[-1])(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    x = WeightedSum()([x, res])\n",
    "\n",
    "    return tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HS6-Hc5jJfO-"
   },
   "outputs": [],
   "source": [
    "def custom_mean_absolute_error(y_true, y_pred, w_out):\n",
    "    return K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "def custom_metric(y_true, y_pred, w_out):\n",
    "    return mae(y_true[w_out == 0], y_pred[w_out == 0])\n",
    "\n",
    "def get_LSTM_model(Config):\n",
    "    w_out = tf.keras.layers.Input(shape=(WINDOW_SIZE, 1))\n",
    "    targets = tf.keras.layers.Input(shape=(WINDOW_SIZE, 1))\n",
    "    inputs = tf.keras.layers.Input(shape=(WINDOW_SIZE, N_FEATS))\n",
    "    \n",
    "    num_inputs = inputs[:, :, :NUMERICAL_INPUTS_CNT]\n",
    "    cat_inputs = inputs[:, :, NUMERICAL_INPUTS_CNT:]\n",
    "\n",
    "    initializer = tf.keras.initializers.glorot_uniform(seed=66)\n",
    "\n",
    "    all_conv_layers = []\n",
    "    for sz in [2, 3, 5, 7, 11, 15]:\n",
    "        conv_curr = tf.keras.layers.Conv1D(16, kernel_size=(sz,), padding='same', use_bias=False)(num_inputs)\n",
    "        # conv_curr = tf.keras.layers.LayerNormalization()(conv_curr)\n",
    "        all_conv_layers.append(conv_curr)\n",
    "\n",
    "    conv_op = tf.keras.layers.Concatenate()(all_conv_layers)\n",
    "    num_inputs = tf.keras.layers.Concatenate()([num_inputs, conv_op])\n",
    "\n",
    "    for i, c in enumerate(cat_cols):\n",
    "      cat_inp = cat_inputs[:, :, i:i+1]\n",
    "      embed = tf.keras.layers.Embedding(input_dim=cat_cols_unq_dct[c]+2, output_dim=Config.CAT_DIM, embeddings_initializer=initializer)(cat_inp)\n",
    "      reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
    "      reshaped = tf.keras.layers.SpatialDropout1D(Config.EMBEDDING_DROPOUT, seed=2)(reshaped)\n",
    "      num_inputs = tf.keras.layers.Concatenate(axis=2)([reshaped, num_inputs])\n",
    "    \n",
    "    \n",
    "    x = num_inputs\n",
    "    for i in range(16):\n",
    "        x = transformer_encoder(x, 8, 128, 1024, 0)\n",
    "        x_c = tf.keras.layers.Conv1D(16, kernel_size=(3,), padding='same', use_bias=False)(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, x_c])\n",
    "\n",
    "         \n",
    "    x = tf.keras.layers.Dense(Config.DENSE_SIZE)(x)\n",
    "    x = tfa.activations.mish(x)\n",
    "    x = tf.keras.layers.Dropout(Config.DROPOUT)(x)\n",
    "    out = tf.keras.layers.Dense(N_CLASSES)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs, w_out], outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbCcFea3-kDv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preds(best_model, X_test, wt_test, STEP=5000):\n",
    "    test_preds = []\n",
    "    for start in range(0, X_test.shape[0], STEP):\n",
    "        preds = best_model.predict([X_test[start: start+STEP], wt_test[start: start+STEP]], verbose=0,\n",
    "                                   batch_size=STEP, use_multiprocessing=False)\n",
    "        preds = np.argmax(preds, axis=2)\n",
    "        test_preds.append(preds)\n",
    "\n",
    "    test_preds = np.concatenate(test_preds, axis=0)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzMCjsmhuHYD"
   },
   "outputs": [],
   "source": [
    "\n",
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data=(), interval=10, save_model=True, Config={}, fold=0):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val, self.w_out_val = validation_data\n",
    "        self.best_score = np.inf\n",
    "        self.save_model = save_model\n",
    "        self.learning_rates = pd.Series(index=np.arange(Config.N_EPOCHS),\n",
    "                                        data=np.linspace(Config.END_LR,\n",
    "                                                         Config.START_LR,\n",
    "                                                         num=Config.N_EPOCHS)[::-1]).to_dict()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.start_time = time()\n",
    "\n",
    "        if epoch == 0:\n",
    "            self.first_epoch_start_time = self.start_time\n",
    "\n",
    "        if epoch in self.learning_rates:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[epoch])\n",
    "\n",
    "        self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "\n",
    "        y_pred = get_preds(self.model, self.X_val, self.w_out_val, STEP=3000)\n",
    "        y_pred = np.vectorize(map_class_to_cont)(y_pred)\n",
    "        _ = gc.collect()\n",
    "        \n",
    "        val_score = custom_metric(self.y_val.flatten(), y_pred.flatten(), self.w_out_val.flatten())\n",
    "        \n",
    "        if val_score < self.best_score:\n",
    "            self.best_score = val_score\n",
    "            if self.save_model and (epoch == 0 or epoch > 150): ##### Saving models takes time, so don't save for the first N_EPOCHS // 2\n",
    "                tf.keras.models.save_model(self.model, f'best_model', save_format=\"h5\")\n",
    "\n",
    "        total_time = round(time() - self.start_time, 2)\n",
    "        total_seconds_till_now = round(time() - self.first_epoch_start_time, 0)\n",
    "\n",
    "        if Config.WANDB_ENABLED:\n",
    "            wandb.log({f'Fold {fold} epoch': epoch, f\"Fold {fold} train_loss\": logs['loss'], f\"Fold {fold} val_score\": val_score, f'Fold {fold} best_val_score': self.best_score, f'Fold {fold} total_time_in_seconds': total_time})\n",
    "        \n",
    "        if epoch % self.interval == 0:\n",
    "            print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.03f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time:.02f}s, total_time_elapsed t={total_seconds_till_now}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIpzPQIaFo-j"
   },
   "outputs": [],
   "source": [
    "Config_dct = {key:value for key, value in Config.__dict__.items() if not key.startswith('__') and not callable(key)}\n",
    "if Config.WANDB_ENABLED:\n",
    "    import wandb\n",
    "    wandb.login(key=Config.WANDB_API_KEY)\n",
    "    wandb.init(project=\"google-brain\",\n",
    "               name=f'nikhil_pressure_prediction_dl_{Config.VER}',\n",
    "               save_code=True,\n",
    "               allow_val_change=True,\n",
    "               config=Config_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwsjzlWzSiD0",
    "outputId": "d107bd14-f650-45e4-c5eb-5baac1bb936a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_lst = list(range(Config.START_FOLD, Config.END_FOLD+1))\n",
    "folds_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1juyeJV2UAg1"
   },
   "outputs": [],
   "source": [
    "def save_test_preds(preds, path):\n",
    "\n",
    "  global submission\n",
    " \n",
    "  sample_preds_df['pressure'] = preds.flatten()\n",
    "  submission = pd.merge(submission[['id']], sample_preds_df, on='id', how='left')\n",
    "  submission['pressure'] = submission['pressure'].fillna(0)\n",
    "  \n",
    "  submission.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9UN3WHZapzU",
    "outputId": "d10d5b8b-7716-4926-b185-03d44c71744c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.105.186.218:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.105.186.218:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 0 < --------------- \n",
      "\n",
      "Epoch: 000 curr_lr: 2.0e-04 - train_loss: 5.002 val_score: 1.398  best_val_score: 1.398  last_epoch t=313.82s, total_time_elapsed t=314.0s\n",
      "Epoch: 001 curr_lr: 2.0e-04 - train_loss: 4.044 val_score: 0.904  best_val_score: 0.904  last_epoch t=74.93s, total_time_elapsed t=389.0s\n",
      "Epoch: 002 curr_lr: 2.0e-04 - train_loss: 3.762 val_score: 0.709  best_val_score: 0.709  last_epoch t=75.35s, total_time_elapsed t=464.0s\n",
      "Epoch: 003 curr_lr: 2.0e-04 - train_loss: 3.609 val_score: 0.644  best_val_score: 0.644  last_epoch t=74.93s, total_time_elapsed t=539.0s\n",
      "Epoch: 004 curr_lr: 2.0e-04 - train_loss: 3.496 val_score: 0.577  best_val_score: 0.577  last_epoch t=75.18s, total_time_elapsed t=614.0s\n",
      "Epoch: 005 curr_lr: 2.0e-04 - train_loss: 3.415 val_score: 0.516  best_val_score: 0.516  last_epoch t=75.88s, total_time_elapsed t=690.0s\n",
      "Epoch: 006 curr_lr: 2.0e-04 - train_loss: 3.345 val_score: 0.516  best_val_score: 0.516  last_epoch t=75.22s, total_time_elapsed t=765.0s\n",
      "Epoch: 007 curr_lr: 2.0e-04 - train_loss: 3.275 val_score: 0.429  best_val_score: 0.429  last_epoch t=75.17s, total_time_elapsed t=841.0s\n",
      "Epoch: 008 curr_lr: 2.0e-04 - train_loss: 3.220 val_score: 0.457  best_val_score: 0.429  last_epoch t=75.49s, total_time_elapsed t=916.0s\n",
      "Epoch: 009 curr_lr: 2.0e-04 - train_loss: 3.169 val_score: 0.410  best_val_score: 0.410  last_epoch t=75.37s, total_time_elapsed t=992.0s\n",
      "Epoch: 010 curr_lr: 2.0e-04 - train_loss: 3.120 val_score: 0.375  best_val_score: 0.375  last_epoch t=75.03s, total_time_elapsed t=1067.0s\n",
      "Epoch: 011 curr_lr: 1.9e-04 - train_loss: 3.081 val_score: 0.380  best_val_score: 0.375  last_epoch t=75.15s, total_time_elapsed t=1142.0s\n",
      "Epoch: 012 curr_lr: 1.9e-04 - train_loss: 3.046 val_score: 0.349  best_val_score: 0.349  last_epoch t=75.03s, total_time_elapsed t=1217.0s\n",
      "Epoch: 013 curr_lr: 1.9e-04 - train_loss: 3.003 val_score: 0.347  best_val_score: 0.347  last_epoch t=75.95s, total_time_elapsed t=1293.0s\n",
      "Epoch: 014 curr_lr: 1.9e-04 - train_loss: 2.976 val_score: 0.325  best_val_score: 0.325  last_epoch t=76.40s, total_time_elapsed t=1369.0s\n",
      "Epoch: 015 curr_lr: 1.9e-04 - train_loss: 2.941 val_score: 0.318  best_val_score: 0.318  last_epoch t=75.16s, total_time_elapsed t=1444.0s\n",
      "Epoch: 016 curr_lr: 1.9e-04 - train_loss: 2.914 val_score: 0.314  best_val_score: 0.314  last_epoch t=75.07s, total_time_elapsed t=1520.0s\n",
      "Epoch: 017 curr_lr: 1.9e-04 - train_loss: 2.891 val_score: 0.314  best_val_score: 0.314  last_epoch t=75.16s, total_time_elapsed t=1595.0s\n",
      "Epoch: 018 curr_lr: 1.9e-04 - train_loss: 2.864 val_score: 0.291  best_val_score: 0.291  last_epoch t=75.31s, total_time_elapsed t=1670.0s\n",
      "Epoch: 019 curr_lr: 1.9e-04 - train_loss: 2.842 val_score: 0.306  best_val_score: 0.291  last_epoch t=75.10s, total_time_elapsed t=1745.0s\n",
      "Epoch: 020 curr_lr: 1.9e-04 - train_loss: 2.834 val_score: 0.297  best_val_score: 0.291  last_epoch t=75.06s, total_time_elapsed t=1820.0s\n",
      "Epoch: 021 curr_lr: 1.9e-04 - train_loss: 2.800 val_score: 0.290  best_val_score: 0.290  last_epoch t=75.13s, total_time_elapsed t=1895.0s\n",
      "Epoch: 022 curr_lr: 1.9e-04 - train_loss: 2.779 val_score: 0.282  best_val_score: 0.282  last_epoch t=75.35s, total_time_elapsed t=1971.0s\n",
      "Epoch: 023 curr_lr: 1.9e-04 - train_loss: 2.770 val_score: 0.280  best_val_score: 0.280  last_epoch t=74.94s, total_time_elapsed t=2046.0s\n",
      "Epoch: 024 curr_lr: 1.9e-04 - train_loss: 2.743 val_score: 0.275  best_val_score: 0.275  last_epoch t=75.24s, total_time_elapsed t=2121.0s\n",
      "Epoch: 025 curr_lr: 1.9e-04 - train_loss: 2.729 val_score: 0.270  best_val_score: 0.270  last_epoch t=75.15s, total_time_elapsed t=2196.0s\n",
      "Epoch: 026 curr_lr: 1.9e-04 - train_loss: 2.708 val_score: 0.291  best_val_score: 0.270  last_epoch t=75.05s, total_time_elapsed t=2271.0s\n",
      "Epoch: 027 curr_lr: 1.9e-04 - train_loss: 2.697 val_score: 0.253  best_val_score: 0.253  last_epoch t=75.17s, total_time_elapsed t=2346.0s\n",
      "Epoch: 028 curr_lr: 1.9e-04 - train_loss: 2.677 val_score: 0.240  best_val_score: 0.240  last_epoch t=75.06s, total_time_elapsed t=2422.0s\n",
      "Epoch: 029 curr_lr: 1.9e-04 - train_loss: 2.661 val_score: 0.249  best_val_score: 0.240  last_epoch t=75.17s, total_time_elapsed t=2497.0s\n",
      "Epoch: 030 curr_lr: 1.9e-04 - train_loss: 2.660 val_score: 0.243  best_val_score: 0.240  last_epoch t=75.05s, total_time_elapsed t=2572.0s\n",
      "Epoch: 031 curr_lr: 1.8e-04 - train_loss: 2.637 val_score: 0.239  best_val_score: 0.239  last_epoch t=75.07s, total_time_elapsed t=2647.0s\n",
      "Epoch: 032 curr_lr: 1.8e-04 - train_loss: 2.620 val_score: 0.232  best_val_score: 0.232  last_epoch t=75.24s, total_time_elapsed t=2722.0s\n",
      "Epoch: 033 curr_lr: 1.8e-04 - train_loss: 2.605 val_score: 0.239  best_val_score: 0.232  last_epoch t=75.16s, total_time_elapsed t=2797.0s\n",
      "Epoch: 034 curr_lr: 1.8e-04 - train_loss: 2.598 val_score: 0.239  best_val_score: 0.232  last_epoch t=75.07s, total_time_elapsed t=2872.0s\n",
      "Epoch: 035 curr_lr: 1.8e-04 - train_loss: 2.585 val_score: 0.220  best_val_score: 0.220  last_epoch t=75.12s, total_time_elapsed t=2948.0s\n",
      "Epoch: 036 curr_lr: 1.8e-04 - train_loss: 2.575 val_score: 0.233  best_val_score: 0.220  last_epoch t=75.05s, total_time_elapsed t=3023.0s\n",
      "Epoch: 037 curr_lr: 1.8e-04 - train_loss: 2.564 val_score: 0.218  best_val_score: 0.218  last_epoch t=75.21s, total_time_elapsed t=3098.0s\n",
      "Epoch: 038 curr_lr: 1.8e-04 - train_loss: 2.550 val_score: 0.220  best_val_score: 0.218  last_epoch t=75.71s, total_time_elapsed t=3174.0s\n",
      "Epoch: 039 curr_lr: 1.8e-04 - train_loss: 2.543 val_score: 0.213  best_val_score: 0.213  last_epoch t=75.02s, total_time_elapsed t=3249.0s\n",
      "Epoch: 040 curr_lr: 1.8e-04 - train_loss: 2.535 val_score: 0.223  best_val_score: 0.213  last_epoch t=75.16s, total_time_elapsed t=3324.0s\n",
      "Epoch: 041 curr_lr: 1.8e-04 - train_loss: 2.517 val_score: 0.220  best_val_score: 0.213  last_epoch t=75.21s, total_time_elapsed t=3399.0s\n",
      "Epoch: 042 curr_lr: 1.8e-04 - train_loss: 2.507 val_score: 0.217  best_val_score: 0.213  last_epoch t=75.33s, total_time_elapsed t=3474.0s\n",
      "Epoch: 043 curr_lr: 1.8e-04 - train_loss: 2.500 val_score: 0.206  best_val_score: 0.206  last_epoch t=75.01s, total_time_elapsed t=3549.0s\n",
      "Epoch: 044 curr_lr: 1.8e-04 - train_loss: 2.490 val_score: 0.208  best_val_score: 0.206  last_epoch t=75.27s, total_time_elapsed t=3625.0s\n",
      "Epoch: 045 curr_lr: 1.8e-04 - train_loss: 2.479 val_score: 0.204  best_val_score: 0.204  last_epoch t=75.09s, total_time_elapsed t=3700.0s\n",
      "Epoch: 046 curr_lr: 1.8e-04 - train_loss: 2.477 val_score: 0.215  best_val_score: 0.204  last_epoch t=75.25s, total_time_elapsed t=3775.0s\n",
      "Epoch: 047 curr_lr: 1.8e-04 - train_loss: 2.464 val_score: 0.200  best_val_score: 0.200  last_epoch t=75.11s, total_time_elapsed t=3850.0s\n",
      "Epoch: 048 curr_lr: 1.8e-04 - train_loss: 2.459 val_score: 0.209  best_val_score: 0.200  last_epoch t=75.07s, total_time_elapsed t=3925.0s\n",
      "Epoch: 049 curr_lr: 1.8e-04 - train_loss: 2.451 val_score: 0.193  best_val_score: 0.193  last_epoch t=75.16s, total_time_elapsed t=4001.0s\n",
      "Epoch: 050 curr_lr: 1.8e-04 - train_loss: 2.433 val_score: 0.208  best_val_score: 0.193  last_epoch t=75.16s, total_time_elapsed t=4076.0s\n",
      "Epoch: 051 curr_lr: 1.7e-04 - train_loss: 2.429 val_score: 0.202  best_val_score: 0.193  last_epoch t=75.26s, total_time_elapsed t=4151.0s\n",
      "Epoch: 052 curr_lr: 1.7e-04 - train_loss: 2.425 val_score: 0.200  best_val_score: 0.193  last_epoch t=75.18s, total_time_elapsed t=4226.0s\n",
      "Epoch: 053 curr_lr: 1.7e-04 - train_loss: 2.412 val_score: 0.219  best_val_score: 0.193  last_epoch t=75.03s, total_time_elapsed t=4301.0s\n",
      "Epoch: 054 curr_lr: 1.7e-04 - train_loss: 2.402 val_score: 0.192  best_val_score: 0.192  last_epoch t=75.15s, total_time_elapsed t=4376.0s\n",
      "Epoch: 055 curr_lr: 1.7e-04 - train_loss: 2.398 val_score: 0.191  best_val_score: 0.191  last_epoch t=75.05s, total_time_elapsed t=4451.0s\n",
      "Epoch: 056 curr_lr: 1.7e-04 - train_loss: 2.390 val_score: 0.193  best_val_score: 0.191  last_epoch t=75.02s, total_time_elapsed t=4527.0s\n",
      "Epoch: 057 curr_lr: 1.7e-04 - train_loss: 2.386 val_score: 0.195  best_val_score: 0.191  last_epoch t=75.20s, total_time_elapsed t=4602.0s\n",
      "Epoch: 058 curr_lr: 1.7e-04 - train_loss: 2.371 val_score: 0.187  best_val_score: 0.187  last_epoch t=75.02s, total_time_elapsed t=4677.0s\n",
      "Epoch: 059 curr_lr: 1.7e-04 - train_loss: 2.369 val_score: 0.194  best_val_score: 0.187  last_epoch t=75.03s, total_time_elapsed t=4752.0s\n",
      "Epoch: 060 curr_lr: 1.7e-04 - train_loss: 2.367 val_score: 0.186  best_val_score: 0.186  last_epoch t=75.02s, total_time_elapsed t=4827.0s\n",
      "Epoch: 061 curr_lr: 1.7e-04 - train_loss: 2.353 val_score: 0.190  best_val_score: 0.186  last_epoch t=75.13s, total_time_elapsed t=4902.0s\n",
      "Epoch: 062 curr_lr: 1.7e-04 - train_loss: 2.348 val_score: 0.186  best_val_score: 0.186  last_epoch t=75.05s, total_time_elapsed t=4977.0s\n",
      "Epoch: 063 curr_lr: 1.7e-04 - train_loss: 2.351 val_score: 0.194  best_val_score: 0.186  last_epoch t=75.07s, total_time_elapsed t=5052.0s\n",
      "Epoch: 064 curr_lr: 1.7e-04 - train_loss: 2.335 val_score: 0.184  best_val_score: 0.184  last_epoch t=75.00s, total_time_elapsed t=5127.0s\n",
      "Epoch: 065 curr_lr: 1.7e-04 - train_loss: 2.327 val_score: 0.181  best_val_score: 0.181  last_epoch t=76.24s, total_time_elapsed t=5204.0s\n",
      "Epoch: 066 curr_lr: 1.7e-04 - train_loss: 2.323 val_score: 0.184  best_val_score: 0.181  last_epoch t=75.19s, total_time_elapsed t=5279.0s\n",
      "Epoch: 067 curr_lr: 1.7e-04 - train_loss: 2.320 val_score: 0.198  best_val_score: 0.181  last_epoch t=75.13s, total_time_elapsed t=5354.0s\n",
      "Epoch: 068 curr_lr: 1.7e-04 - train_loss: 2.309 val_score: 0.174  best_val_score: 0.174  last_epoch t=75.15s, total_time_elapsed t=5429.0s\n",
      "Epoch: 069 curr_lr: 1.7e-04 - train_loss: 2.306 val_score: 0.177  best_val_score: 0.174  last_epoch t=75.14s, total_time_elapsed t=5504.0s\n",
      "Epoch: 070 curr_lr: 1.7e-04 - train_loss: 2.297 val_score: 0.177  best_val_score: 0.174  last_epoch t=75.13s, total_time_elapsed t=5579.0s\n",
      "Epoch: 071 curr_lr: 1.6e-04 - train_loss: 2.291 val_score: 0.174  best_val_score: 0.174  last_epoch t=75.17s, total_time_elapsed t=5655.0s\n",
      "Epoch: 072 curr_lr: 1.6e-04 - train_loss: 2.290 val_score: 0.175  best_val_score: 0.174  last_epoch t=75.06s, total_time_elapsed t=5730.0s\n",
      "Epoch: 073 curr_lr: 1.6e-04 - train_loss: 2.277 val_score: 0.179  best_val_score: 0.174  last_epoch t=76.48s, total_time_elapsed t=5806.0s\n",
      "Epoch: 074 curr_lr: 1.6e-04 - train_loss: 2.271 val_score: 0.171  best_val_score: 0.171  last_epoch t=75.10s, total_time_elapsed t=5881.0s\n",
      "Epoch: 075 curr_lr: 1.6e-04 - train_loss: 2.265 val_score: 0.177  best_val_score: 0.171  last_epoch t=75.24s, total_time_elapsed t=5957.0s\n",
      "Epoch: 076 curr_lr: 1.6e-04 - train_loss: 2.264 val_score: 0.172  best_val_score: 0.171  last_epoch t=75.05s, total_time_elapsed t=6032.0s\n",
      "Epoch: 077 curr_lr: 1.6e-04 - train_loss: 2.263 val_score: 0.173  best_val_score: 0.171  last_epoch t=75.16s, total_time_elapsed t=6107.0s\n",
      "Epoch: 078 curr_lr: 1.6e-04 - train_loss: 2.254 val_score: 0.175  best_val_score: 0.171  last_epoch t=75.05s, total_time_elapsed t=6182.0s\n",
      "Epoch: 079 curr_lr: 1.6e-04 - train_loss: 2.244 val_score: 0.173  best_val_score: 0.171  last_epoch t=75.08s, total_time_elapsed t=6257.0s\n",
      "Epoch: 080 curr_lr: 1.6e-04 - train_loss: 2.238 val_score: 0.173  best_val_score: 0.171  last_epoch t=75.28s, total_time_elapsed t=6332.0s\n",
      "Epoch: 081 curr_lr: 1.6e-04 - train_loss: 2.234 val_score: 0.172  best_val_score: 0.171  last_epoch t=76.14s, total_time_elapsed t=6408.0s\n",
      "Epoch: 082 curr_lr: 1.6e-04 - train_loss: 2.242 val_score: 0.177  best_val_score: 0.171  last_epoch t=75.51s, total_time_elapsed t=6484.0s\n",
      "Epoch: 083 curr_lr: 1.6e-04 - train_loss: 2.232 val_score: 0.168  best_val_score: 0.168  last_epoch t=75.13s, total_time_elapsed t=6559.0s\n",
      "Epoch: 084 curr_lr: 1.6e-04 - train_loss: 2.216 val_score: 0.170  best_val_score: 0.168  last_epoch t=75.09s, total_time_elapsed t=6634.0s\n",
      "Epoch: 085 curr_lr: 1.6e-04 - train_loss: 2.214 val_score: 0.173  best_val_score: 0.168  last_epoch t=75.20s, total_time_elapsed t=6709.0s\n",
      "Epoch: 086 curr_lr: 1.6e-04 - train_loss: 2.206 val_score: 0.166  best_val_score: 0.166  last_epoch t=75.12s, total_time_elapsed t=6785.0s\n",
      "Epoch: 087 curr_lr: 1.6e-04 - train_loss: 2.207 val_score: 0.164  best_val_score: 0.164  last_epoch t=75.12s, total_time_elapsed t=6860.0s\n",
      "Epoch: 088 curr_lr: 1.6e-04 - train_loss: 2.200 val_score: 0.170  best_val_score: 0.164  last_epoch t=75.03s, total_time_elapsed t=6935.0s\n",
      "Epoch: 089 curr_lr: 1.6e-04 - train_loss: 2.197 val_score: 0.166  best_val_score: 0.164  last_epoch t=76.88s, total_time_elapsed t=7012.0s\n",
      "Epoch: 090 curr_lr: 1.6e-04 - train_loss: 2.188 val_score: 0.176  best_val_score: 0.164  last_epoch t=76.33s, total_time_elapsed t=7088.0s\n",
      "Epoch: 091 curr_lr: 1.5e-04 - train_loss: 2.187 val_score: 0.160  best_val_score: 0.160  last_epoch t=75.05s, total_time_elapsed t=7163.0s\n",
      "Epoch: 092 curr_lr: 1.5e-04 - train_loss: 2.185 val_score: 0.166  best_val_score: 0.160  last_epoch t=75.22s, total_time_elapsed t=7238.0s\n",
      "Epoch: 093 curr_lr: 1.5e-04 - train_loss: 2.180 val_score: 0.165  best_val_score: 0.160  last_epoch t=75.04s, total_time_elapsed t=7313.0s\n",
      "Epoch: 094 curr_lr: 1.5e-04 - train_loss: 2.176 val_score: 0.168  best_val_score: 0.160  last_epoch t=75.20s, total_time_elapsed t=7389.0s\n",
      "Epoch: 095 curr_lr: 1.5e-04 - train_loss: 2.167 val_score: 0.159  best_val_score: 0.159  last_epoch t=75.37s, total_time_elapsed t=7464.0s\n",
      "Epoch: 096 curr_lr: 1.5e-04 - train_loss: 2.162 val_score: 0.160  best_val_score: 0.159  last_epoch t=75.21s, total_time_elapsed t=7539.0s\n",
      "Epoch: 097 curr_lr: 1.5e-04 - train_loss: 2.160 val_score: 0.160  best_val_score: 0.159  last_epoch t=75.08s, total_time_elapsed t=7614.0s\n",
      "Epoch: 098 curr_lr: 1.5e-04 - train_loss: 2.154 val_score: 0.163  best_val_score: 0.159  last_epoch t=75.48s, total_time_elapsed t=7690.0s\n",
      "Epoch: 099 curr_lr: 1.5e-04 - train_loss: 2.147 val_score: 0.161  best_val_score: 0.159  last_epoch t=75.25s, total_time_elapsed t=7765.0s\n",
      "Epoch: 100 curr_lr: 1.5e-04 - train_loss: 2.151 val_score: 0.160  best_val_score: 0.159  last_epoch t=75.14s, total_time_elapsed t=7840.0s\n",
      "Epoch: 101 curr_lr: 1.5e-04 - train_loss: 2.143 val_score: 0.159  best_val_score: 0.159  last_epoch t=75.04s, total_time_elapsed t=7915.0s\n",
      "Epoch: 102 curr_lr: 1.5e-04 - train_loss: 2.138 val_score: 0.157  best_val_score: 0.157  last_epoch t=75.57s, total_time_elapsed t=7991.0s\n",
      "Epoch: 103 curr_lr: 1.5e-04 - train_loss: 2.134 val_score: 0.154  best_val_score: 0.154  last_epoch t=75.22s, total_time_elapsed t=8066.0s\n",
      "Epoch: 104 curr_lr: 1.5e-04 - train_loss: 2.131 val_score: 0.156  best_val_score: 0.154  last_epoch t=75.19s, total_time_elapsed t=8141.0s\n",
      "Epoch: 105 curr_lr: 1.5e-04 - train_loss: 2.130 val_score: 0.154  best_val_score: 0.154  last_epoch t=74.97s, total_time_elapsed t=8216.0s\n",
      "Epoch: 106 curr_lr: 1.5e-04 - train_loss: 2.121 val_score: 0.155  best_val_score: 0.154  last_epoch t=75.12s, total_time_elapsed t=8292.0s\n",
      "Epoch: 107 curr_lr: 1.5e-04 - train_loss: 2.116 val_score: 0.156  best_val_score: 0.154  last_epoch t=75.22s, total_time_elapsed t=8367.0s\n",
      "Epoch: 108 curr_lr: 1.5e-04 - train_loss: 2.116 val_score: 0.153  best_val_score: 0.153  last_epoch t=75.47s, total_time_elapsed t=8442.0s\n",
      "Epoch: 109 curr_lr: 1.5e-04 - train_loss: 2.112 val_score: 0.165  best_val_score: 0.153  last_epoch t=75.21s, total_time_elapsed t=8518.0s\n",
      "Epoch: 110 curr_lr: 1.5e-04 - train_loss: 2.107 val_score: 0.157  best_val_score: 0.153  last_epoch t=75.21s, total_time_elapsed t=8593.0s\n",
      "Epoch: 111 curr_lr: 1.4e-04 - train_loss: 2.100 val_score: 0.151  best_val_score: 0.151  last_epoch t=75.16s, total_time_elapsed t=8668.0s\n",
      "Epoch: 112 curr_lr: 1.4e-04 - train_loss: 2.115 val_score: 0.158  best_val_score: 0.151  last_epoch t=75.29s, total_time_elapsed t=8743.0s\n",
      "Epoch: 113 curr_lr: 1.4e-04 - train_loss: 2.095 val_score: 0.156  best_val_score: 0.151  last_epoch t=75.02s, total_time_elapsed t=8818.0s\n",
      "Epoch: 114 curr_lr: 1.4e-04 - train_loss: 2.095 val_score: 0.158  best_val_score: 0.151  last_epoch t=74.97s, total_time_elapsed t=8893.0s\n",
      "Epoch: 115 curr_lr: 1.4e-04 - train_loss: 2.085 val_score: 0.152  best_val_score: 0.151  last_epoch t=75.09s, total_time_elapsed t=8968.0s\n",
      "Epoch: 116 curr_lr: 1.4e-04 - train_loss: 2.088 val_score: 0.155  best_val_score: 0.151  last_epoch t=75.09s, total_time_elapsed t=9044.0s\n",
      "Epoch: 117 curr_lr: 1.4e-04 - train_loss: 2.079 val_score: 0.149  best_val_score: 0.149  last_epoch t=75.00s, total_time_elapsed t=9119.0s\n",
      "Epoch: 118 curr_lr: 1.4e-04 - train_loss: 2.075 val_score: 0.150  best_val_score: 0.149  last_epoch t=75.40s, total_time_elapsed t=9194.0s\n",
      "Epoch: 119 curr_lr: 1.4e-04 - train_loss: 2.077 val_score: 0.153  best_val_score: 0.149  last_epoch t=75.11s, total_time_elapsed t=9269.0s\n",
      "Epoch: 120 curr_lr: 1.4e-04 - train_loss: 2.068 val_score: 0.151  best_val_score: 0.149  last_epoch t=75.29s, total_time_elapsed t=9344.0s\n",
      "Epoch: 121 curr_lr: 1.4e-04 - train_loss: 2.074 val_score: 0.151  best_val_score: 0.149  last_epoch t=75.24s, total_time_elapsed t=9420.0s\n",
      "Epoch: 122 curr_lr: 1.4e-04 - train_loss: 2.064 val_score: 0.150  best_val_score: 0.149  last_epoch t=74.97s, total_time_elapsed t=9495.0s\n",
      "Epoch: 123 curr_lr: 1.4e-04 - train_loss: 2.061 val_score: 0.151  best_val_score: 0.149  last_epoch t=75.17s, total_time_elapsed t=9570.0s\n",
      "Epoch: 124 curr_lr: 1.4e-04 - train_loss: 2.053 val_score: 0.152  best_val_score: 0.149  last_epoch t=75.16s, total_time_elapsed t=9645.0s\n",
      "Epoch: 125 curr_lr: 1.4e-04 - train_loss: 2.050 val_score: 0.147  best_val_score: 0.147  last_epoch t=75.17s, total_time_elapsed t=9720.0s\n",
      "Epoch: 126 curr_lr: 1.4e-04 - train_loss: 2.047 val_score: 0.157  best_val_score: 0.147  last_epoch t=75.14s, total_time_elapsed t=9795.0s\n",
      "Epoch: 127 curr_lr: 1.4e-04 - train_loss: 2.046 val_score: 0.149  best_val_score: 0.147  last_epoch t=75.23s, total_time_elapsed t=9871.0s\n",
      "Epoch: 128 curr_lr: 1.4e-04 - train_loss: 2.048 val_score: 0.155  best_val_score: 0.147  last_epoch t=75.10s, total_time_elapsed t=9946.0s\n",
      "Epoch: 129 curr_lr: 1.4e-04 - train_loss: 2.037 val_score: 0.149  best_val_score: 0.147  last_epoch t=75.48s, total_time_elapsed t=10021.0s\n",
      "Epoch: 130 curr_lr: 1.4e-04 - train_loss: 2.032 val_score: 0.148  best_val_score: 0.147  last_epoch t=75.13s, total_time_elapsed t=10096.0s\n",
      "Epoch: 131 curr_lr: 1.3e-04 - train_loss: 2.031 val_score: 0.148  best_val_score: 0.147  last_epoch t=75.34s, total_time_elapsed t=10172.0s\n",
      "Epoch: 132 curr_lr: 1.3e-04 - train_loss: 2.025 val_score: 0.143  best_val_score: 0.143  last_epoch t=75.30s, total_time_elapsed t=10247.0s\n",
      "Epoch: 133 curr_lr: 1.3e-04 - train_loss: 2.027 val_score: 0.148  best_val_score: 0.143  last_epoch t=75.17s, total_time_elapsed t=10322.0s\n",
      "Epoch: 134 curr_lr: 1.3e-04 - train_loss: 2.026 val_score: 0.151  best_val_score: 0.143  last_epoch t=75.52s, total_time_elapsed t=10398.0s\n",
      "Epoch: 135 curr_lr: 1.3e-04 - train_loss: 2.017 val_score: 0.149  best_val_score: 0.143  last_epoch t=75.48s, total_time_elapsed t=10473.0s\n",
      "Epoch: 136 curr_lr: 1.3e-04 - train_loss: 2.015 val_score: 0.147  best_val_score: 0.143  last_epoch t=75.30s, total_time_elapsed t=10549.0s\n",
      "Epoch: 137 curr_lr: 1.3e-04 - train_loss: 2.016 val_score: 0.147  best_val_score: 0.143  last_epoch t=75.27s, total_time_elapsed t=10624.0s\n",
      "Epoch: 138 curr_lr: 1.3e-04 - train_loss: 2.007 val_score: 0.145  best_val_score: 0.143  last_epoch t=75.39s, total_time_elapsed t=10699.0s\n",
      "Epoch: 139 curr_lr: 1.3e-04 - train_loss: 2.003 val_score: 0.148  best_val_score: 0.143  last_epoch t=75.33s, total_time_elapsed t=10775.0s\n",
      "Epoch: 140 curr_lr: 1.3e-04 - train_loss: 2.009 val_score: 0.145  best_val_score: 0.143  last_epoch t=75.05s, total_time_elapsed t=10850.0s\n",
      "Epoch: 141 curr_lr: 1.3e-04 - train_loss: 1.997 val_score: 0.149  best_val_score: 0.143  last_epoch t=75.03s, total_time_elapsed t=10925.0s\n",
      "Epoch: 142 curr_lr: 1.3e-04 - train_loss: 1.998 val_score: 0.146  best_val_score: 0.143  last_epoch t=75.11s, total_time_elapsed t=11000.0s\n",
      "Epoch: 143 curr_lr: 1.3e-04 - train_loss: 1.987 val_score: 0.150  best_val_score: 0.143  last_epoch t=75.26s, total_time_elapsed t=11075.0s\n",
      "Epoch: 144 curr_lr: 1.3e-04 - train_loss: 1.997 val_score: 0.147  best_val_score: 0.143  last_epoch t=75.35s, total_time_elapsed t=11151.0s\n",
      "Epoch: 145 curr_lr: 1.3e-04 - train_loss: 1.987 val_score: 0.144  best_val_score: 0.143  last_epoch t=75.51s, total_time_elapsed t=11226.0s\n",
      "Epoch: 146 curr_lr: 1.3e-04 - train_loss: 1.979 val_score: 0.144  best_val_score: 0.143  last_epoch t=75.25s, total_time_elapsed t=11301.0s\n",
      "Epoch: 147 curr_lr: 1.3e-04 - train_loss: 1.975 val_score: 0.141  best_val_score: 0.141  last_epoch t=75.16s, total_time_elapsed t=11377.0s\n",
      "Epoch: 148 curr_lr: 1.3e-04 - train_loss: 1.976 val_score: 0.140  best_val_score: 0.140  last_epoch t=75.71s, total_time_elapsed t=11452.0s\n",
      "Epoch: 149 curr_lr: 1.3e-04 - train_loss: 1.978 val_score: 0.143  best_val_score: 0.140  last_epoch t=75.23s, total_time_elapsed t=11528.0s\n",
      "Epoch: 150 curr_lr: 1.3e-04 - train_loss: 1.969 val_score: 0.144  best_val_score: 0.140  last_epoch t=75.31s, total_time_elapsed t=11603.0s\n",
      "Epoch: 151 curr_lr: 1.2e-04 - train_loss: 1.971 val_score: 0.146  best_val_score: 0.140  last_epoch t=75.20s, total_time_elapsed t=11678.0s\n",
      "Epoch: 152 curr_lr: 1.2e-04 - train_loss: 1.968 val_score: 0.144  best_val_score: 0.140  last_epoch t=75.32s, total_time_elapsed t=11754.0s\n",
      "Epoch: 153 curr_lr: 1.2e-04 - train_loss: 1.963 val_score: 0.146  best_val_score: 0.140  last_epoch t=76.19s, total_time_elapsed t=11830.0s\n",
      "Epoch: 154 curr_lr: 1.2e-04 - train_loss: 1.967 val_score: 0.143  best_val_score: 0.140  last_epoch t=75.65s, total_time_elapsed t=11905.0s\n",
      "Epoch: 155 curr_lr: 1.2e-04 - train_loss: 1.962 val_score: 0.143  best_val_score: 0.140  last_epoch t=75.55s, total_time_elapsed t=11981.0s\n",
      "Epoch: 156 curr_lr: 1.2e-04 - train_loss: 1.954 val_score: 0.144  best_val_score: 0.140  last_epoch t=75.48s, total_time_elapsed t=12056.0s\n",
      "Epoch: 157 curr_lr: 1.2e-04 - train_loss: 1.951 val_score: 0.201  best_val_score: 0.140  last_epoch t=75.29s, total_time_elapsed t=12132.0s\n",
      "Epoch: 158 curr_lr: 1.2e-04 - train_loss: 1.958 val_score: 0.142  best_val_score: 0.140  last_epoch t=75.48s, total_time_elapsed t=12207.0s\n",
      "Epoch: 159 curr_lr: 1.2e-04 - train_loss: 1.943 val_score: 0.143  best_val_score: 0.140  last_epoch t=75.22s, total_time_elapsed t=12283.0s\n",
      "Epoch: 160 curr_lr: 1.2e-04 - train_loss: 1.940 val_score: 0.142  best_val_score: 0.140  last_epoch t=75.44s, total_time_elapsed t=12358.0s\n",
      "Epoch: 161 curr_lr: 1.2e-04 - train_loss: 1.941 val_score: 0.144  best_val_score: 0.140  last_epoch t=75.30s, total_time_elapsed t=12433.0s\n",
      "Epoch: 162 curr_lr: 1.2e-04 - train_loss: 1.935 val_score: 0.147  best_val_score: 0.140  last_epoch t=75.81s, total_time_elapsed t=12509.0s\n",
      "Epoch: 163 curr_lr: 1.2e-04 - train_loss: 1.931 val_score: 0.143  best_val_score: 0.140  last_epoch t=75.44s, total_time_elapsed t=12585.0s\n",
      "Epoch: 164 curr_lr: 1.2e-04 - train_loss: 1.933 val_score: 0.144  best_val_score: 0.140  last_epoch t=75.43s, total_time_elapsed t=12660.0s\n",
      "Epoch: 165 curr_lr: 1.2e-04 - train_loss: 1.932 val_score: 0.139  best_val_score: 0.139  last_epoch t=88.43s, total_time_elapsed t=12749.0s\n",
      "Epoch: 166 curr_lr: 1.2e-04 - train_loss: 1.924 val_score: 0.139  best_val_score: 0.139  last_epoch t=77.05s, total_time_elapsed t=12826.0s\n",
      "Epoch: 167 curr_lr: 1.2e-04 - train_loss: 1.923 val_score: 0.139  best_val_score: 0.139  last_epoch t=75.44s, total_time_elapsed t=12901.0s\n",
      "Epoch: 168 curr_lr: 1.2e-04 - train_loss: 1.922 val_score: 0.136  best_val_score: 0.136  last_epoch t=87.08s, total_time_elapsed t=12988.0s\n",
      "Epoch: 169 curr_lr: 1.2e-04 - train_loss: 1.916 val_score: 0.140  best_val_score: 0.136  last_epoch t=75.08s, total_time_elapsed t=13063.0s\n",
      "Epoch: 170 curr_lr: 1.2e-04 - train_loss: 1.910 val_score: 0.141  best_val_score: 0.136  last_epoch t=75.24s, total_time_elapsed t=13139.0s\n",
      "Epoch: 171 curr_lr: 1.1e-04 - train_loss: 1.916 val_score: 0.142  best_val_score: 0.136  last_epoch t=75.75s, total_time_elapsed t=13214.0s\n",
      "Epoch: 172 curr_lr: 1.1e-04 - train_loss: 1.906 val_score: 0.141  best_val_score: 0.136  last_epoch t=75.51s, total_time_elapsed t=13290.0s\n",
      "Epoch: 173 curr_lr: 1.1e-04 - train_loss: 1.906 val_score: 0.139  best_val_score: 0.136  last_epoch t=78.71s, total_time_elapsed t=13369.0s\n",
      "Epoch: 174 curr_lr: 1.1e-04 - train_loss: 1.902 val_score: 0.135  best_val_score: 0.135  last_epoch t=86.76s, total_time_elapsed t=13455.0s\n",
      "Epoch: 175 curr_lr: 1.1e-04 - train_loss: 1.902 val_score: 0.140  best_val_score: 0.135  last_epoch t=75.73s, total_time_elapsed t=13531.0s\n",
      "Epoch: 176 curr_lr: 1.1e-04 - train_loss: 1.900 val_score: 0.148  best_val_score: 0.135  last_epoch t=75.50s, total_time_elapsed t=13607.0s\n",
      "Epoch: 177 curr_lr: 1.1e-04 - train_loss: 1.902 val_score: 0.139  best_val_score: 0.135  last_epoch t=75.39s, total_time_elapsed t=13682.0s\n",
      "Epoch: 178 curr_lr: 1.1e-04 - train_loss: 1.893 val_score: 0.136  best_val_score: 0.135  last_epoch t=75.65s, total_time_elapsed t=13758.0s\n",
      "Epoch: 179 curr_lr: 1.1e-04 - train_loss: 1.889 val_score: 0.139  best_val_score: 0.135  last_epoch t=75.83s, total_time_elapsed t=13834.0s\n",
      "Epoch: 180 curr_lr: 1.1e-04 - train_loss: 1.886 val_score: 0.138  best_val_score: 0.135  last_epoch t=75.49s, total_time_elapsed t=13909.0s\n",
      "Epoch: 181 curr_lr: 1.1e-04 - train_loss: 1.885 val_score: 0.137  best_val_score: 0.135  last_epoch t=76.86s, total_time_elapsed t=13986.0s\n",
      "Epoch: 182 curr_lr: 1.1e-04 - train_loss: 1.889 val_score: 0.142  best_val_score: 0.135  last_epoch t=75.38s, total_time_elapsed t=14061.0s\n",
      "Epoch: 183 curr_lr: 1.1e-04 - train_loss: 1.882 val_score: 0.138  best_val_score: 0.135  last_epoch t=75.35s, total_time_elapsed t=14137.0s\n",
      "Epoch: 184 curr_lr: 1.1e-04 - train_loss: 1.877 val_score: 0.139  best_val_score: 0.135  last_epoch t=75.64s, total_time_elapsed t=14212.0s\n",
      "Epoch: 185 curr_lr: 1.1e-04 - train_loss: 1.876 val_score: 0.136  best_val_score: 0.135  last_epoch t=76.32s, total_time_elapsed t=14289.0s\n",
      "Epoch: 186 curr_lr: 1.1e-04 - train_loss: 1.874 val_score: 0.137  best_val_score: 0.135  last_epoch t=75.40s, total_time_elapsed t=14364.0s\n",
      "Epoch: 187 curr_lr: 1.1e-04 - train_loss: 1.876 val_score: 0.134  best_val_score: 0.134  last_epoch t=89.39s, total_time_elapsed t=14454.0s\n",
      "Epoch: 188 curr_lr: 1.1e-04 - train_loss: 1.871 val_score: 0.138  best_val_score: 0.134  last_epoch t=75.57s, total_time_elapsed t=14529.0s\n",
      "Epoch: 189 curr_lr: 1.1e-04 - train_loss: 1.865 val_score: 0.134  best_val_score: 0.134  last_epoch t=76.63s, total_time_elapsed t=14606.0s\n",
      "Epoch: 190 curr_lr: 1.1e-04 - train_loss: 1.861 val_score: 0.134  best_val_score: 0.134  last_epoch t=75.63s, total_time_elapsed t=14682.0s\n",
      "Epoch: 191 curr_lr: 1.0e-04 - train_loss: 1.859 val_score: 0.136  best_val_score: 0.134  last_epoch t=75.32s, total_time_elapsed t=14757.0s\n",
      "Epoch: 192 curr_lr: 1.0e-04 - train_loss: 1.858 val_score: 0.144  best_val_score: 0.134  last_epoch t=75.96s, total_time_elapsed t=14833.0s\n",
      "Epoch: 193 curr_lr: 1.0e-04 - train_loss: 1.861 val_score: 0.136  best_val_score: 0.134  last_epoch t=75.56s, total_time_elapsed t=14908.0s\n",
      "Epoch: 194 curr_lr: 1.0e-04 - train_loss: 1.852 val_score: 0.133  best_val_score: 0.133  last_epoch t=86.94s, total_time_elapsed t=14995.0s\n",
      "Epoch: 195 curr_lr: 1.0e-04 - train_loss: 1.850 val_score: 0.135  best_val_score: 0.133  last_epoch t=75.58s, total_time_elapsed t=15071.0s\n",
      "Epoch: 196 curr_lr: 1.0e-04 - train_loss: 1.850 val_score: 0.140  best_val_score: 0.133  last_epoch t=75.70s, total_time_elapsed t=15147.0s\n",
      "Epoch: 197 curr_lr: 1.0e-04 - train_loss: 1.845 val_score: 0.133  best_val_score: 0.133  last_epoch t=77.47s, total_time_elapsed t=15224.0s\n",
      "Epoch: 198 curr_lr: 1.0e-04 - train_loss: 1.849 val_score: 0.136  best_val_score: 0.133  last_epoch t=76.33s, total_time_elapsed t=15301.0s\n",
      "Epoch: 199 curr_lr: 1.0e-04 - train_loss: 1.842 val_score: 0.137  best_val_score: 0.133  last_epoch t=75.50s, total_time_elapsed t=15376.0s\n",
      "Epoch: 200 curr_lr: 1.0e-04 - train_loss: 1.840 val_score: 0.134  best_val_score: 0.133  last_epoch t=75.48s, total_time_elapsed t=15452.0s\n",
      "Epoch: 201 curr_lr: 1.0e-04 - train_loss: 1.835 val_score: 0.135  best_val_score: 0.133  last_epoch t=75.65s, total_time_elapsed t=15527.0s\n",
      "Epoch: 202 curr_lr: 9.9e-05 - train_loss: 1.835 val_score: 0.136  best_val_score: 0.133  last_epoch t=75.35s, total_time_elapsed t=15603.0s\n",
      "Epoch: 203 curr_lr: 9.9e-05 - train_loss: 1.834 val_score: 0.135  best_val_score: 0.133  last_epoch t=75.56s, total_time_elapsed t=15678.0s\n",
      "Epoch: 204 curr_lr: 9.8e-05 - train_loss: 1.828 val_score: 0.136  best_val_score: 0.133  last_epoch t=75.18s, total_time_elapsed t=15753.0s\n",
      "Epoch: 205 curr_lr: 9.8e-05 - train_loss: 1.829 val_score: 0.133  best_val_score: 0.133  last_epoch t=89.31s, total_time_elapsed t=15843.0s\n",
      "Epoch: 206 curr_lr: 9.7e-05 - train_loss: 1.826 val_score: 0.133  best_val_score: 0.133  last_epoch t=86.36s, total_time_elapsed t=15929.0s\n",
      "Epoch: 207 curr_lr: 9.7e-05 - train_loss: 1.821 val_score: 0.133  best_val_score: 0.133  last_epoch t=75.59s, total_time_elapsed t=16005.0s\n",
      "Epoch: 208 curr_lr: 9.6e-05 - train_loss: 1.819 val_score: 0.133  best_val_score: 0.133  last_epoch t=75.74s, total_time_elapsed t=16081.0s\n",
      "Epoch: 209 curr_lr: 9.6e-05 - train_loss: 1.817 val_score: 0.135  best_val_score: 0.133  last_epoch t=75.54s, total_time_elapsed t=16156.0s\n",
      "Epoch: 210 curr_lr: 9.5e-05 - train_loss: 1.815 val_score: 0.134  best_val_score: 0.133  last_epoch t=75.79s, total_time_elapsed t=16232.0s\n",
      "Epoch: 211 curr_lr: 9.5e-05 - train_loss: 1.817 val_score: 0.132  best_val_score: 0.132  last_epoch t=86.30s, total_time_elapsed t=16318.0s\n",
      "Epoch: 212 curr_lr: 9.4e-05 - train_loss: 1.820 val_score: 0.133  best_val_score: 0.132  last_epoch t=75.76s, total_time_elapsed t=16394.0s\n",
      "Epoch: 213 curr_lr: 9.4e-05 - train_loss: 1.811 val_score: 0.133  best_val_score: 0.132  last_epoch t=76.76s, total_time_elapsed t=16471.0s\n",
      "Epoch: 214 curr_lr: 9.3e-05 - train_loss: 1.809 val_score: 0.133  best_val_score: 0.132  last_epoch t=76.04s, total_time_elapsed t=16547.0s\n",
      "Epoch: 215 curr_lr: 9.3e-05 - train_loss: 1.804 val_score: 0.131  best_val_score: 0.131  last_epoch t=87.53s, total_time_elapsed t=16634.0s\n",
      "Epoch: 216 curr_lr: 9.2e-05 - train_loss: 1.805 val_score: 0.133  best_val_score: 0.131  last_epoch t=75.41s, total_time_elapsed t=16710.0s\n",
      "Epoch: 217 curr_lr: 9.2e-05 - train_loss: 1.799 val_score: 0.131  best_val_score: 0.131  last_epoch t=87.11s, total_time_elapsed t=16797.0s\n",
      "Epoch: 218 curr_lr: 9.1e-05 - train_loss: 1.796 val_score: 0.136  best_val_score: 0.131  last_epoch t=75.68s, total_time_elapsed t=16873.0s\n",
      "Epoch: 219 curr_lr: 9.1e-05 - train_loss: 1.798 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.67s, total_time_elapsed t=16948.0s\n",
      "Epoch: 220 curr_lr: 9.0e-05 - train_loss: 1.794 val_score: 0.132  best_val_score: 0.131  last_epoch t=78.53s, total_time_elapsed t=17027.0s\n",
      "Epoch: 221 curr_lr: 9.0e-05 - train_loss: 1.790 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.48s, total_time_elapsed t=17102.0s\n",
      "Epoch: 222 curr_lr: 8.9e-05 - train_loss: 1.788 val_score: 0.141  best_val_score: 0.131  last_epoch t=75.43s, total_time_elapsed t=17178.0s\n",
      "Epoch: 223 curr_lr: 8.9e-05 - train_loss: 1.786 val_score: 0.135  best_val_score: 0.131  last_epoch t=75.36s, total_time_elapsed t=17253.0s\n",
      "Epoch: 224 curr_lr: 8.8e-05 - train_loss: 1.790 val_score: 0.131  best_val_score: 0.131  last_epoch t=75.61s, total_time_elapsed t=17329.0s\n",
      "Epoch: 225 curr_lr: 8.8e-05 - train_loss: 1.785 val_score: 0.131  best_val_score: 0.131  last_epoch t=75.76s, total_time_elapsed t=17405.0s\n",
      "Epoch: 226 curr_lr: 8.7e-05 - train_loss: 1.779 val_score: 0.133  best_val_score: 0.131  last_epoch t=75.63s, total_time_elapsed t=17480.0s\n",
      "Epoch: 227 curr_lr: 8.7e-05 - train_loss: 1.780 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.71s, total_time_elapsed t=17556.0s\n",
      "Epoch: 228 curr_lr: 8.6e-05 - train_loss: 1.775 val_score: 0.130  best_val_score: 0.130  last_epoch t=90.47s, total_time_elapsed t=17647.0s\n",
      "Epoch: 229 curr_lr: 8.6e-05 - train_loss: 1.775 val_score: 0.132  best_val_score: 0.130  last_epoch t=75.98s, total_time_elapsed t=17723.0s\n",
      "Epoch: 230 curr_lr: 8.5e-05 - train_loss: 1.771 val_score: 0.132  best_val_score: 0.130  last_epoch t=75.89s, total_time_elapsed t=17799.0s\n",
      "Epoch: 231 curr_lr: 8.5e-05 - train_loss: 1.771 val_score: 0.128  best_val_score: 0.128  last_epoch t=86.22s, total_time_elapsed t=17885.0s\n",
      "Epoch: 232 curr_lr: 8.4e-05 - train_loss: 1.772 val_score: 0.131  best_val_score: 0.128  last_epoch t=75.62s, total_time_elapsed t=17960.0s\n",
      "Epoch: 233 curr_lr: 8.4e-05 - train_loss: 1.767 val_score: 0.129  best_val_score: 0.128  last_epoch t=75.50s, total_time_elapsed t=18036.0s\n",
      "Epoch: 234 curr_lr: 8.3e-05 - train_loss: 1.765 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.48s, total_time_elapsed t=18111.0s\n",
      "Epoch: 235 curr_lr: 8.3e-05 - train_loss: 1.760 val_score: 0.128  best_val_score: 0.128  last_epoch t=75.90s, total_time_elapsed t=18187.0s\n",
      "Epoch: 236 curr_lr: 8.2e-05 - train_loss: 1.761 val_score: 0.129  best_val_score: 0.128  last_epoch t=76.73s, total_time_elapsed t=18264.0s\n",
      "Epoch: 237 curr_lr: 8.2e-05 - train_loss: 1.754 val_score: 0.132  best_val_score: 0.128  last_epoch t=75.37s, total_time_elapsed t=18340.0s\n",
      "Epoch: 238 curr_lr: 8.1e-05 - train_loss: 1.756 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.44s, total_time_elapsed t=18415.0s\n",
      "Epoch: 239 curr_lr: 8.1e-05 - train_loss: 1.753 val_score: 0.128  best_val_score: 0.128  last_epoch t=75.68s, total_time_elapsed t=18491.0s\n",
      "Epoch: 240 curr_lr: 8.0e-05 - train_loss: 1.751 val_score: 0.131  best_val_score: 0.128  last_epoch t=75.60s, total_time_elapsed t=18566.0s\n",
      "Epoch: 241 curr_lr: 8.0e-05 - train_loss: 1.746 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.50s, total_time_elapsed t=18642.0s\n",
      "Epoch: 242 curr_lr: 7.9e-05 - train_loss: 1.744 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.38s, total_time_elapsed t=18717.0s\n",
      "Epoch: 243 curr_lr: 7.9e-05 - train_loss: 1.744 val_score: 0.132  best_val_score: 0.128  last_epoch t=75.53s, total_time_elapsed t=18793.0s\n",
      "Epoch: 244 curr_lr: 7.8e-05 - train_loss: 1.744 val_score: 0.128  best_val_score: 0.128  last_epoch t=75.84s, total_time_elapsed t=18869.0s\n",
      "Epoch: 245 curr_lr: 7.8e-05 - train_loss: 1.738 val_score: 0.128  best_val_score: 0.128  last_epoch t=87.44s, total_time_elapsed t=18956.0s\n",
      "Epoch: 246 curr_lr: 7.7e-05 - train_loss: 1.737 val_score: 0.128  best_val_score: 0.128  last_epoch t=85.95s, total_time_elapsed t=19042.0s\n",
      "Epoch: 247 curr_lr: 7.7e-05 - train_loss: 1.742 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.70s, total_time_elapsed t=19118.0s\n",
      "Epoch: 248 curr_lr: 7.6e-05 - train_loss: 1.735 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.40s, total_time_elapsed t=19193.0s\n",
      "Epoch: 249 curr_lr: 7.6e-05 - train_loss: 1.733 val_score: 0.127  best_val_score: 0.127  last_epoch t=86.16s, total_time_elapsed t=19279.0s\n",
      "Epoch: 250 curr_lr: 7.5e-05 - train_loss: 1.729 val_score: 0.127  best_val_score: 0.127  last_epoch t=87.17s, total_time_elapsed t=19367.0s\n",
      "Epoch: 251 curr_lr: 7.5e-05 - train_loss: 1.730 val_score: 0.128  best_val_score: 0.127  last_epoch t=75.82s, total_time_elapsed t=19443.0s\n",
      "Epoch: 252 curr_lr: 7.4e-05 - train_loss: 1.726 val_score: 0.130  best_val_score: 0.127  last_epoch t=76.39s, total_time_elapsed t=19519.0s\n",
      "Epoch: 253 curr_lr: 7.4e-05 - train_loss: 1.724 val_score: 0.128  best_val_score: 0.127  last_epoch t=75.36s, total_time_elapsed t=19594.0s\n",
      "Epoch: 254 curr_lr: 7.3e-05 - train_loss: 1.725 val_score: 0.130  best_val_score: 0.127  last_epoch t=75.54s, total_time_elapsed t=19670.0s\n",
      "Epoch: 255 curr_lr: 7.3e-05 - train_loss: 1.722 val_score: 0.131  best_val_score: 0.127  last_epoch t=75.64s, total_time_elapsed t=19746.0s\n",
      "Epoch: 256 curr_lr: 7.2e-05 - train_loss: 1.716 val_score: 0.127  best_val_score: 0.127  last_epoch t=75.32s, total_time_elapsed t=19821.0s\n",
      "Epoch: 257 curr_lr: 7.2e-05 - train_loss: 1.717 val_score: 0.126  best_val_score: 0.126  last_epoch t=86.53s, total_time_elapsed t=19907.0s\n",
      "Epoch: 258 curr_lr: 7.1e-05 - train_loss: 1.713 val_score: 0.126  best_val_score: 0.126  last_epoch t=85.87s, total_time_elapsed t=19993.0s\n",
      "Epoch: 259 curr_lr: 7.1e-05 - train_loss: 1.713 val_score: 0.128  best_val_score: 0.126  last_epoch t=75.49s, total_time_elapsed t=20069.0s\n",
      "Epoch: 260 curr_lr: 7.0e-05 - train_loss: 1.710 val_score: 0.128  best_val_score: 0.126  last_epoch t=76.75s, total_time_elapsed t=20146.0s\n",
      "Epoch: 261 curr_lr: 7.0e-05 - train_loss: 1.709 val_score: 0.127  best_val_score: 0.126  last_epoch t=75.87s, total_time_elapsed t=20222.0s\n",
      "Epoch: 262 curr_lr: 6.9e-05 - train_loss: 1.708 val_score: 0.125  best_val_score: 0.125  last_epoch t=86.34s, total_time_elapsed t=20308.0s\n",
      "Epoch: 263 curr_lr: 6.9e-05 - train_loss: 1.704 val_score: 0.128  best_val_score: 0.125  last_epoch t=75.74s, total_time_elapsed t=20384.0s\n",
      "Epoch: 264 curr_lr: 6.8e-05 - train_loss: 1.701 val_score: 0.126  best_val_score: 0.125  last_epoch t=75.30s, total_time_elapsed t=20459.0s\n",
      "Epoch: 265 curr_lr: 6.8e-05 - train_loss: 1.700 val_score: 0.127  best_val_score: 0.125  last_epoch t=75.31s, total_time_elapsed t=20534.0s\n",
      "Epoch: 266 curr_lr: 6.7e-05 - train_loss: 1.696 val_score: 0.126  best_val_score: 0.125  last_epoch t=75.52s, total_time_elapsed t=20610.0s\n",
      "Epoch: 267 curr_lr: 6.7e-05 - train_loss: 1.698 val_score: 0.126  best_val_score: 0.125  last_epoch t=79.24s, total_time_elapsed t=20689.0s\n",
      "Epoch: 268 curr_lr: 6.6e-05 - train_loss: 1.693 val_score: 0.126  best_val_score: 0.125  last_epoch t=75.61s, total_time_elapsed t=20765.0s\n",
      "Epoch: 269 curr_lr: 6.6e-05 - train_loss: 1.693 val_score: 0.125  best_val_score: 0.125  last_epoch t=86.80s, total_time_elapsed t=20852.0s\n",
      "Epoch: 270 curr_lr: 6.5e-05 - train_loss: 1.695 val_score: 0.124  best_val_score: 0.124  last_epoch t=86.98s, total_time_elapsed t=20939.0s\n",
      "Epoch: 271 curr_lr: 6.5e-05 - train_loss: 1.689 val_score: 0.126  best_val_score: 0.124  last_epoch t=75.77s, total_time_elapsed t=21014.0s\n",
      "Epoch: 272 curr_lr: 6.4e-05 - train_loss: 1.686 val_score: 0.128  best_val_score: 0.124  last_epoch t=75.65s, total_time_elapsed t=21090.0s\n",
      "Epoch: 273 curr_lr: 6.4e-05 - train_loss: 1.686 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.72s, total_time_elapsed t=21166.0s\n",
      "Epoch: 274 curr_lr: 6.3e-05 - train_loss: 1.685 val_score: 0.127  best_val_score: 0.124  last_epoch t=75.31s, total_time_elapsed t=21241.0s\n",
      "Epoch: 275 curr_lr: 6.3e-05 - train_loss: 1.681 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.25s, total_time_elapsed t=21317.0s\n",
      "Epoch: 276 curr_lr: 6.2e-05 - train_loss: 1.680 val_score: 0.128  best_val_score: 0.124  last_epoch t=75.72s, total_time_elapsed t=21393.0s\n",
      "Epoch: 277 curr_lr: 6.2e-05 - train_loss: 1.678 val_score: 0.127  best_val_score: 0.124  last_epoch t=75.45s, total_time_elapsed t=21469.0s\n",
      "Epoch: 278 curr_lr: 6.1e-05 - train_loss: 1.674 val_score: 0.126  best_val_score: 0.124  last_epoch t=75.48s, total_time_elapsed t=21544.0s\n",
      "Epoch: 279 curr_lr: 6.1e-05 - train_loss: 1.673 val_score: 0.127  best_val_score: 0.124  last_epoch t=75.36s, total_time_elapsed t=21619.0s\n",
      "Epoch: 280 curr_lr: 6.0e-05 - train_loss: 1.673 val_score: 0.127  best_val_score: 0.124  last_epoch t=75.27s, total_time_elapsed t=21695.0s\n",
      "Epoch: 281 curr_lr: 6.0e-05 - train_loss: 1.669 val_score: 0.124  best_val_score: 0.124  last_epoch t=75.47s, total_time_elapsed t=21770.0s\n",
      "Epoch: 282 curr_lr: 5.9e-05 - train_loss: 1.667 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.34s, total_time_elapsed t=21846.0s\n",
      "Epoch: 283 curr_lr: 5.9e-05 - train_loss: 1.666 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.35s, total_time_elapsed t=21921.0s\n",
      "Epoch: 284 curr_lr: 5.8e-05 - train_loss: 1.663 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.46s, total_time_elapsed t=21996.0s\n",
      "Epoch: 285 curr_lr: 5.8e-05 - train_loss: 1.661 val_score: 0.124  best_val_score: 0.124  last_epoch t=75.43s, total_time_elapsed t=22072.0s\n",
      "Epoch: 286 curr_lr: 5.7e-05 - train_loss: 1.661 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.55s, total_time_elapsed t=22148.0s\n",
      "Epoch: 287 curr_lr: 5.7e-05 - train_loss: 1.659 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.52s, total_time_elapsed t=22223.0s\n",
      "Epoch: 288 curr_lr: 5.6e-05 - train_loss: 1.659 val_score: 0.123  best_val_score: 0.123  last_epoch t=88.29s, total_time_elapsed t=22311.0s\n",
      "Epoch: 289 curr_lr: 5.6e-05 - train_loss: 1.653 val_score: 0.123  best_val_score: 0.123  last_epoch t=75.35s, total_time_elapsed t=22387.0s\n",
      "Epoch: 290 curr_lr: 5.5e-05 - train_loss: 1.653 val_score: 0.126  best_val_score: 0.123  last_epoch t=75.16s, total_time_elapsed t=22462.0s\n",
      "Epoch: 291 curr_lr: 5.5e-05 - train_loss: 1.651 val_score: 0.123  best_val_score: 0.123  last_epoch t=76.79s, total_time_elapsed t=22539.0s\n",
      "Epoch: 292 curr_lr: 5.4e-05 - train_loss: 1.649 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.45s, total_time_elapsed t=22614.0s\n",
      "Epoch: 293 curr_lr: 5.4e-05 - train_loss: 1.650 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.48s, total_time_elapsed t=22690.0s\n",
      "Epoch: 294 curr_lr: 5.3e-05 - train_loss: 1.645 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.84s, total_time_elapsed t=22766.0s\n",
      "Epoch: 295 curr_lr: 5.3e-05 - train_loss: 1.642 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.43s, total_time_elapsed t=22841.0s\n",
      "Epoch: 296 curr_lr: 5.2e-05 - train_loss: 1.644 val_score: 0.127  best_val_score: 0.123  last_epoch t=76.06s, total_time_elapsed t=22917.0s\n",
      "Epoch: 297 curr_lr: 5.2e-05 - train_loss: 1.640 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.60s, total_time_elapsed t=22993.0s\n",
      "Epoch: 298 curr_lr: 5.1e-05 - train_loss: 1.641 val_score: 0.123  best_val_score: 0.123  last_epoch t=86.20s, total_time_elapsed t=23079.0s\n",
      "Epoch: 299 curr_lr: 5.1e-05 - train_loss: 1.637 val_score: 0.123  best_val_score: 0.123  last_epoch t=87.92s, total_time_elapsed t=23167.0s\n",
      "Epoch: 300 curr_lr: 5.0e-05 - train_loss: 1.636 val_score: 0.123  best_val_score: 0.123  last_epoch t=75.45s, total_time_elapsed t=23242.0s\n",
      "Epoch: 301 curr_lr: 5.0e-05 - train_loss: 1.633 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.31s, total_time_elapsed t=23318.0s\n",
      "Epoch: 302 curr_lr: 4.9e-05 - train_loss: 1.631 val_score: 0.126  best_val_score: 0.123  last_epoch t=75.76s, total_time_elapsed t=23394.0s\n",
      "Epoch: 303 curr_lr: 4.9e-05 - train_loss: 1.632 val_score: 0.123  best_val_score: 0.123  last_epoch t=85.61s, total_time_elapsed t=23479.0s\n",
      "Epoch: 304 curr_lr: 4.8e-05 - train_loss: 1.627 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.60s, total_time_elapsed t=23555.0s\n",
      "Epoch: 305 curr_lr: 4.8e-05 - train_loss: 1.629 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.58s, total_time_elapsed t=23630.0s\n",
      "Epoch: 306 curr_lr: 4.7e-05 - train_loss: 1.624 val_score: 0.123  best_val_score: 0.123  last_epoch t=75.47s, total_time_elapsed t=23706.0s\n",
      "Epoch: 307 curr_lr: 4.7e-05 - train_loss: 1.624 val_score: 0.123  best_val_score: 0.123  last_epoch t=76.54s, total_time_elapsed t=23782.0s\n",
      "Epoch: 308 curr_lr: 4.6e-05 - train_loss: 1.621 val_score: 0.122  best_val_score: 0.122  last_epoch t=89.01s, total_time_elapsed t=23871.0s\n",
      "Epoch: 309 curr_lr: 4.6e-05 - train_loss: 1.618 val_score: 0.124  best_val_score: 0.122  last_epoch t=75.37s, total_time_elapsed t=23947.0s\n",
      "Epoch: 310 curr_lr: 4.5e-05 - train_loss: 1.620 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.52s, total_time_elapsed t=24022.0s\n",
      "Epoch: 311 curr_lr: 4.5e-05 - train_loss: 1.615 val_score: 0.124  best_val_score: 0.122  last_epoch t=75.60s, total_time_elapsed t=24098.0s\n",
      "Epoch: 312 curr_lr: 4.4e-05 - train_loss: 1.615 val_score: 0.122  best_val_score: 0.122  last_epoch t=86.34s, total_time_elapsed t=24184.0s\n",
      "Epoch: 313 curr_lr: 4.4e-05 - train_loss: 1.612 val_score: 0.124  best_val_score: 0.122  last_epoch t=75.49s, total_time_elapsed t=24260.0s\n",
      "Epoch: 314 curr_lr: 4.3e-05 - train_loss: 1.611 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.33s, total_time_elapsed t=24335.0s\n",
      "Epoch: 315 curr_lr: 4.3e-05 - train_loss: 1.610 val_score: 0.123  best_val_score: 0.122  last_epoch t=76.32s, total_time_elapsed t=24412.0s\n",
      "Epoch: 316 curr_lr: 4.2e-05 - train_loss: 1.607 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.44s, total_time_elapsed t=24487.0s\n",
      "Epoch: 317 curr_lr: 4.2e-05 - train_loss: 1.606 val_score: 0.121  best_val_score: 0.121  last_epoch t=87.16s, total_time_elapsed t=24574.0s\n",
      "Epoch: 318 curr_lr: 4.1e-05 - train_loss: 1.604 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.49s, total_time_elapsed t=24650.0s\n",
      "Epoch: 319 curr_lr: 4.1e-05 - train_loss: 1.601 val_score: 0.121  best_val_score: 0.121  last_epoch t=75.79s, total_time_elapsed t=24726.0s\n",
      "Epoch: 320 curr_lr: 4.0e-05 - train_loss: 1.599 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.45s, total_time_elapsed t=24801.0s\n",
      "Epoch: 321 curr_lr: 4.0e-05 - train_loss: 1.597 val_score: 0.123  best_val_score: 0.121  last_epoch t=75.32s, total_time_elapsed t=24876.0s\n",
      "Epoch: 322 curr_lr: 3.9e-05 - train_loss: 1.598 val_score: 0.123  best_val_score: 0.121  last_epoch t=78.02s, total_time_elapsed t=24954.0s\n",
      "Epoch: 323 curr_lr: 3.9e-05 - train_loss: 1.595 val_score: 0.121  best_val_score: 0.121  last_epoch t=88.06s, total_time_elapsed t=25043.0s\n",
      "Epoch: 324 curr_lr: 3.8e-05 - train_loss: 1.594 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.59s, total_time_elapsed t=25118.0s\n",
      "Epoch: 325 curr_lr: 3.8e-05 - train_loss: 1.592 val_score: 0.124  best_val_score: 0.121  last_epoch t=76.07s, total_time_elapsed t=25194.0s\n",
      "Epoch: 326 curr_lr: 3.7e-05 - train_loss: 1.592 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.33s, total_time_elapsed t=25270.0s\n",
      "Epoch: 327 curr_lr: 3.7e-05 - train_loss: 1.588 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.79s, total_time_elapsed t=25345.0s\n",
      "Epoch: 328 curr_lr: 3.6e-05 - train_loss: 1.589 val_score: 0.122  best_val_score: 0.121  last_epoch t=76.00s, total_time_elapsed t=25421.0s\n",
      "Epoch: 329 curr_lr: 3.6e-05 - train_loss: 1.584 val_score: 0.121  best_val_score: 0.121  last_epoch t=75.75s, total_time_elapsed t=25497.0s\n",
      "Epoch: 330 curr_lr: 3.5e-05 - train_loss: 1.583 val_score: 0.121  best_val_score: 0.121  last_epoch t=76.61s, total_time_elapsed t=25574.0s\n",
      "Epoch: 331 curr_lr: 3.5e-05 - train_loss: 1.580 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.72s, total_time_elapsed t=25650.0s\n",
      "Epoch: 332 curr_lr: 3.4e-05 - train_loss: 1.581 val_score: 0.121  best_val_score: 0.121  last_epoch t=88.84s, total_time_elapsed t=25738.0s\n",
      "Epoch: 333 curr_lr: 3.4e-05 - train_loss: 1.579 val_score: 0.123  best_val_score: 0.121  last_epoch t=75.27s, total_time_elapsed t=25814.0s\n",
      "Epoch: 334 curr_lr: 3.3e-05 - train_loss: 1.578 val_score: 0.120  best_val_score: 0.120  last_epoch t=86.05s, total_time_elapsed t=25900.0s\n",
      "Epoch: 335 curr_lr: 3.3e-05 - train_loss: 1.576 val_score: 0.122  best_val_score: 0.120  last_epoch t=75.92s, total_time_elapsed t=25976.0s\n",
      "Epoch: 336 curr_lr: 3.2e-05 - train_loss: 1.575 val_score: 0.122  best_val_score: 0.120  last_epoch t=76.14s, total_time_elapsed t=26052.0s\n",
      "Epoch: 337 curr_lr: 3.2e-05 - train_loss: 1.572 val_score: 0.121  best_val_score: 0.120  last_epoch t=75.57s, total_time_elapsed t=26128.0s\n",
      "Epoch: 338 curr_lr: 3.1e-05 - train_loss: 1.571 val_score: 0.122  best_val_score: 0.120  last_epoch t=76.53s, total_time_elapsed t=26204.0s\n",
      "Epoch: 339 curr_lr: 3.1e-05 - train_loss: 1.569 val_score: 0.122  best_val_score: 0.120  last_epoch t=75.45s, total_time_elapsed t=26280.0s\n",
      "Epoch: 340 curr_lr: 3.0e-05 - train_loss: 1.568 val_score: 0.122  best_val_score: 0.120  last_epoch t=76.19s, total_time_elapsed t=26356.0s\n",
      "Epoch: 341 curr_lr: 3.0e-05 - train_loss: 1.566 val_score: 0.121  best_val_score: 0.120  last_epoch t=75.56s, total_time_elapsed t=26431.0s\n",
      "Epoch: 342 curr_lr: 2.9e-05 - train_loss: 1.565 val_score: 0.122  best_val_score: 0.120  last_epoch t=75.89s, total_time_elapsed t=26507.0s\n",
      "Epoch: 343 curr_lr: 2.9e-05 - train_loss: 1.564 val_score: 0.122  best_val_score: 0.120  last_epoch t=75.67s, total_time_elapsed t=26583.0s\n",
      "Epoch: 344 curr_lr: 2.8e-05 - train_loss: 1.562 val_score: 0.120  best_val_score: 0.120  last_epoch t=75.58s, total_time_elapsed t=26659.0s\n",
      "Epoch: 345 curr_lr: 2.8e-05 - train_loss: 1.559 val_score: 0.121  best_val_score: 0.120  last_epoch t=75.35s, total_time_elapsed t=26734.0s\n",
      "Epoch: 346 curr_lr: 2.7e-05 - train_loss: 1.559 val_score: 0.122  best_val_score: 0.120  last_epoch t=75.47s, total_time_elapsed t=26809.0s\n",
      "Epoch: 347 curr_lr: 2.7e-05 - train_loss: 1.558 val_score: 0.121  best_val_score: 0.120  last_epoch t=75.61s, total_time_elapsed t=26885.0s\n",
      "Epoch: 348 curr_lr: 2.6e-05 - train_loss: 1.556 val_score: 0.120  best_val_score: 0.120  last_epoch t=87.01s, total_time_elapsed t=26972.0s\n",
      "Epoch: 349 curr_lr: 2.6e-05 - train_loss: 1.554 val_score: 0.120  best_val_score: 0.120  last_epoch t=75.44s, total_time_elapsed t=27048.0s\n",
      "Epoch: 350 curr_lr: 2.5e-05 - train_loss: 1.550 val_score: 0.120  best_val_score: 0.120  last_epoch t=75.51s, total_time_elapsed t=27123.0s\n",
      "Epoch: 351 curr_lr: 2.5e-05 - train_loss: 1.551 val_score: 0.119  best_val_score: 0.119  last_epoch t=86.38s, total_time_elapsed t=27210.0s\n",
      "Epoch: 352 curr_lr: 2.4e-05 - train_loss: 1.548 val_score: 0.120  best_val_score: 0.119  last_epoch t=75.72s, total_time_elapsed t=27285.0s\n",
      "Epoch: 353 curr_lr: 2.4e-05 - train_loss: 1.547 val_score: 0.119  best_val_score: 0.119  last_epoch t=86.70s, total_time_elapsed t=27372.0s\n",
      "Epoch: 354 curr_lr: 2.3e-05 - train_loss: 1.545 val_score: 0.120  best_val_score: 0.119  last_epoch t=76.52s, total_time_elapsed t=27449.0s\n",
      "Epoch: 355 curr_lr: 2.3e-05 - train_loss: 1.544 val_score: 0.119  best_val_score: 0.119  last_epoch t=75.56s, total_time_elapsed t=27524.0s\n",
      "Epoch: 356 curr_lr: 2.2e-05 - train_loss: 1.543 val_score: 0.119  best_val_score: 0.119  last_epoch t=75.36s, total_time_elapsed t=27600.0s\n",
      "Epoch: 357 curr_lr: 2.2e-05 - train_loss: 1.541 val_score: 0.119  best_val_score: 0.119  last_epoch t=75.71s, total_time_elapsed t=27675.0s\n",
      "Epoch: 358 curr_lr: 2.1e-05 - train_loss: 1.539 val_score: 0.118  best_val_score: 0.118  last_epoch t=86.81s, total_time_elapsed t=27762.0s\n",
      "Epoch: 359 curr_lr: 2.1e-05 - train_loss: 1.538 val_score: 0.120  best_val_score: 0.118  last_epoch t=75.58s, total_time_elapsed t=27838.0s\n",
      "Epoch: 360 curr_lr: 2.0e-05 - train_loss: 1.536 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.33s, total_time_elapsed t=27913.0s\n",
      "Epoch: 361 curr_lr: 2.0e-05 - train_loss: 1.535 val_score: 0.120  best_val_score: 0.118  last_epoch t=75.19s, total_time_elapsed t=27988.0s\n",
      "Epoch: 362 curr_lr: 1.9e-05 - train_loss: 1.533 val_score: 0.119  best_val_score: 0.118  last_epoch t=77.16s, total_time_elapsed t=28065.0s\n",
      "Epoch: 363 curr_lr: 1.9e-05 - train_loss: 1.532 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.53s, total_time_elapsed t=28141.0s\n",
      "Epoch: 364 curr_lr: 1.8e-05 - train_loss: 1.532 val_score: 0.120  best_val_score: 0.118  last_epoch t=75.53s, total_time_elapsed t=28217.0s\n",
      "Epoch: 365 curr_lr: 1.8e-05 - train_loss: 1.530 val_score: 0.120  best_val_score: 0.118  last_epoch t=75.66s, total_time_elapsed t=28292.0s\n",
      "Epoch: 366 curr_lr: 1.7e-05 - train_loss: 1.528 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.49s, total_time_elapsed t=28368.0s\n",
      "Epoch: 367 curr_lr: 1.7e-05 - train_loss: 1.527 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.22s, total_time_elapsed t=28443.0s\n",
      "Epoch: 368 curr_lr: 1.6e-05 - train_loss: 1.525 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.37s, total_time_elapsed t=28518.0s\n",
      "Epoch: 369 curr_lr: 1.6e-05 - train_loss: 1.524 val_score: 0.118  best_val_score: 0.118  last_epoch t=88.17s, total_time_elapsed t=28607.0s\n",
      "Epoch: 370 curr_lr: 1.5e-05 - train_loss: 1.521 val_score: 0.119  best_val_score: 0.118  last_epoch t=76.65s, total_time_elapsed t=28683.0s\n",
      "Epoch: 371 curr_lr: 1.5e-05 - train_loss: 1.523 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.59s, total_time_elapsed t=28759.0s\n",
      "Epoch: 372 curr_lr: 1.4e-05 - train_loss: 1.520 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.72s, total_time_elapsed t=28835.0s\n",
      "Epoch: 373 curr_lr: 1.4e-05 - train_loss: 1.517 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.37s, total_time_elapsed t=28910.0s\n",
      "Epoch: 374 curr_lr: 1.3e-05 - train_loss: 1.516 val_score: 0.118  best_val_score: 0.118  last_epoch t=87.31s, total_time_elapsed t=28997.0s\n",
      "Epoch: 375 curr_lr: 1.3e-05 - train_loss: 1.516 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.77s, total_time_elapsed t=29073.0s\n",
      "Epoch: 376 curr_lr: 1.2e-05 - train_loss: 1.515 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.65s, total_time_elapsed t=29149.0s\n",
      "Epoch: 377 curr_lr: 1.2e-05 - train_loss: 1.514 val_score: 0.118  best_val_score: 0.118  last_epoch t=78.04s, total_time_elapsed t=29227.0s\n",
      "Epoch: 378 curr_lr: 1.1e-05 - train_loss: 1.511 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.57s, total_time_elapsed t=29303.0s\n",
      "Epoch: 379 curr_lr: 1.1e-05 - train_loss: 1.511 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.45s, total_time_elapsed t=29378.0s\n",
      "Epoch: 380 curr_lr: 1.0e-05 - train_loss: 1.509 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.27s, total_time_elapsed t=29453.0s\n",
      "Epoch: 381 curr_lr: 1.0e-05 - train_loss: 1.508 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.44s, total_time_elapsed t=29529.0s\n",
      "Epoch: 382 curr_lr: 9.5e-06 - train_loss: 1.506 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.44s, total_time_elapsed t=29604.0s\n",
      "Epoch: 383 curr_lr: 9.0e-06 - train_loss: 1.505 val_score: 0.118  best_val_score: 0.118  last_epoch t=86.30s, total_time_elapsed t=29691.0s\n",
      "Epoch: 384 curr_lr: 8.5e-06 - train_loss: 1.504 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.42s, total_time_elapsed t=29766.0s\n",
      "Epoch: 385 curr_lr: 8.0e-06 - train_loss: 1.503 val_score: 0.119  best_val_score: 0.118  last_epoch t=76.80s, total_time_elapsed t=29843.0s\n",
      "Epoch: 386 curr_lr: 7.5e-06 - train_loss: 1.502 val_score: 0.119  best_val_score: 0.118  last_epoch t=75.28s, total_time_elapsed t=29918.0s\n",
      "Epoch: 387 curr_lr: 7.0e-06 - train_loss: 1.499 val_score: 0.118  best_val_score: 0.118  last_epoch t=87.53s, total_time_elapsed t=30006.0s\n",
      "Epoch: 388 curr_lr: 6.5e-06 - train_loss: 1.500 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.41s, total_time_elapsed t=30081.0s\n",
      "Epoch: 389 curr_lr: 6.0e-06 - train_loss: 1.498 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.39s, total_time_elapsed t=30157.0s\n",
      "Epoch: 390 curr_lr: 5.5e-06 - train_loss: 1.497 val_score: 0.118  best_val_score: 0.118  last_epoch t=76.20s, total_time_elapsed t=30233.0s\n",
      "Epoch: 391 curr_lr: 5.0e-06 - train_loss: 1.497 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.50s, total_time_elapsed t=30308.0s\n",
      "Epoch: 392 curr_lr: 4.5e-06 - train_loss: 1.494 val_score: 0.118  best_val_score: 0.118  last_epoch t=75.44s, total_time_elapsed t=30384.0s\n",
      "Epoch: 393 curr_lr: 4.0e-06 - train_loss: 1.495 val_score: 0.117  best_val_score: 0.117  last_epoch t=88.47s, total_time_elapsed t=30472.0s\n",
      "Epoch: 394 curr_lr: 3.5e-06 - train_loss: 1.493 val_score: 0.118  best_val_score: 0.117  last_epoch t=75.30s, total_time_elapsed t=30548.0s\n",
      "Epoch: 395 curr_lr: 3.0e-06 - train_loss: 1.491 val_score: 0.117  best_val_score: 0.117  last_epoch t=86.99s, total_time_elapsed t=30635.0s\n",
      "Epoch: 396 curr_lr: 2.5e-06 - train_loss: 1.491 val_score: 0.117  best_val_score: 0.117  last_epoch t=75.46s, total_time_elapsed t=30710.0s\n",
      "Epoch: 397 curr_lr: 2.0e-06 - train_loss: 1.489 val_score: 0.117  best_val_score: 0.117  last_epoch t=75.55s, total_time_elapsed t=30786.0s\n",
      "Epoch: 398 curr_lr: 1.5e-06 - train_loss: 1.489 val_score: 0.117  best_val_score: 0.117  last_epoch t=75.65s, total_time_elapsed t=30861.0s\n",
      "Epoch: 399 curr_lr: 1.0e-06 - train_loss: 1.488 val_score: 0.117  best_val_score: 0.117  last_epoch t=90.84s, total_time_elapsed t=30952.0s\n",
      "\n",
      "MAE val: 0.1171\n",
      "--------------- > Fold 1 < --------------- \n",
      "\n",
      "Epoch: 000 curr_lr: 2.0e-04 - train_loss: 4.981 val_score: 1.244  best_val_score: 1.244  last_epoch t=319.18s, total_time_elapsed t=319.0s\n",
      "Epoch: 001 curr_lr: 2.0e-04 - train_loss: 4.033 val_score: 0.847  best_val_score: 0.847  last_epoch t=76.64s, total_time_elapsed t=396.0s\n",
      "Epoch: 002 curr_lr: 2.0e-04 - train_loss: 3.759 val_score: 0.704  best_val_score: 0.704  last_epoch t=76.64s, total_time_elapsed t=473.0s\n",
      "Epoch: 003 curr_lr: 2.0e-04 - train_loss: 3.601 val_score: 0.649  best_val_score: 0.649  last_epoch t=76.30s, total_time_elapsed t=549.0s\n",
      "Epoch: 004 curr_lr: 2.0e-04 - train_loss: 3.486 val_score: 0.628  best_val_score: 0.628  last_epoch t=76.01s, total_time_elapsed t=625.0s\n",
      "Epoch: 005 curr_lr: 2.0e-04 - train_loss: 3.400 val_score: 0.527  best_val_score: 0.527  last_epoch t=76.10s, total_time_elapsed t=701.0s\n",
      "Epoch: 006 curr_lr: 2.0e-04 - train_loss: 3.331 val_score: 0.501  best_val_score: 0.501  last_epoch t=76.08s, total_time_elapsed t=777.0s\n",
      "Epoch: 007 curr_lr: 2.0e-04 - train_loss: 3.265 val_score: 0.444  best_val_score: 0.444  last_epoch t=76.02s, total_time_elapsed t=853.0s\n",
      "Epoch: 008 curr_lr: 2.0e-04 - train_loss: 3.203 val_score: 0.457  best_val_score: 0.444  last_epoch t=76.16s, total_time_elapsed t=929.0s\n",
      "Epoch: 009 curr_lr: 2.0e-04 - train_loss: 3.154 val_score: 0.429  best_val_score: 0.429  last_epoch t=76.32s, total_time_elapsed t=1006.0s\n",
      "Epoch: 010 curr_lr: 2.0e-04 - train_loss: 3.109 val_score: 0.377  best_val_score: 0.377  last_epoch t=76.71s, total_time_elapsed t=1082.0s\n",
      "Epoch: 011 curr_lr: 1.9e-04 - train_loss: 3.062 val_score: 0.375  best_val_score: 0.375  last_epoch t=76.69s, total_time_elapsed t=1159.0s\n",
      "Epoch: 012 curr_lr: 1.9e-04 - train_loss: 3.034 val_score: 0.359  best_val_score: 0.359  last_epoch t=76.31s, total_time_elapsed t=1235.0s\n",
      "Epoch: 013 curr_lr: 1.9e-04 - train_loss: 2.997 val_score: 0.364  best_val_score: 0.359  last_epoch t=76.18s, total_time_elapsed t=1312.0s\n",
      "Epoch: 014 curr_lr: 1.9e-04 - train_loss: 2.958 val_score: 0.337  best_val_score: 0.337  last_epoch t=76.10s, total_time_elapsed t=1388.0s\n",
      "Epoch: 015 curr_lr: 1.9e-04 - train_loss: 2.936 val_score: 0.329  best_val_score: 0.329  last_epoch t=76.05s, total_time_elapsed t=1464.0s\n",
      "Epoch: 016 curr_lr: 1.9e-04 - train_loss: 2.908 val_score: 0.324  best_val_score: 0.324  last_epoch t=76.35s, total_time_elapsed t=1540.0s\n",
      "Epoch: 017 curr_lr: 1.9e-04 - train_loss: 2.882 val_score: 0.297  best_val_score: 0.297  last_epoch t=76.47s, total_time_elapsed t=1617.0s\n",
      "Epoch: 018 curr_lr: 1.9e-04 - train_loss: 2.855 val_score: 0.313  best_val_score: 0.297  last_epoch t=76.56s, total_time_elapsed t=1693.0s\n",
      "Epoch: 019 curr_lr: 1.9e-04 - train_loss: 2.835 val_score: 0.303  best_val_score: 0.297  last_epoch t=76.10s, total_time_elapsed t=1769.0s\n",
      "Epoch: 020 curr_lr: 1.9e-04 - train_loss: 2.813 val_score: 0.291  best_val_score: 0.291  last_epoch t=76.16s, total_time_elapsed t=1846.0s\n",
      "Epoch: 021 curr_lr: 1.9e-04 - train_loss: 2.796 val_score: 0.278  best_val_score: 0.278  last_epoch t=75.90s, total_time_elapsed t=1922.0s\n",
      "Epoch: 022 curr_lr: 1.9e-04 - train_loss: 2.773 val_score: 0.298  best_val_score: 0.278  last_epoch t=76.12s, total_time_elapsed t=1998.0s\n",
      "Epoch: 023 curr_lr: 1.9e-04 - train_loss: 2.758 val_score: 0.285  best_val_score: 0.278  last_epoch t=76.22s, total_time_elapsed t=2074.0s\n",
      "Epoch: 024 curr_lr: 1.9e-04 - train_loss: 2.745 val_score: 0.256  best_val_score: 0.256  last_epoch t=75.99s, total_time_elapsed t=2150.0s\n",
      "Epoch: 025 curr_lr: 1.9e-04 - train_loss: 2.718 val_score: 0.274  best_val_score: 0.256  last_epoch t=76.51s, total_time_elapsed t=2226.0s\n",
      "Epoch: 026 curr_lr: 1.9e-04 - train_loss: 2.702 val_score: 0.254  best_val_score: 0.254  last_epoch t=76.19s, total_time_elapsed t=2303.0s\n",
      "Epoch: 027 curr_lr: 1.9e-04 - train_loss: 2.696 val_score: 0.262  best_val_score: 0.254  last_epoch t=76.03s, total_time_elapsed t=2379.0s\n",
      "Epoch: 028 curr_lr: 1.9e-04 - train_loss: 2.675 val_score: 0.254  best_val_score: 0.254  last_epoch t=77.07s, total_time_elapsed t=2456.0s\n",
      "Epoch: 029 curr_lr: 1.9e-04 - train_loss: 2.656 val_score: 0.236  best_val_score: 0.236  last_epoch t=76.64s, total_time_elapsed t=2533.0s\n",
      "Epoch: 030 curr_lr: 1.9e-04 - train_loss: 2.643 val_score: 0.254  best_val_score: 0.236  last_epoch t=76.09s, total_time_elapsed t=2609.0s\n",
      "Epoch: 031 curr_lr: 1.8e-04 - train_loss: 2.630 val_score: 0.243  best_val_score: 0.236  last_epoch t=76.83s, total_time_elapsed t=2685.0s\n",
      "Epoch: 032 curr_lr: 1.8e-04 - train_loss: 2.615 val_score: 0.256  best_val_score: 0.236  last_epoch t=76.02s, total_time_elapsed t=2762.0s\n",
      "Epoch: 033 curr_lr: 1.8e-04 - train_loss: 2.607 val_score: 0.232  best_val_score: 0.232  last_epoch t=76.05s, total_time_elapsed t=2838.0s\n",
      "Epoch: 034 curr_lr: 1.8e-04 - train_loss: 2.596 val_score: 0.228  best_val_score: 0.228  last_epoch t=76.43s, total_time_elapsed t=2914.0s\n",
      "Epoch: 035 curr_lr: 1.8e-04 - train_loss: 2.577 val_score: 0.228  best_val_score: 0.228  last_epoch t=76.06s, total_time_elapsed t=2990.0s\n",
      "Epoch: 036 curr_lr: 1.8e-04 - train_loss: 2.567 val_score: 0.223  best_val_score: 0.223  last_epoch t=76.00s, total_time_elapsed t=3066.0s\n",
      "Epoch: 037 curr_lr: 1.8e-04 - train_loss: 2.550 val_score: 0.225  best_val_score: 0.223  last_epoch t=76.30s, total_time_elapsed t=3142.0s\n",
      "Epoch: 038 curr_lr: 1.8e-04 - train_loss: 2.554 val_score: 0.216  best_val_score: 0.216  last_epoch t=76.14s, total_time_elapsed t=3219.0s\n",
      "Epoch: 039 curr_lr: 1.8e-04 - train_loss: 2.532 val_score: 0.233  best_val_score: 0.216  last_epoch t=76.46s, total_time_elapsed t=3295.0s\n",
      "Epoch: 040 curr_lr: 1.8e-04 - train_loss: 2.527 val_score: 0.229  best_val_score: 0.216  last_epoch t=76.38s, total_time_elapsed t=3372.0s\n",
      "Epoch: 041 curr_lr: 1.8e-04 - train_loss: 2.513 val_score: 0.208  best_val_score: 0.208  last_epoch t=76.15s, total_time_elapsed t=3448.0s\n",
      "Epoch: 042 curr_lr: 1.8e-04 - train_loss: 2.499 val_score: 0.219  best_val_score: 0.208  last_epoch t=75.94s, total_time_elapsed t=3524.0s\n",
      "Epoch: 043 curr_lr: 1.8e-04 - train_loss: 2.493 val_score: 0.223  best_val_score: 0.208  last_epoch t=76.08s, total_time_elapsed t=3600.0s\n",
      "Epoch: 044 curr_lr: 1.8e-04 - train_loss: 2.486 val_score: 0.223  best_val_score: 0.208  last_epoch t=76.07s, total_time_elapsed t=3676.0s\n",
      "Epoch: 045 curr_lr: 1.8e-04 - train_loss: 2.473 val_score: 0.210  best_val_score: 0.208  last_epoch t=76.17s, total_time_elapsed t=3752.0s\n",
      "Epoch: 046 curr_lr: 1.8e-04 - train_loss: 2.462 val_score: 0.210  best_val_score: 0.208  last_epoch t=75.99s, total_time_elapsed t=3828.0s\n",
      "Epoch: 047 curr_lr: 1.8e-04 - train_loss: 2.451 val_score: 0.211  best_val_score: 0.208  last_epoch t=75.91s, total_time_elapsed t=3904.0s\n",
      "Epoch: 048 curr_lr: 1.8e-04 - train_loss: 2.443 val_score: 0.204  best_val_score: 0.204  last_epoch t=76.23s, total_time_elapsed t=3980.0s\n",
      "Epoch: 049 curr_lr: 1.8e-04 - train_loss: 2.438 val_score: 0.201  best_val_score: 0.201  last_epoch t=76.41s, total_time_elapsed t=4057.0s\n",
      "Epoch: 050 curr_lr: 1.8e-04 - train_loss: 2.423 val_score: 0.201  best_val_score: 0.201  last_epoch t=75.99s, total_time_elapsed t=4133.0s\n",
      "Epoch: 051 curr_lr: 1.7e-04 - train_loss: 2.417 val_score: 0.196  best_val_score: 0.196  last_epoch t=75.88s, total_time_elapsed t=4209.0s\n",
      "Epoch: 052 curr_lr: 1.7e-04 - train_loss: 2.416 val_score: 0.204  best_val_score: 0.196  last_epoch t=76.36s, total_time_elapsed t=4285.0s\n",
      "Epoch: 053 curr_lr: 1.7e-04 - train_loss: 2.403 val_score: 0.203  best_val_score: 0.196  last_epoch t=76.29s, total_time_elapsed t=4361.0s\n",
      "Epoch: 054 curr_lr: 1.7e-04 - train_loss: 2.398 val_score: 0.205  best_val_score: 0.196  last_epoch t=76.29s, total_time_elapsed t=4438.0s\n",
      "Epoch: 055 curr_lr: 1.7e-04 - train_loss: 2.390 val_score: 0.193  best_val_score: 0.193  last_epoch t=76.38s, total_time_elapsed t=4514.0s\n",
      "Epoch: 056 curr_lr: 1.7e-04 - train_loss: 2.387 val_score: 0.193  best_val_score: 0.193  last_epoch t=76.05s, total_time_elapsed t=4590.0s\n",
      "Epoch: 057 curr_lr: 1.7e-04 - train_loss: 2.377 val_score: 0.182  best_val_score: 0.182  last_epoch t=76.34s, total_time_elapsed t=4666.0s\n",
      "Epoch: 058 curr_lr: 1.7e-04 - train_loss: 2.358 val_score: 0.185  best_val_score: 0.182  last_epoch t=76.50s, total_time_elapsed t=4743.0s\n",
      "Epoch: 059 curr_lr: 1.7e-04 - train_loss: 2.355 val_score: 0.194  best_val_score: 0.182  last_epoch t=76.20s, total_time_elapsed t=4819.0s\n",
      "Epoch: 060 curr_lr: 1.7e-04 - train_loss: 2.353 val_score: 0.186  best_val_score: 0.182  last_epoch t=76.09s, total_time_elapsed t=4895.0s\n",
      "Epoch: 061 curr_lr: 1.7e-04 - train_loss: 2.346 val_score: 0.184  best_val_score: 0.182  last_epoch t=75.93s, total_time_elapsed t=4971.0s\n",
      "Epoch: 062 curr_lr: 1.7e-04 - train_loss: 2.337 val_score: 0.191  best_val_score: 0.182  last_epoch t=76.01s, total_time_elapsed t=5047.0s\n",
      "Epoch: 063 curr_lr: 1.7e-04 - train_loss: 2.333 val_score: 0.185  best_val_score: 0.182  last_epoch t=76.82s, total_time_elapsed t=5124.0s\n",
      "Epoch: 064 curr_lr: 1.7e-04 - train_loss: 2.323 val_score: 0.187  best_val_score: 0.182  last_epoch t=76.03s, total_time_elapsed t=5200.0s\n",
      "Epoch: 065 curr_lr: 1.7e-04 - train_loss: 2.313 val_score: 0.186  best_val_score: 0.182  last_epoch t=76.26s, total_time_elapsed t=5276.0s\n",
      "Epoch: 066 curr_lr: 1.7e-04 - train_loss: 2.305 val_score: 0.183  best_val_score: 0.182  last_epoch t=76.02s, total_time_elapsed t=5353.0s\n",
      "Epoch: 067 curr_lr: 1.7e-04 - train_loss: 2.307 val_score: 0.185  best_val_score: 0.182  last_epoch t=76.41s, total_time_elapsed t=5429.0s\n",
      "Epoch: 068 curr_lr: 1.7e-04 - train_loss: 2.301 val_score: 0.184  best_val_score: 0.182  last_epoch t=76.06s, total_time_elapsed t=5505.0s\n",
      "Epoch: 069 curr_lr: 1.7e-04 - train_loss: 2.291 val_score: 0.186  best_val_score: 0.182  last_epoch t=76.16s, total_time_elapsed t=5581.0s\n",
      "Epoch: 070 curr_lr: 1.7e-04 - train_loss: 2.287 val_score: 0.181  best_val_score: 0.181  last_epoch t=76.24s, total_time_elapsed t=5657.0s\n",
      "Epoch: 071 curr_lr: 1.6e-04 - train_loss: 2.280 val_score: 0.185  best_val_score: 0.181  last_epoch t=75.82s, total_time_elapsed t=5733.0s\n",
      "Epoch: 072 curr_lr: 1.6e-04 - train_loss: 2.267 val_score: 0.179  best_val_score: 0.179  last_epoch t=76.04s, total_time_elapsed t=5809.0s\n",
      "Epoch: 073 curr_lr: 1.6e-04 - train_loss: 2.269 val_score: 0.187  best_val_score: 0.179  last_epoch t=75.88s, total_time_elapsed t=5885.0s\n",
      "Epoch: 074 curr_lr: 1.6e-04 - train_loss: 2.267 val_score: 0.189  best_val_score: 0.179  last_epoch t=75.96s, total_time_elapsed t=5961.0s\n",
      "Epoch: 075 curr_lr: 1.6e-04 - train_loss: 2.254 val_score: 0.181  best_val_score: 0.179  last_epoch t=76.79s, total_time_elapsed t=6038.0s\n",
      "Epoch: 076 curr_lr: 1.6e-04 - train_loss: 2.256 val_score: 0.186  best_val_score: 0.179  last_epoch t=76.10s, total_time_elapsed t=6114.0s\n",
      "Epoch: 077 curr_lr: 1.6e-04 - train_loss: 2.246 val_score: 0.184  best_val_score: 0.179  last_epoch t=76.31s, total_time_elapsed t=6191.0s\n",
      "Epoch: 078 curr_lr: 1.6e-04 - train_loss: 2.240 val_score: 0.173  best_val_score: 0.173  last_epoch t=76.12s, total_time_elapsed t=6267.0s\n",
      "Epoch: 079 curr_lr: 1.6e-04 - train_loss: 2.234 val_score: 0.174  best_val_score: 0.173  last_epoch t=75.88s, total_time_elapsed t=6343.0s\n",
      "Epoch: 080 curr_lr: 1.6e-04 - train_loss: 2.232 val_score: 0.172  best_val_score: 0.172  last_epoch t=76.30s, total_time_elapsed t=6419.0s\n",
      "Epoch: 081 curr_lr: 1.6e-04 - train_loss: 2.222 val_score: 0.172  best_val_score: 0.172  last_epoch t=76.01s, total_time_elapsed t=6495.0s\n",
      "Epoch: 082 curr_lr: 1.6e-04 - train_loss: 2.217 val_score: 0.167  best_val_score: 0.167  last_epoch t=76.21s, total_time_elapsed t=6571.0s\n",
      "Epoch: 083 curr_lr: 1.6e-04 - train_loss: 2.215 val_score: 0.171  best_val_score: 0.167  last_epoch t=76.41s, total_time_elapsed t=6648.0s\n",
      "Epoch: 084 curr_lr: 1.6e-04 - train_loss: 2.207 val_score: 0.167  best_val_score: 0.167  last_epoch t=76.20s, total_time_elapsed t=6724.0s\n",
      "Epoch: 085 curr_lr: 1.6e-04 - train_loss: 2.203 val_score: 0.170  best_val_score: 0.167  last_epoch t=76.33s, total_time_elapsed t=6800.0s\n",
      "Epoch: 086 curr_lr: 1.6e-04 - train_loss: 2.211 val_score: 0.178  best_val_score: 0.167  last_epoch t=76.22s, total_time_elapsed t=6876.0s\n",
      "Epoch: 087 curr_lr: 1.6e-04 - train_loss: 2.195 val_score: 0.172  best_val_score: 0.167  last_epoch t=75.83s, total_time_elapsed t=6952.0s\n",
      "Epoch: 088 curr_lr: 1.6e-04 - train_loss: 2.188 val_score: 0.164  best_val_score: 0.164  last_epoch t=75.89s, total_time_elapsed t=7028.0s\n",
      "Epoch: 089 curr_lr: 1.6e-04 - train_loss: 2.184 val_score: 0.171  best_val_score: 0.164  last_epoch t=76.08s, total_time_elapsed t=7104.0s\n",
      "Epoch: 090 curr_lr: 1.6e-04 - train_loss: 2.178 val_score: 0.171  best_val_score: 0.164  last_epoch t=76.53s, total_time_elapsed t=7181.0s\n",
      "Epoch: 091 curr_lr: 1.5e-04 - train_loss: 2.179 val_score: 0.179  best_val_score: 0.164  last_epoch t=76.18s, total_time_elapsed t=7257.0s\n",
      "Epoch: 092 curr_lr: 1.5e-04 - train_loss: 2.172 val_score: 0.167  best_val_score: 0.164  last_epoch t=76.27s, total_time_elapsed t=7333.0s\n",
      "Epoch: 093 curr_lr: 1.5e-04 - train_loss: 2.168 val_score: 0.166  best_val_score: 0.164  last_epoch t=76.29s, total_time_elapsed t=7410.0s\n",
      "Epoch: 094 curr_lr: 1.5e-04 - train_loss: 2.165 val_score: 0.174  best_val_score: 0.164  last_epoch t=76.28s, total_time_elapsed t=7486.0s\n",
      "Epoch: 095 curr_lr: 1.5e-04 - train_loss: 2.155 val_score: 0.163  best_val_score: 0.163  last_epoch t=75.93s, total_time_elapsed t=7562.0s\n",
      "Epoch: 096 curr_lr: 1.5e-04 - train_loss: 2.154 val_score: 0.167  best_val_score: 0.163  last_epoch t=76.19s, total_time_elapsed t=7638.0s\n",
      "Epoch: 097 curr_lr: 1.5e-04 - train_loss: 2.151 val_score: 0.164  best_val_score: 0.163  last_epoch t=76.15s, total_time_elapsed t=7714.0s\n",
      "Epoch: 098 curr_lr: 1.5e-04 - train_loss: 2.141 val_score: 0.168  best_val_score: 0.163  last_epoch t=76.90s, total_time_elapsed t=7791.0s\n",
      "Epoch: 099 curr_lr: 1.5e-04 - train_loss: 2.142 val_score: 0.166  best_val_score: 0.163  last_epoch t=76.17s, total_time_elapsed t=7867.0s\n",
      "Epoch: 100 curr_lr: 1.5e-04 - train_loss: 2.132 val_score: 0.167  best_val_score: 0.163  last_epoch t=75.99s, total_time_elapsed t=7943.0s\n",
      "Epoch: 101 curr_lr: 1.5e-04 - train_loss: 2.135 val_score: 0.163  best_val_score: 0.163  last_epoch t=76.03s, total_time_elapsed t=8020.0s\n",
      "Epoch: 102 curr_lr: 1.5e-04 - train_loss: 2.126 val_score: 0.171  best_val_score: 0.163  last_epoch t=76.10s, total_time_elapsed t=8096.0s\n",
      "Epoch: 103 curr_lr: 1.5e-04 - train_loss: 2.124 val_score: 0.167  best_val_score: 0.163  last_epoch t=76.07s, total_time_elapsed t=8172.0s\n",
      "Epoch: 104 curr_lr: 1.5e-04 - train_loss: 2.126 val_score: 0.163  best_val_score: 0.163  last_epoch t=75.94s, total_time_elapsed t=8248.0s\n",
      "Epoch: 105 curr_lr: 1.5e-04 - train_loss: 2.115 val_score: 0.158  best_val_score: 0.158  last_epoch t=75.95s, total_time_elapsed t=8324.0s\n",
      "Epoch: 106 curr_lr: 1.5e-04 - train_loss: 2.111 val_score: 0.162  best_val_score: 0.158  last_epoch t=76.09s, total_time_elapsed t=8400.0s\n",
      "Epoch: 107 curr_lr: 1.5e-04 - train_loss: 2.111 val_score: 0.158  best_val_score: 0.158  last_epoch t=76.27s, total_time_elapsed t=8476.0s\n",
      "Epoch: 108 curr_lr: 1.5e-04 - train_loss: 2.097 val_score: 0.160  best_val_score: 0.158  last_epoch t=76.30s, total_time_elapsed t=8552.0s\n",
      "Epoch: 109 curr_lr: 1.5e-04 - train_loss: 2.098 val_score: 0.155  best_val_score: 0.155  last_epoch t=76.42s, total_time_elapsed t=8629.0s\n",
      "Epoch: 110 curr_lr: 1.5e-04 - train_loss: 2.095 val_score: 0.161  best_val_score: 0.155  last_epoch t=76.03s, total_time_elapsed t=8705.0s\n",
      "Epoch: 111 curr_lr: 1.4e-04 - train_loss: 2.090 val_score: 0.160  best_val_score: 0.155  last_epoch t=76.46s, total_time_elapsed t=8781.0s\n",
      "Epoch: 112 curr_lr: 1.4e-04 - train_loss: 2.090 val_score: 0.161  best_val_score: 0.155  last_epoch t=76.33s, total_time_elapsed t=8858.0s\n",
      "Epoch: 113 curr_lr: 1.4e-04 - train_loss: 2.089 val_score: 0.161  best_val_score: 0.155  last_epoch t=76.07s, total_time_elapsed t=8934.0s\n",
      "Epoch: 114 curr_lr: 1.4e-04 - train_loss: 2.080 val_score: 0.155  best_val_score: 0.155  last_epoch t=76.38s, total_time_elapsed t=9010.0s\n",
      "Epoch: 115 curr_lr: 1.4e-04 - train_loss: 2.077 val_score: 0.156  best_val_score: 0.155  last_epoch t=76.53s, total_time_elapsed t=9087.0s\n",
      "Epoch: 116 curr_lr: 1.4e-04 - train_loss: 2.070 val_score: 0.156  best_val_score: 0.155  last_epoch t=76.70s, total_time_elapsed t=9164.0s\n",
      "Epoch: 117 curr_lr: 1.4e-04 - train_loss: 2.071 val_score: 0.150  best_val_score: 0.150  last_epoch t=75.90s, total_time_elapsed t=9239.0s\n",
      "Epoch: 118 curr_lr: 1.4e-04 - train_loss: 2.063 val_score: 0.158  best_val_score: 0.150  last_epoch t=76.52s, total_time_elapsed t=9316.0s\n",
      "Epoch: 119 curr_lr: 1.4e-04 - train_loss: 2.063 val_score: 0.160  best_val_score: 0.150  last_epoch t=76.62s, total_time_elapsed t=9393.0s\n",
      "Epoch: 120 curr_lr: 1.4e-04 - train_loss: 2.066 val_score: 0.161  best_val_score: 0.150  last_epoch t=75.98s, total_time_elapsed t=9469.0s\n",
      "Epoch: 121 curr_lr: 1.4e-04 - train_loss: 2.059 val_score: 0.158  best_val_score: 0.150  last_epoch t=75.96s, total_time_elapsed t=9545.0s\n",
      "Epoch: 122 curr_lr: 1.4e-04 - train_loss: 2.050 val_score: 0.156  best_val_score: 0.150  last_epoch t=76.64s, total_time_elapsed t=9621.0s\n",
      "Epoch: 123 curr_lr: 1.4e-04 - train_loss: 2.046 val_score: 0.153  best_val_score: 0.150  last_epoch t=76.02s, total_time_elapsed t=9697.0s\n",
      "Epoch: 124 curr_lr: 1.4e-04 - train_loss: 2.046 val_score: 0.157  best_val_score: 0.150  last_epoch t=76.44s, total_time_elapsed t=9774.0s\n",
      "Epoch: 125 curr_lr: 1.4e-04 - train_loss: 2.040 val_score: 0.154  best_val_score: 0.150  last_epoch t=75.88s, total_time_elapsed t=9850.0s\n",
      "Epoch: 126 curr_lr: 1.4e-04 - train_loss: 2.040 val_score: 0.157  best_val_score: 0.150  last_epoch t=76.02s, total_time_elapsed t=9926.0s\n",
      "Epoch: 127 curr_lr: 1.4e-04 - train_loss: 2.032 val_score: 0.155  best_val_score: 0.150  last_epoch t=76.05s, total_time_elapsed t=10002.0s\n",
      "Epoch: 128 curr_lr: 1.4e-04 - train_loss: 2.029 val_score: 0.154  best_val_score: 0.150  last_epoch t=76.06s, total_time_elapsed t=10078.0s\n",
      "Epoch: 129 curr_lr: 1.4e-04 - train_loss: 2.039 val_score: 0.152  best_val_score: 0.150  last_epoch t=76.53s, total_time_elapsed t=10155.0s\n",
      "Epoch: 130 curr_lr: 1.4e-04 - train_loss: 2.024 val_score: 0.149  best_val_score: 0.149  last_epoch t=76.05s, total_time_elapsed t=10231.0s\n",
      "Epoch: 131 curr_lr: 1.3e-04 - train_loss: 2.020 val_score: 0.150  best_val_score: 0.149  last_epoch t=75.78s, total_time_elapsed t=10306.0s\n",
      "Epoch: 132 curr_lr: 1.3e-04 - train_loss: 2.018 val_score: 0.155  best_val_score: 0.149  last_epoch t=75.97s, total_time_elapsed t=10382.0s\n",
      "Epoch: 133 curr_lr: 1.3e-04 - train_loss: 2.012 val_score: 0.150  best_val_score: 0.149  last_epoch t=75.81s, total_time_elapsed t=10458.0s\n",
      "Epoch: 134 curr_lr: 1.3e-04 - train_loss: 2.011 val_score: 0.155  best_val_score: 0.149  last_epoch t=76.17s, total_time_elapsed t=10534.0s\n",
      "Epoch: 135 curr_lr: 1.3e-04 - train_loss: 2.005 val_score: 0.148  best_val_score: 0.148  last_epoch t=76.75s, total_time_elapsed t=10611.0s\n",
      "Epoch: 136 curr_lr: 1.3e-04 - train_loss: 2.003 val_score: 0.152  best_val_score: 0.148  last_epoch t=76.29s, total_time_elapsed t=10688.0s\n",
      "Epoch: 137 curr_lr: 1.3e-04 - train_loss: 1.996 val_score: 0.153  best_val_score: 0.148  last_epoch t=75.97s, total_time_elapsed t=10763.0s\n",
      "Epoch: 138 curr_lr: 1.3e-04 - train_loss: 2.001 val_score: 0.153  best_val_score: 0.148  last_epoch t=75.99s, total_time_elapsed t=10840.0s\n",
      "Epoch: 139 curr_lr: 1.3e-04 - train_loss: 1.998 val_score: 0.155  best_val_score: 0.148  last_epoch t=76.14s, total_time_elapsed t=10916.0s\n",
      "Epoch: 140 curr_lr: 1.3e-04 - train_loss: 1.992 val_score: 0.153  best_val_score: 0.148  last_epoch t=75.87s, total_time_elapsed t=10992.0s\n",
      "Epoch: 141 curr_lr: 1.3e-04 - train_loss: 1.989 val_score: 0.152  best_val_score: 0.148  last_epoch t=76.74s, total_time_elapsed t=11068.0s\n",
      "Epoch: 142 curr_lr: 1.3e-04 - train_loss: 1.984 val_score: 0.156  best_val_score: 0.148  last_epoch t=76.63s, total_time_elapsed t=11145.0s\n",
      "Epoch: 143 curr_lr: 1.3e-04 - train_loss: 1.983 val_score: 0.148  best_val_score: 0.148  last_epoch t=76.09s, total_time_elapsed t=11221.0s\n",
      "Epoch: 144 curr_lr: 1.3e-04 - train_loss: 1.978 val_score: 0.146  best_val_score: 0.146  last_epoch t=76.02s, total_time_elapsed t=11297.0s\n",
      "Epoch: 145 curr_lr: 1.3e-04 - train_loss: 1.975 val_score: 0.149  best_val_score: 0.146  last_epoch t=76.99s, total_time_elapsed t=11374.0s\n",
      "Epoch: 146 curr_lr: 1.3e-04 - train_loss: 1.979 val_score: 0.154  best_val_score: 0.146  last_epoch t=76.21s, total_time_elapsed t=11450.0s\n",
      "Epoch: 147 curr_lr: 1.3e-04 - train_loss: 1.970 val_score: 0.151  best_val_score: 0.146  last_epoch t=76.67s, total_time_elapsed t=11527.0s\n",
      "Epoch: 148 curr_lr: 1.3e-04 - train_loss: 1.965 val_score: 0.154  best_val_score: 0.146  last_epoch t=76.20s, total_time_elapsed t=11603.0s\n",
      "Epoch: 149 curr_lr: 1.3e-04 - train_loss: 1.964 val_score: 0.144  best_val_score: 0.144  last_epoch t=76.08s, total_time_elapsed t=11679.0s\n",
      "Epoch: 150 curr_lr: 1.3e-04 - train_loss: 1.962 val_score: 0.149  best_val_score: 0.144  last_epoch t=76.06s, total_time_elapsed t=11755.0s\n",
      "Epoch: 151 curr_lr: 1.2e-04 - train_loss: 1.961 val_score: 0.148  best_val_score: 0.144  last_epoch t=76.14s, total_time_elapsed t=11832.0s\n",
      "Epoch: 152 curr_lr: 1.2e-04 - train_loss: 1.958 val_score: 0.149  best_val_score: 0.144  last_epoch t=76.55s, total_time_elapsed t=11908.0s\n",
      "Epoch: 153 curr_lr: 1.2e-04 - train_loss: 1.948 val_score: 0.147  best_val_score: 0.144  last_epoch t=76.17s, total_time_elapsed t=11984.0s\n",
      "Epoch: 154 curr_lr: 1.2e-04 - train_loss: 1.951 val_score: 0.146  best_val_score: 0.144  last_epoch t=76.20s, total_time_elapsed t=12061.0s\n",
      "Epoch: 155 curr_lr: 1.2e-04 - train_loss: 1.943 val_score: 0.147  best_val_score: 0.144  last_epoch t=76.41s, total_time_elapsed t=12137.0s\n",
      "Epoch: 156 curr_lr: 1.2e-04 - train_loss: 1.941 val_score: 0.146  best_val_score: 0.144  last_epoch t=76.12s, total_time_elapsed t=12213.0s\n",
      "Epoch: 157 curr_lr: 1.2e-04 - train_loss: 1.940 val_score: 0.147  best_val_score: 0.144  last_epoch t=76.14s, total_time_elapsed t=12289.0s\n",
      "Epoch: 158 curr_lr: 1.2e-04 - train_loss: 1.936 val_score: 0.145  best_val_score: 0.144  last_epoch t=76.02s, total_time_elapsed t=12365.0s\n",
      "Epoch: 159 curr_lr: 1.2e-04 - train_loss: 1.932 val_score: 0.146  best_val_score: 0.144  last_epoch t=75.80s, total_time_elapsed t=12441.0s\n",
      "Epoch: 160 curr_lr: 1.2e-04 - train_loss: 1.931 val_score: 0.146  best_val_score: 0.144  last_epoch t=76.07s, total_time_elapsed t=12517.0s\n",
      "Epoch: 161 curr_lr: 1.2e-04 - train_loss: 1.926 val_score: 0.145  best_val_score: 0.144  last_epoch t=76.44s, total_time_elapsed t=12594.0s\n",
      "Epoch: 162 curr_lr: 1.2e-04 - train_loss: 1.924 val_score: 0.147  best_val_score: 0.144  last_epoch t=76.34s, total_time_elapsed t=12670.0s\n",
      "Epoch: 163 curr_lr: 1.2e-04 - train_loss: 1.931 val_score: 0.146  best_val_score: 0.144  last_epoch t=76.82s, total_time_elapsed t=12747.0s\n",
      "Epoch: 164 curr_lr: 1.2e-04 - train_loss: 1.922 val_score: 0.148  best_val_score: 0.144  last_epoch t=76.29s, total_time_elapsed t=12823.0s\n",
      "Epoch: 165 curr_lr: 1.2e-04 - train_loss: 1.920 val_score: 0.148  best_val_score: 0.144  last_epoch t=76.04s, total_time_elapsed t=12899.0s\n",
      "Epoch: 166 curr_lr: 1.2e-04 - train_loss: 1.915 val_score: 0.145  best_val_score: 0.144  last_epoch t=76.53s, total_time_elapsed t=12976.0s\n",
      "Epoch: 167 curr_lr: 1.2e-04 - train_loss: 1.914 val_score: 0.144  best_val_score: 0.144  last_epoch t=76.09s, total_time_elapsed t=13052.0s\n",
      "Epoch: 168 curr_lr: 1.2e-04 - train_loss: 1.911 val_score: 0.144  best_val_score: 0.144  last_epoch t=76.11s, total_time_elapsed t=13128.0s\n",
      "Epoch: 169 curr_lr: 1.2e-04 - train_loss: 1.906 val_score: 0.142  best_val_score: 0.142  last_epoch t=87.32s, total_time_elapsed t=13216.0s\n",
      "Epoch: 170 curr_lr: 1.2e-04 - train_loss: 1.899 val_score: 0.145  best_val_score: 0.142  last_epoch t=78.18s, total_time_elapsed t=13294.0s\n",
      "Epoch: 171 curr_lr: 1.1e-04 - train_loss: 1.899 val_score: 0.143  best_val_score: 0.142  last_epoch t=76.43s, total_time_elapsed t=13370.0s\n",
      "Epoch: 172 curr_lr: 1.1e-04 - train_loss: 1.896 val_score: 0.145  best_val_score: 0.142  last_epoch t=76.29s, total_time_elapsed t=13447.0s\n",
      "Epoch: 173 curr_lr: 1.1e-04 - train_loss: 1.898 val_score: 0.148  best_val_score: 0.142  last_epoch t=76.08s, total_time_elapsed t=13523.0s\n",
      "Epoch: 174 curr_lr: 1.1e-04 - train_loss: 1.893 val_score: 0.146  best_val_score: 0.142  last_epoch t=76.42s, total_time_elapsed t=13599.0s\n",
      "Epoch: 175 curr_lr: 1.1e-04 - train_loss: 1.893 val_score: 0.143  best_val_score: 0.142  last_epoch t=76.03s, total_time_elapsed t=13675.0s\n",
      "Epoch: 176 curr_lr: 1.1e-04 - train_loss: 1.887 val_score: 0.146  best_val_score: 0.142  last_epoch t=76.45s, total_time_elapsed t=13752.0s\n",
      "Epoch: 177 curr_lr: 1.1e-04 - train_loss: 1.891 val_score: 0.144  best_val_score: 0.142  last_epoch t=76.77s, total_time_elapsed t=13828.0s\n",
      "Epoch: 178 curr_lr: 1.1e-04 - train_loss: 1.885 val_score: 0.142  best_val_score: 0.142  last_epoch t=90.96s, total_time_elapsed t=13919.0s\n",
      "Epoch: 179 curr_lr: 1.1e-04 - train_loss: 1.890 val_score: 0.143  best_val_score: 0.142  last_epoch t=76.95s, total_time_elapsed t=13996.0s\n",
      "Epoch: 180 curr_lr: 1.1e-04 - train_loss: 1.877 val_score: 0.141  best_val_score: 0.141  last_epoch t=87.29s, total_time_elapsed t=14084.0s\n",
      "Epoch: 181 curr_lr: 1.1e-04 - train_loss: 1.873 val_score: 0.144  best_val_score: 0.141  last_epoch t=76.29s, total_time_elapsed t=14160.0s\n",
      "Epoch: 182 curr_lr: 1.1e-04 - train_loss: 1.871 val_score: 0.142  best_val_score: 0.141  last_epoch t=76.10s, total_time_elapsed t=14236.0s\n",
      "Epoch: 183 curr_lr: 1.1e-04 - train_loss: 1.869 val_score: 0.143  best_val_score: 0.141  last_epoch t=76.29s, total_time_elapsed t=14312.0s\n",
      "Epoch: 184 curr_lr: 1.1e-04 - train_loss: 1.868 val_score: 0.141  best_val_score: 0.141  last_epoch t=86.62s, total_time_elapsed t=14399.0s\n",
      "Epoch: 185 curr_lr: 1.1e-04 - train_loss: 1.863 val_score: 0.141  best_val_score: 0.141  last_epoch t=76.16s, total_time_elapsed t=14475.0s\n",
      "Epoch: 186 curr_lr: 1.1e-04 - train_loss: 1.861 val_score: 0.141  best_val_score: 0.141  last_epoch t=77.90s, total_time_elapsed t=14553.0s\n",
      "Epoch: 187 curr_lr: 1.1e-04 - train_loss: 1.859 val_score: 0.141  best_val_score: 0.141  last_epoch t=87.57s, total_time_elapsed t=14641.0s\n",
      "Epoch: 188 curr_lr: 1.1e-04 - train_loss: 1.858 val_score: 0.142  best_val_score: 0.141  last_epoch t=76.15s, total_time_elapsed t=14717.0s\n",
      "Epoch: 189 curr_lr: 1.1e-04 - train_loss: 1.856 val_score: 0.141  best_val_score: 0.141  last_epoch t=76.39s, total_time_elapsed t=14793.0s\n",
      "Epoch: 190 curr_lr: 1.1e-04 - train_loss: 1.853 val_score: 0.139  best_val_score: 0.139  last_epoch t=86.41s, total_time_elapsed t=14880.0s\n",
      "Epoch: 191 curr_lr: 1.0e-04 - train_loss: 1.850 val_score: 0.143  best_val_score: 0.139  last_epoch t=76.40s, total_time_elapsed t=14956.0s\n",
      "Epoch: 192 curr_lr: 1.0e-04 - train_loss: 1.858 val_score: 0.141  best_val_score: 0.139  last_epoch t=76.32s, total_time_elapsed t=15033.0s\n",
      "Epoch: 193 curr_lr: 1.0e-04 - train_loss: 1.848 val_score: 0.141  best_val_score: 0.139  last_epoch t=75.98s, total_time_elapsed t=15109.0s\n",
      "Epoch: 194 curr_lr: 1.0e-04 - train_loss: 1.841 val_score: 0.142  best_val_score: 0.139  last_epoch t=76.77s, total_time_elapsed t=15185.0s\n",
      "Epoch: 195 curr_lr: 1.0e-04 - train_loss: 1.841 val_score: 0.140  best_val_score: 0.139  last_epoch t=76.46s, total_time_elapsed t=15262.0s\n",
      "Epoch: 196 curr_lr: 1.0e-04 - train_loss: 1.836 val_score: 0.142  best_val_score: 0.139  last_epoch t=76.20s, total_time_elapsed t=15338.0s\n",
      "Epoch: 197 curr_lr: 1.0e-04 - train_loss: 1.832 val_score: 0.147  best_val_score: 0.139  last_epoch t=75.94s, total_time_elapsed t=15414.0s\n",
      "Epoch: 198 curr_lr: 1.0e-04 - train_loss: 1.834 val_score: 0.136  best_val_score: 0.136  last_epoch t=87.17s, total_time_elapsed t=15501.0s\n",
      "Epoch: 199 curr_lr: 1.0e-04 - train_loss: 1.834 val_score: 0.142  best_val_score: 0.136  last_epoch t=75.90s, total_time_elapsed t=15577.0s\n",
      "Epoch: 200 curr_lr: 1.0e-04 - train_loss: 1.830 val_score: 0.141  best_val_score: 0.136  last_epoch t=76.13s, total_time_elapsed t=15653.0s\n",
      "Epoch: 201 curr_lr: 1.0e-04 - train_loss: 1.825 val_score: 0.139  best_val_score: 0.136  last_epoch t=75.98s, total_time_elapsed t=15729.0s\n",
      "Epoch: 202 curr_lr: 9.9e-05 - train_loss: 1.820 val_score: 0.142  best_val_score: 0.136  last_epoch t=77.29s, total_time_elapsed t=15807.0s\n",
      "Epoch: 203 curr_lr: 9.9e-05 - train_loss: 1.823 val_score: 0.140  best_val_score: 0.136  last_epoch t=76.34s, total_time_elapsed t=15883.0s\n",
      "Epoch: 204 curr_lr: 9.8e-05 - train_loss: 1.818 val_score: 0.137  best_val_score: 0.136  last_epoch t=76.01s, total_time_elapsed t=15959.0s\n",
      "Epoch: 205 curr_lr: 9.8e-05 - train_loss: 1.816 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.15s, total_time_elapsed t=16035.0s\n",
      "Epoch: 206 curr_lr: 9.7e-05 - train_loss: 1.813 val_score: 0.139  best_val_score: 0.136  last_epoch t=76.09s, total_time_elapsed t=16111.0s\n",
      "Epoch: 207 curr_lr: 9.7e-05 - train_loss: 1.814 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.02s, total_time_elapsed t=16187.0s\n",
      "Epoch: 208 curr_lr: 9.6e-05 - train_loss: 1.814 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.24s, total_time_elapsed t=16264.0s\n",
      "Epoch: 209 curr_lr: 9.6e-05 - train_loss: 1.810 val_score: 0.136  best_val_score: 0.136  last_epoch t=76.15s, total_time_elapsed t=16340.0s\n",
      "Epoch: 210 curr_lr: 9.5e-05 - train_loss: 1.809 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.88s, total_time_elapsed t=16417.0s\n",
      "Epoch: 211 curr_lr: 9.5e-05 - train_loss: 1.806 val_score: 0.137  best_val_score: 0.136  last_epoch t=76.31s, total_time_elapsed t=16493.0s\n",
      "Epoch: 212 curr_lr: 9.4e-05 - train_loss: 1.799 val_score: 0.136  best_val_score: 0.136  last_epoch t=75.81s, total_time_elapsed t=16569.0s\n",
      "Epoch: 213 curr_lr: 9.4e-05 - train_loss: 1.798 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.33s, total_time_elapsed t=16645.0s\n",
      "Epoch: 214 curr_lr: 9.3e-05 - train_loss: 1.798 val_score: 0.138  best_val_score: 0.136  last_epoch t=76.66s, total_time_elapsed t=16722.0s\n",
      "Epoch: 215 curr_lr: 9.3e-05 - train_loss: 1.793 val_score: 0.136  best_val_score: 0.136  last_epoch t=76.05s, total_time_elapsed t=16798.0s\n",
      "Epoch: 216 curr_lr: 9.2e-05 - train_loss: 1.796 val_score: 0.138  best_val_score: 0.136  last_epoch t=75.87s, total_time_elapsed t=16874.0s\n",
      "Epoch: 217 curr_lr: 9.2e-05 - train_loss: 1.789 val_score: 0.137  best_val_score: 0.136  last_epoch t=76.27s, total_time_elapsed t=16950.0s\n",
      "Epoch: 218 curr_lr: 9.1e-05 - train_loss: 1.789 val_score: 0.135  best_val_score: 0.135  last_epoch t=92.20s, total_time_elapsed t=17043.0s\n",
      "Epoch: 219 curr_lr: 9.1e-05 - train_loss: 1.784 val_score: 0.135  best_val_score: 0.135  last_epoch t=87.34s, total_time_elapsed t=17130.0s\n",
      "Epoch: 220 curr_lr: 9.0e-05 - train_loss: 1.777 val_score: 0.139  best_val_score: 0.135  last_epoch t=76.25s, total_time_elapsed t=17206.0s\n",
      "Epoch: 221 curr_lr: 9.0e-05 - train_loss: 1.779 val_score: 0.138  best_val_score: 0.135  last_epoch t=75.91s, total_time_elapsed t=17282.0s\n",
      "Epoch: 222 curr_lr: 8.9e-05 - train_loss: 1.780 val_score: 0.138  best_val_score: 0.135  last_epoch t=76.20s, total_time_elapsed t=17358.0s\n",
      "Epoch: 223 curr_lr: 8.9e-05 - train_loss: 1.779 val_score: 0.136  best_val_score: 0.135  last_epoch t=76.12s, total_time_elapsed t=17434.0s\n",
      "Epoch: 224 curr_lr: 8.8e-05 - train_loss: 1.774 val_score: 0.136  best_val_score: 0.135  last_epoch t=76.75s, total_time_elapsed t=17511.0s\n",
      "Epoch: 225 curr_lr: 8.8e-05 - train_loss: 1.774 val_score: 0.135  best_val_score: 0.135  last_epoch t=76.14s, total_time_elapsed t=17587.0s\n",
      "Epoch: 226 curr_lr: 8.7e-05 - train_loss: 1.770 val_score: 0.139  best_val_score: 0.135  last_epoch t=77.40s, total_time_elapsed t=17665.0s\n",
      "Epoch: 227 curr_lr: 8.7e-05 - train_loss: 1.765 val_score: 0.134  best_val_score: 0.134  last_epoch t=86.45s, total_time_elapsed t=17751.0s\n",
      "Epoch: 228 curr_lr: 8.6e-05 - train_loss: 1.766 val_score: 0.134  best_val_score: 0.134  last_epoch t=86.17s, total_time_elapsed t=17838.0s\n",
      "Epoch: 229 curr_lr: 8.6e-05 - train_loss: 1.762 val_score: 0.134  best_val_score: 0.134  last_epoch t=87.13s, total_time_elapsed t=17925.0s\n",
      "Epoch: 230 curr_lr: 8.5e-05 - train_loss: 1.759 val_score: 0.134  best_val_score: 0.134  last_epoch t=87.09s, total_time_elapsed t=18012.0s\n",
      "Epoch: 231 curr_lr: 8.5e-05 - train_loss: 1.760 val_score: 0.134  best_val_score: 0.134  last_epoch t=76.37s, total_time_elapsed t=18088.0s\n",
      "Epoch: 232 curr_lr: 8.4e-05 - train_loss: 1.758 val_score: 0.134  best_val_score: 0.134  last_epoch t=76.13s, total_time_elapsed t=18164.0s\n",
      "Epoch: 233 curr_lr: 8.4e-05 - train_loss: 1.758 val_score: 0.140  best_val_score: 0.134  last_epoch t=77.53s, total_time_elapsed t=18242.0s\n",
      "Epoch: 234 curr_lr: 8.3e-05 - train_loss: 1.753 val_score: 0.133  best_val_score: 0.133  last_epoch t=89.08s, total_time_elapsed t=18331.0s\n",
      "Epoch: 235 curr_lr: 8.3e-05 - train_loss: 1.748 val_score: 0.134  best_val_score: 0.133  last_epoch t=76.37s, total_time_elapsed t=18407.0s\n",
      "Epoch: 236 curr_lr: 8.2e-05 - train_loss: 1.748 val_score: 0.134  best_val_score: 0.133  last_epoch t=75.94s, total_time_elapsed t=18483.0s\n",
      "Epoch: 237 curr_lr: 8.2e-05 - train_loss: 1.745 val_score: 0.135  best_val_score: 0.133  last_epoch t=75.97s, total_time_elapsed t=18559.0s\n",
      "Epoch: 238 curr_lr: 8.1e-05 - train_loss: 1.740 val_score: 0.133  best_val_score: 0.133  last_epoch t=76.17s, total_time_elapsed t=18636.0s\n",
      "Epoch: 239 curr_lr: 8.1e-05 - train_loss: 1.738 val_score: 0.134  best_val_score: 0.133  last_epoch t=76.12s, total_time_elapsed t=18712.0s\n",
      "Epoch: 240 curr_lr: 8.0e-05 - train_loss: 1.741 val_score: 0.132  best_val_score: 0.132  last_epoch t=87.41s, total_time_elapsed t=18799.0s\n",
      "Epoch: 241 curr_lr: 8.0e-05 - train_loss: 1.737 val_score: 0.135  best_val_score: 0.132  last_epoch t=77.76s, total_time_elapsed t=18877.0s\n",
      "Epoch: 242 curr_lr: 7.9e-05 - train_loss: 1.737 val_score: 0.134  best_val_score: 0.132  last_epoch t=76.18s, total_time_elapsed t=18953.0s\n",
      "Epoch: 243 curr_lr: 7.9e-05 - train_loss: 1.734 val_score: 0.136  best_val_score: 0.132  last_epoch t=76.31s, total_time_elapsed t=19029.0s\n",
      "Epoch: 244 curr_lr: 7.8e-05 - train_loss: 1.734 val_score: 0.133  best_val_score: 0.132  last_epoch t=75.90s, total_time_elapsed t=19105.0s\n",
      "Epoch: 245 curr_lr: 7.8e-05 - train_loss: 1.729 val_score: 0.137  best_val_score: 0.132  last_epoch t=75.83s, total_time_elapsed t=19181.0s\n",
      "Epoch: 246 curr_lr: 7.7e-05 - train_loss: 1.725 val_score: 0.134  best_val_score: 0.132  last_epoch t=76.28s, total_time_elapsed t=19258.0s\n",
      "Epoch: 247 curr_lr: 7.7e-05 - train_loss: 1.727 val_score: 0.135  best_val_score: 0.132  last_epoch t=76.75s, total_time_elapsed t=19334.0s\n",
      "Epoch: 248 curr_lr: 7.6e-05 - train_loss: 1.723 val_score: 0.134  best_val_score: 0.132  last_epoch t=76.03s, total_time_elapsed t=19410.0s\n",
      "Epoch: 249 curr_lr: 7.6e-05 - train_loss: 1.723 val_score: 0.134  best_val_score: 0.132  last_epoch t=77.22s, total_time_elapsed t=19488.0s\n",
      "Epoch: 250 curr_lr: 7.5e-05 - train_loss: 1.720 val_score: 0.134  best_val_score: 0.132  last_epoch t=76.21s, total_time_elapsed t=19564.0s\n",
      "Epoch: 251 curr_lr: 7.5e-05 - train_loss: 1.718 val_score: 0.134  best_val_score: 0.132  last_epoch t=76.10s, total_time_elapsed t=19640.0s\n",
      "Epoch: 252 curr_lr: 7.4e-05 - train_loss: 1.722 val_score: 0.133  best_val_score: 0.132  last_epoch t=76.03s, total_time_elapsed t=19716.0s\n",
      "Epoch: 253 curr_lr: 7.4e-05 - train_loss: 1.713 val_score: 0.133  best_val_score: 0.132  last_epoch t=75.90s, total_time_elapsed t=19792.0s\n",
      "Epoch: 254 curr_lr: 7.3e-05 - train_loss: 1.711 val_score: 0.131  best_val_score: 0.131  last_epoch t=87.10s, total_time_elapsed t=19879.0s\n",
      "Epoch: 255 curr_lr: 7.3e-05 - train_loss: 1.713 val_score: 0.132  best_val_score: 0.131  last_epoch t=76.69s, total_time_elapsed t=19956.0s\n",
      "Epoch: 256 curr_lr: 7.2e-05 - train_loss: 1.707 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.78s, total_time_elapsed t=20032.0s\n",
      "Epoch: 257 curr_lr: 7.2e-05 - train_loss: 1.705 val_score: 0.133  best_val_score: 0.131  last_epoch t=77.29s, total_time_elapsed t=20109.0s\n",
      "Epoch: 258 curr_lr: 7.1e-05 - train_loss: 1.705 val_score: 0.135  best_val_score: 0.131  last_epoch t=75.94s, total_time_elapsed t=20185.0s\n",
      "Epoch: 259 curr_lr: 7.1e-05 - train_loss: 1.702 val_score: 0.134  best_val_score: 0.131  last_epoch t=76.32s, total_time_elapsed t=20261.0s\n",
      "Epoch: 260 curr_lr: 7.0e-05 - train_loss: 1.699 val_score: 0.133  best_val_score: 0.131  last_epoch t=75.85s, total_time_elapsed t=20337.0s\n",
      "Epoch: 261 curr_lr: 7.0e-05 - train_loss: 1.699 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.78s, total_time_elapsed t=20413.0s\n",
      "Epoch: 262 curr_lr: 6.9e-05 - train_loss: 1.697 val_score: 0.132  best_val_score: 0.131  last_epoch t=75.98s, total_time_elapsed t=20489.0s\n",
      "Epoch: 263 curr_lr: 6.9e-05 - train_loss: 1.693 val_score: 0.131  best_val_score: 0.131  last_epoch t=86.84s, total_time_elapsed t=20576.0s\n",
      "Epoch: 264 curr_lr: 6.8e-05 - train_loss: 1.692 val_score: 0.133  best_val_score: 0.131  last_epoch t=76.18s, total_time_elapsed t=20652.0s\n",
      "Epoch: 265 curr_lr: 6.8e-05 - train_loss: 1.691 val_score: 0.133  best_val_score: 0.131  last_epoch t=77.11s, total_time_elapsed t=20729.0s\n",
      "Epoch: 266 curr_lr: 6.7e-05 - train_loss: 1.690 val_score: 0.133  best_val_score: 0.131  last_epoch t=75.94s, total_time_elapsed t=20805.0s\n",
      "Epoch: 267 curr_lr: 6.7e-05 - train_loss: 1.685 val_score: 0.133  best_val_score: 0.131  last_epoch t=76.04s, total_time_elapsed t=20881.0s\n",
      "Epoch: 268 curr_lr: 6.6e-05 - train_loss: 1.684 val_score: 0.132  best_val_score: 0.131  last_epoch t=76.10s, total_time_elapsed t=20957.0s\n",
      "Epoch: 269 curr_lr: 6.6e-05 - train_loss: 1.681 val_score: 0.131  best_val_score: 0.131  last_epoch t=86.47s, total_time_elapsed t=21044.0s\n",
      "Epoch: 270 curr_lr: 6.5e-05 - train_loss: 1.679 val_score: 0.132  best_val_score: 0.131  last_epoch t=78.42s, total_time_elapsed t=21122.0s\n",
      "Epoch: 271 curr_lr: 6.5e-05 - train_loss: 1.678 val_score: 0.130  best_val_score: 0.130  last_epoch t=85.77s, total_time_elapsed t=21208.0s\n",
      "Epoch: 272 curr_lr: 6.4e-05 - train_loss: 1.674 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.72s, total_time_elapsed t=21285.0s\n",
      "Epoch: 273 curr_lr: 6.4e-05 - train_loss: 1.676 val_score: 0.132  best_val_score: 0.130  last_epoch t=76.96s, total_time_elapsed t=21362.0s\n",
      "Epoch: 274 curr_lr: 6.3e-05 - train_loss: 1.673 val_score: 0.133  best_val_score: 0.130  last_epoch t=75.86s, total_time_elapsed t=21438.0s\n",
      "Epoch: 275 curr_lr: 6.3e-05 - train_loss: 1.672 val_score: 0.131  best_val_score: 0.130  last_epoch t=75.98s, total_time_elapsed t=21514.0s\n",
      "Epoch: 276 curr_lr: 6.2e-05 - train_loss: 1.667 val_score: 0.133  best_val_score: 0.130  last_epoch t=76.06s, total_time_elapsed t=21590.0s\n",
      "Epoch: 277 curr_lr: 6.2e-05 - train_loss: 1.665 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.05s, total_time_elapsed t=21666.0s\n",
      "Epoch: 278 curr_lr: 6.1e-05 - train_loss: 1.665 val_score: 0.130  best_val_score: 0.130  last_epoch t=87.12s, total_time_elapsed t=21753.0s\n",
      "Epoch: 279 curr_lr: 6.1e-05 - train_loss: 1.663 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.57s, total_time_elapsed t=21830.0s\n",
      "Epoch: 280 curr_lr: 6.0e-05 - train_loss: 1.660 val_score: 0.132  best_val_score: 0.130  last_epoch t=76.39s, total_time_elapsed t=21906.0s\n",
      "Epoch: 281 curr_lr: 6.0e-05 - train_loss: 1.658 val_score: 0.130  best_val_score: 0.130  last_epoch t=77.46s, total_time_elapsed t=21983.0s\n",
      "Epoch: 282 curr_lr: 5.9e-05 - train_loss: 1.657 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.01s, total_time_elapsed t=22059.0s\n",
      "Epoch: 283 curr_lr: 5.9e-05 - train_loss: 1.656 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.00s, total_time_elapsed t=22135.0s\n",
      "Epoch: 284 curr_lr: 5.8e-05 - train_loss: 1.655 val_score: 0.131  best_val_score: 0.130  last_epoch t=76.60s, total_time_elapsed t=22212.0s\n",
      "Epoch: 285 curr_lr: 5.8e-05 - train_loss: 1.650 val_score: 0.132  best_val_score: 0.130  last_epoch t=76.11s, total_time_elapsed t=22288.0s\n",
      "Epoch: 286 curr_lr: 5.7e-05 - train_loss: 1.649 val_score: 0.128  best_val_score: 0.128  last_epoch t=86.18s, total_time_elapsed t=22374.0s\n",
      "Epoch: 287 curr_lr: 5.7e-05 - train_loss: 1.648 val_score: 0.128  best_val_score: 0.128  last_epoch t=85.77s, total_time_elapsed t=22460.0s\n",
      "Epoch: 288 curr_lr: 5.6e-05 - train_loss: 1.646 val_score: 0.130  best_val_score: 0.128  last_epoch t=78.00s, total_time_elapsed t=22538.0s\n",
      "Epoch: 289 curr_lr: 5.6e-05 - train_loss: 1.643 val_score: 0.130  best_val_score: 0.128  last_epoch t=76.06s, total_time_elapsed t=22614.0s\n",
      "Epoch: 290 curr_lr: 5.5e-05 - train_loss: 1.641 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.88s, total_time_elapsed t=22690.0s\n",
      "Epoch: 291 curr_lr: 5.5e-05 - train_loss: 1.640 val_score: 0.130  best_val_score: 0.128  last_epoch t=76.90s, total_time_elapsed t=22767.0s\n",
      "Epoch: 292 curr_lr: 5.4e-05 - train_loss: 1.638 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.93s, total_time_elapsed t=22843.0s\n",
      "Epoch: 293 curr_lr: 5.4e-05 - train_loss: 1.636 val_score: 0.130  best_val_score: 0.128  last_epoch t=75.89s, total_time_elapsed t=22919.0s\n",
      "Epoch: 294 curr_lr: 5.3e-05 - train_loss: 1.634 val_score: 0.127  best_val_score: 0.127  last_epoch t=86.88s, total_time_elapsed t=23006.0s\n",
      "Epoch: 295 curr_lr: 5.3e-05 - train_loss: 1.629 val_score: 0.129  best_val_score: 0.127  last_epoch t=76.66s, total_time_elapsed t=23083.0s\n",
      "Epoch: 296 curr_lr: 5.2e-05 - train_loss: 1.630 val_score: 0.130  best_val_score: 0.127  last_epoch t=77.42s, total_time_elapsed t=23160.0s\n",
      "Epoch: 297 curr_lr: 5.2e-05 - train_loss: 1.630 val_score: 0.128  best_val_score: 0.127  last_epoch t=76.70s, total_time_elapsed t=23237.0s\n",
      "Epoch: 298 curr_lr: 5.1e-05 - train_loss: 1.626 val_score: 0.128  best_val_score: 0.127  last_epoch t=76.08s, total_time_elapsed t=23313.0s\n",
      "Epoch: 299 curr_lr: 5.1e-05 - train_loss: 1.627 val_score: 0.129  best_val_score: 0.127  last_epoch t=75.81s, total_time_elapsed t=23389.0s\n",
      "Epoch: 300 curr_lr: 5.0e-05 - train_loss: 1.624 val_score: 0.126  best_val_score: 0.126  last_epoch t=87.44s, total_time_elapsed t=23476.0s\n",
      "Epoch: 301 curr_lr: 5.0e-05 - train_loss: 1.623 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.09s, total_time_elapsed t=23552.0s\n",
      "Epoch: 302 curr_lr: 4.9e-05 - train_loss: 1.619 val_score: 0.129  best_val_score: 0.126  last_epoch t=76.03s, total_time_elapsed t=23628.0s\n",
      "Epoch: 303 curr_lr: 4.9e-05 - train_loss: 1.618 val_score: 0.129  best_val_score: 0.126  last_epoch t=75.96s, total_time_elapsed t=23704.0s\n",
      "Epoch: 304 curr_lr: 4.8e-05 - train_loss: 1.615 val_score: 0.128  best_val_score: 0.126  last_epoch t=77.34s, total_time_elapsed t=23782.0s\n",
      "Epoch: 305 curr_lr: 4.8e-05 - train_loss: 1.616 val_score: 0.127  best_val_score: 0.126  last_epoch t=75.94s, total_time_elapsed t=23858.0s\n",
      "Epoch: 306 curr_lr: 4.7e-05 - train_loss: 1.613 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.23s, total_time_elapsed t=23934.0s\n",
      "Epoch: 307 curr_lr: 4.7e-05 - train_loss: 1.612 val_score: 0.128  best_val_score: 0.126  last_epoch t=75.92s, total_time_elapsed t=24010.0s\n",
      "Epoch: 308 curr_lr: 4.6e-05 - train_loss: 1.609 val_score: 0.129  best_val_score: 0.126  last_epoch t=75.94s, total_time_elapsed t=24086.0s\n",
      "Epoch: 309 curr_lr: 4.6e-05 - train_loss: 1.607 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.01s, total_time_elapsed t=24162.0s\n",
      "Epoch: 310 curr_lr: 4.5e-05 - train_loss: 1.606 val_score: 0.127  best_val_score: 0.126  last_epoch t=75.99s, total_time_elapsed t=24238.0s\n",
      "Epoch: 311 curr_lr: 4.5e-05 - train_loss: 1.605 val_score: 0.128  best_val_score: 0.126  last_epoch t=76.16s, total_time_elapsed t=24314.0s\n",
      "Epoch: 312 curr_lr: 4.4e-05 - train_loss: 1.602 val_score: 0.128  best_val_score: 0.126  last_epoch t=76.93s, total_time_elapsed t=24391.0s\n",
      "Epoch: 313 curr_lr: 4.4e-05 - train_loss: 1.600 val_score: 0.128  best_val_score: 0.126  last_epoch t=75.80s, total_time_elapsed t=24467.0s\n",
      "Epoch: 314 curr_lr: 4.3e-05 - train_loss: 1.598 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.01s, total_time_elapsed t=24543.0s\n",
      "Epoch: 315 curr_lr: 4.3e-05 - train_loss: 1.597 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.02s, total_time_elapsed t=24619.0s\n",
      "Epoch: 316 curr_lr: 4.2e-05 - train_loss: 1.597 val_score: 0.128  best_val_score: 0.126  last_epoch t=76.04s, total_time_elapsed t=24695.0s\n",
      "Epoch: 317 curr_lr: 4.2e-05 - train_loss: 1.595 val_score: 0.126  best_val_score: 0.126  last_epoch t=86.47s, total_time_elapsed t=24781.0s\n",
      "Epoch: 318 curr_lr: 4.1e-05 - train_loss: 1.593 val_score: 0.128  best_val_score: 0.126  last_epoch t=75.94s, total_time_elapsed t=24857.0s\n",
      "Epoch: 319 curr_lr: 4.1e-05 - train_loss: 1.589 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.03s, total_time_elapsed t=24934.0s\n",
      "Epoch: 320 curr_lr: 4.0e-05 - train_loss: 1.588 val_score: 0.128  best_val_score: 0.126  last_epoch t=77.21s, total_time_elapsed t=25011.0s\n",
      "Epoch: 321 curr_lr: 4.0e-05 - train_loss: 1.588 val_score: 0.126  best_val_score: 0.126  last_epoch t=86.12s, total_time_elapsed t=25097.0s\n",
      "Epoch: 322 curr_lr: 3.9e-05 - train_loss: 1.585 val_score: 0.128  best_val_score: 0.126  last_epoch t=76.08s, total_time_elapsed t=25173.0s\n",
      "Epoch: 323 curr_lr: 3.9e-05 - train_loss: 1.584 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.13s, total_time_elapsed t=25249.0s\n",
      "Epoch: 324 curr_lr: 3.8e-05 - train_loss: 1.583 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.40s, total_time_elapsed t=25326.0s\n",
      "Epoch: 325 curr_lr: 3.8e-05 - train_loss: 1.579 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.46s, total_time_elapsed t=25402.0s\n",
      "Epoch: 326 curr_lr: 3.7e-05 - train_loss: 1.580 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.31s, total_time_elapsed t=25478.0s\n",
      "Epoch: 327 curr_lr: 3.7e-05 - train_loss: 1.578 val_score: 0.126  best_val_score: 0.126  last_epoch t=75.91s, total_time_elapsed t=25554.0s\n",
      "Epoch: 328 curr_lr: 3.6e-05 - train_loss: 1.576 val_score: 0.127  best_val_score: 0.126  last_epoch t=77.57s, total_time_elapsed t=25632.0s\n",
      "Epoch: 329 curr_lr: 3.6e-05 - train_loss: 1.576 val_score: 0.127  best_val_score: 0.126  last_epoch t=76.06s, total_time_elapsed t=25708.0s\n",
      "Epoch: 330 curr_lr: 3.5e-05 - train_loss: 1.572 val_score: 0.126  best_val_score: 0.126  last_epoch t=75.81s, total_time_elapsed t=25784.0s\n",
      "Epoch: 331 curr_lr: 3.5e-05 - train_loss: 1.572 val_score: 0.126  best_val_score: 0.126  last_epoch t=76.02s, total_time_elapsed t=25860.0s\n",
      "Epoch: 332 curr_lr: 3.4e-05 - train_loss: 1.569 val_score: 0.125  best_val_score: 0.125  last_epoch t=87.30s, total_time_elapsed t=25947.0s\n",
      "Epoch: 333 curr_lr: 3.4e-05 - train_loss: 1.569 val_score: 0.127  best_val_score: 0.125  last_epoch t=76.28s, total_time_elapsed t=26023.0s\n",
      "Epoch: 334 curr_lr: 3.3e-05 - train_loss: 1.565 val_score: 0.127  best_val_score: 0.125  last_epoch t=75.90s, total_time_elapsed t=26099.0s\n",
      "Epoch: 335 curr_lr: 3.3e-05 - train_loss: 1.564 val_score: 0.126  best_val_score: 0.125  last_epoch t=75.92s, total_time_elapsed t=26175.0s\n",
      "Epoch: 336 curr_lr: 3.2e-05 - train_loss: 1.562 val_score: 0.125  best_val_score: 0.125  last_epoch t=77.48s, total_time_elapsed t=26253.0s\n",
      "Epoch: 337 curr_lr: 3.2e-05 - train_loss: 1.561 val_score: 0.125  best_val_score: 0.125  last_epoch t=86.31s, total_time_elapsed t=26339.0s\n",
      "Epoch: 338 curr_lr: 3.1e-05 - train_loss: 1.558 val_score: 0.124  best_val_score: 0.124  last_epoch t=89.28s, total_time_elapsed t=26429.0s\n",
      "Epoch: 339 curr_lr: 3.1e-05 - train_loss: 1.557 val_score: 0.126  best_val_score: 0.124  last_epoch t=76.26s, total_time_elapsed t=26505.0s\n",
      "Epoch: 340 curr_lr: 3.0e-05 - train_loss: 1.555 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.05s, total_time_elapsed t=26581.0s\n",
      "Epoch: 341 curr_lr: 3.0e-05 - train_loss: 1.556 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.88s, total_time_elapsed t=26658.0s\n",
      "Epoch: 342 curr_lr: 2.9e-05 - train_loss: 1.551 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.92s, total_time_elapsed t=26734.0s\n",
      "Epoch: 343 curr_lr: 2.9e-05 - train_loss: 1.550 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.58s, total_time_elapsed t=26810.0s\n",
      "Epoch: 344 curr_lr: 2.8e-05 - train_loss: 1.549 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.02s, total_time_elapsed t=26886.0s\n",
      "Epoch: 345 curr_lr: 2.8e-05 - train_loss: 1.547 val_score: 0.126  best_val_score: 0.124  last_epoch t=76.12s, total_time_elapsed t=26963.0s\n",
      "Epoch: 346 curr_lr: 2.7e-05 - train_loss: 1.548 val_score: 0.124  best_val_score: 0.124  last_epoch t=86.81s, total_time_elapsed t=27049.0s\n",
      "Epoch: 347 curr_lr: 2.7e-05 - train_loss: 1.545 val_score: 0.125  best_val_score: 0.124  last_epoch t=76.31s, total_time_elapsed t=27126.0s\n",
      "Epoch: 348 curr_lr: 2.6e-05 - train_loss: 1.542 val_score: 0.124  best_val_score: 0.124  last_epoch t=85.82s, total_time_elapsed t=27212.0s\n",
      "Epoch: 349 curr_lr: 2.6e-05 - train_loss: 1.542 val_score: 0.124  best_val_score: 0.124  last_epoch t=76.42s, total_time_elapsed t=27288.0s\n",
      "Epoch: 350 curr_lr: 2.5e-05 - train_loss: 1.539 val_score: 0.125  best_val_score: 0.124  last_epoch t=75.93s, total_time_elapsed t=27364.0s\n",
      "Epoch: 351 curr_lr: 2.5e-05 - train_loss: 1.538 val_score: 0.124  best_val_score: 0.124  last_epoch t=76.35s, total_time_elapsed t=27440.0s\n",
      "Epoch: 352 curr_lr: 2.4e-05 - train_loss: 1.535 val_score: 0.123  best_val_score: 0.123  last_epoch t=86.81s, total_time_elapsed t=27527.0s\n",
      "Epoch: 353 curr_lr: 2.4e-05 - train_loss: 1.534 val_score: 0.123  best_val_score: 0.123  last_epoch t=76.02s, total_time_elapsed t=27603.0s\n",
      "Epoch: 354 curr_lr: 2.3e-05 - train_loss: 1.533 val_score: 0.124  best_val_score: 0.123  last_epoch t=76.04s, total_time_elapsed t=27679.0s\n",
      "Epoch: 355 curr_lr: 2.3e-05 - train_loss: 1.532 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.88s, total_time_elapsed t=27755.0s\n",
      "Epoch: 356 curr_lr: 2.2e-05 - train_loss: 1.531 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.98s, total_time_elapsed t=27831.0s\n",
      "Epoch: 357 curr_lr: 2.2e-05 - train_loss: 1.530 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.80s, total_time_elapsed t=27907.0s\n",
      "Epoch: 358 curr_lr: 2.1e-05 - train_loss: 1.527 val_score: 0.124  best_val_score: 0.123  last_epoch t=76.42s, total_time_elapsed t=27983.0s\n",
      "Epoch: 359 curr_lr: 2.1e-05 - train_loss: 1.527 val_score: 0.123  best_val_score: 0.123  last_epoch t=77.24s, total_time_elapsed t=28061.0s\n",
      "Epoch: 360 curr_lr: 2.0e-05 - train_loss: 1.525 val_score: 0.123  best_val_score: 0.123  last_epoch t=86.04s, total_time_elapsed t=28147.0s\n",
      "Epoch: 361 curr_lr: 2.0e-05 - train_loss: 1.524 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.87s, total_time_elapsed t=28223.0s\n",
      "Epoch: 362 curr_lr: 1.9e-05 - train_loss: 1.522 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.89s, total_time_elapsed t=28299.0s\n",
      "Epoch: 363 curr_lr: 1.9e-05 - train_loss: 1.520 val_score: 0.123  best_val_score: 0.123  last_epoch t=76.78s, total_time_elapsed t=28375.0s\n",
      "Epoch: 364 curr_lr: 1.8e-05 - train_loss: 1.519 val_score: 0.124  best_val_score: 0.123  last_epoch t=76.08s, total_time_elapsed t=28452.0s\n",
      "Epoch: 365 curr_lr: 1.8e-05 - train_loss: 1.519 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.78s, total_time_elapsed t=28527.0s\n",
      "Epoch: 366 curr_lr: 1.7e-05 - train_loss: 1.517 val_score: 0.125  best_val_score: 0.123  last_epoch t=75.89s, total_time_elapsed t=28603.0s\n",
      "Epoch: 367 curr_lr: 1.7e-05 - train_loss: 1.514 val_score: 0.124  best_val_score: 0.123  last_epoch t=77.43s, total_time_elapsed t=28681.0s\n",
      "Epoch: 368 curr_lr: 1.6e-05 - train_loss: 1.513 val_score: 0.123  best_val_score: 0.123  last_epoch t=75.96s, total_time_elapsed t=28757.0s\n",
      "Epoch: 369 curr_lr: 1.6e-05 - train_loss: 1.512 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.87s, total_time_elapsed t=28833.0s\n",
      "Epoch: 370 curr_lr: 1.5e-05 - train_loss: 1.511 val_score: 0.124  best_val_score: 0.123  last_epoch t=75.84s, total_time_elapsed t=28908.0s\n",
      "Epoch: 371 curr_lr: 1.5e-05 - train_loss: 1.511 val_score: 0.124  best_val_score: 0.123  last_epoch t=76.17s, total_time_elapsed t=28985.0s\n",
      "Epoch: 372 curr_lr: 1.4e-05 - train_loss: 1.509 val_score: 0.124  best_val_score: 0.123  last_epoch t=76.05s, total_time_elapsed t=29061.0s\n",
      "Epoch: 373 curr_lr: 1.4e-05 - train_loss: 1.507 val_score: 0.123  best_val_score: 0.123  last_epoch t=88.37s, total_time_elapsed t=29149.0s\n",
      "Epoch: 374 curr_lr: 1.3e-05 - train_loss: 1.506 val_score: 0.123  best_val_score: 0.123  last_epoch t=76.27s, total_time_elapsed t=29225.0s\n",
      "Epoch: 375 curr_lr: 1.3e-05 - train_loss: 1.504 val_score: 0.122  best_val_score: 0.122  last_epoch t=87.08s, total_time_elapsed t=29313.0s\n",
      "Epoch: 376 curr_lr: 1.2e-05 - train_loss: 1.503 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.91s, total_time_elapsed t=29388.0s\n",
      "Epoch: 377 curr_lr: 1.2e-05 - train_loss: 1.502 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.77s, total_time_elapsed t=29464.0s\n",
      "Epoch: 378 curr_lr: 1.1e-05 - train_loss: 1.499 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.95s, total_time_elapsed t=29540.0s\n",
      "Epoch: 379 curr_lr: 1.1e-05 - train_loss: 1.499 val_score: 0.122  best_val_score: 0.122  last_epoch t=75.84s, total_time_elapsed t=29616.0s\n",
      "Epoch: 380 curr_lr: 1.0e-05 - train_loss: 1.498 val_score: 0.123  best_val_score: 0.122  last_epoch t=76.21s, total_time_elapsed t=29692.0s\n",
      "Epoch: 381 curr_lr: 1.0e-05 - train_loss: 1.497 val_score: 0.123  best_val_score: 0.122  last_epoch t=76.05s, total_time_elapsed t=29768.0s\n",
      "Epoch: 382 curr_lr: 9.5e-06 - train_loss: 1.494 val_score: 0.123  best_val_score: 0.122  last_epoch t=79.42s, total_time_elapsed t=29848.0s\n",
      "Epoch: 383 curr_lr: 9.0e-06 - train_loss: 1.494 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.78s, total_time_elapsed t=29924.0s\n",
      "Epoch: 384 curr_lr: 8.5e-06 - train_loss: 1.493 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.98s, total_time_elapsed t=30000.0s\n",
      "Epoch: 385 curr_lr: 8.0e-06 - train_loss: 1.490 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.89s, total_time_elapsed t=30076.0s\n",
      "Epoch: 386 curr_lr: 7.5e-06 - train_loss: 1.490 val_score: 0.122  best_val_score: 0.122  last_epoch t=75.94s, total_time_elapsed t=30152.0s\n",
      "Epoch: 387 curr_lr: 7.0e-06 - train_loss: 1.489 val_score: 0.122  best_val_score: 0.122  last_epoch t=86.02s, total_time_elapsed t=30238.0s\n",
      "Epoch: 388 curr_lr: 6.5e-06 - train_loss: 1.488 val_score: 0.122  best_val_score: 0.122  last_epoch t=75.83s, total_time_elapsed t=30313.0s\n",
      "Epoch: 389 curr_lr: 6.0e-06 - train_loss: 1.485 val_score: 0.123  best_val_score: 0.122  last_epoch t=75.72s, total_time_elapsed t=30389.0s\n",
      "Epoch: 390 curr_lr: 5.5e-06 - train_loss: 1.486 val_score: 0.122  best_val_score: 0.122  last_epoch t=88.05s, total_time_elapsed t=30477.0s\n",
      "Epoch: 391 curr_lr: 5.0e-06 - train_loss: 1.483 val_score: 0.122  best_val_score: 0.122  last_epoch t=76.00s, total_time_elapsed t=30553.0s\n",
      "Epoch: 392 curr_lr: 4.5e-06 - train_loss: 1.483 val_score: 0.122  best_val_score: 0.122  last_epoch t=86.41s, total_time_elapsed t=30640.0s\n",
      "Epoch: 393 curr_lr: 4.0e-06 - train_loss: 1.482 val_score: 0.122  best_val_score: 0.122  last_epoch t=76.40s, total_time_elapsed t=30716.0s\n",
      "Epoch: 394 curr_lr: 3.5e-06 - train_loss: 1.481 val_score: 0.122  best_val_score: 0.122  last_epoch t=76.02s, total_time_elapsed t=30792.0s\n",
      "Epoch: 395 curr_lr: 3.0e-06 - train_loss: 1.479 val_score: 0.121  best_val_score: 0.121  last_epoch t=85.64s, total_time_elapsed t=30878.0s\n",
      "Epoch: 396 curr_lr: 2.5e-06 - train_loss: 1.480 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.95s, total_time_elapsed t=30954.0s\n",
      "Epoch: 397 curr_lr: 2.0e-06 - train_loss: 1.478 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.88s, total_time_elapsed t=31030.0s\n",
      "Epoch: 398 curr_lr: 1.5e-06 - train_loss: 1.477 val_score: 0.122  best_val_score: 0.121  last_epoch t=76.99s, total_time_elapsed t=31107.0s\n",
      "Epoch: 399 curr_lr: 1.0e-06 - train_loss: 1.477 val_score: 0.122  best_val_score: 0.121  last_epoch t=75.74s, total_time_elapsed t=31182.0s\n",
      "\n",
      "MAE val: 0.1213\n"
     ]
    }
   ],
   "source": [
    "oofs = np.zeros(train_data.pressures.shape)\n",
    "kf = KFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "preds_df_tmp = pd.DataFrame()\n",
    "oofs_df_tmp = pd.DataFrame()\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(train_data.pressures, train_data.pressures)):\n",
    "\n",
    "\n",
    "      if fold not in folds_lst:\n",
    "        continue\n",
    "\n",
    "      print('-'*15, '>', f'Fold {fold}', '<', '-'*15, '\\n')\n",
    "\n",
    "      X_trn, X_val = train_data.inputs[trn_idx], train_data.inputs[val_idx]\n",
    "      y_trn, y_val = train_data.pressures[trn_idx], train_data.pressures[val_idx]\n",
    "      y_trn_int, y_val_int = train_data.pressures_int[trn_idx], train_data.pressures_int[val_idx]\n",
    "\n",
    "      wt_trn, wt_val = train_data.u_outs[trn_idx], train_data.u_outs[val_idx]\n",
    "\n",
    "      model = get_LSTM_model(Config)\n",
    "\n",
    "      print_val_results = IntervalEvaluation(\n",
    "          validation_data=(X_val, y_val, wt_val),\n",
    "          interval=1,\n",
    "          Config=Config,\n",
    "          fold=fold,\n",
    "          )\n",
    "\n",
    "      history = model.fit([X_trn, wt_trn], y_trn_int,\n",
    "                  verbose=0,\n",
    "                  epochs=Config.N_EPOCHS,\n",
    "                  batch_size=Config.BATCH_SIZE,\n",
    "                  callbacks=[print_val_results]\n",
    "                  )\n",
    "\n",
    "      del model, X_trn, y_trn, wt_trn, print_val_results\n",
    "      _ = gc.collect()\n",
    "\n",
    "      custom_objects = {\n",
    "          'custom_mean_absolute_error': custom_mean_absolute_error,\n",
    "          'WeightedSum': WeightedSum,\n",
    "      }\n",
    "\n",
    "      best_model = load_model('best_model', custom_objects=custom_objects)\n",
    "\n",
    "      vp = get_preds(best_model, X_val, wt_val, STEP=3000)\n",
    "      vp = np.vectorize(map_class_to_cont)(vp)\n",
    "      np.save(VERSION_OUTPUT_PATH + os.sep + f'val_fold_{fold}.npy', vp)\n",
    "\n",
    "      _ = gc.collect()\n",
    "\n",
    "      val_score = round(custom_metric(y_val.flatten(), vp.flatten(), wt_val.flatten()), 4)\n",
    "      print(f'\\nMAE val: {val_score}')\n",
    "\n",
    "      tp = get_preds(best_model, test_data.inputs, test_data.u_outs, STEP=3000)\n",
    "      tp = np.vectorize(map_class_to_cont)(tp)\n",
    "\n",
    "      path = VERSION_OUTPUT_PATH + os.sep + f'sub_fold_{fold}.csv'\n",
    "      save_test_preds(tp, path)\n",
    "\n",
    "      del X_val, y_val, wt_val, best_model, vp, tp, trn_idx, val_idx\n",
    "      _ = gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pressure_prediction_dl_v55a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
